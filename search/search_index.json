{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Vespa python API","text":"<p>Vespa is the scalable open-sourced serving engine to store, compute and rank big data at user serving time. <code>pyvespa</code> provides a python API to Vespa - use it to create, modify, deploy and interact with running Vespa instances. The main goal of the library is to allow for faster prototyping and get familiar with Vespa features.</p> <p>Warning</p> <p>pyvespa is under active development and backward incompatible changes may occur.</p> <p>Hybrid Search - Quickstart is a good primer on how to create an application, feed data and run queries. See Examples for use cases. The following blog post series will get you started:</p> <ul> <li> <p>Run search engine experiments in Vespa from python</p> </li> <li> <p>Build sentence/paragraph level QA application from python with Vespa</p> </li> <li> <p>Build a basic text search application from python with Vespa: Part 1</p> </li> <li> <p>Build a News recommendation app from python with Vespa: Part 1</p> </li> </ul> <p>The Vespa FAQ is a great resource, also see pyvespa troubleshooting.</p>"},{"location":"index.html#requirements","title":"Requirements","text":"<p>Install <code>pyvespa</code>:</p> <pre><code>  python3 -m pip install pyvespa\n</code></pre> <p>Install jupyter notebook to run the notebooks in a browser:</p> <p><pre><code>  git clone --depth 1 https://github.com/vespa-engine/pyvespa.git\n  jupyter notebook --notebook-dir pyvespa/docs/sphinx/source\n</code></pre> Many of the pyvespa guides / notebooks use Docker - minimum memory requirement is 4 Gb unless other documented:</p> <pre><code>  docker info | grep \"Total Memory\"\n  or\n  podman info | grep \"memTotal\"\n</code></pre> <p>One can also use Vespa Cloud to run the notebooks.</p>"},{"location":"advanced-configuration.html","title":"Advanced Configuration","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Install pyvespa and start Docker Daemon, validate minimum 6G available:</p> In\u00a0[1]: Copied! <pre>!pip3 install pyvespa\n!docker info | grep \"Total Memory\"\n</pre> !pip3 install pyvespa !docker info | grep \"Total Memory\" In\u00a0[2]: Copied! <pre>from vespa.package import Document, Field, Schema, ApplicationPackage\n\napplication_name = \"music\"\nmusic_schema = Schema(\n    name=application_name,\n    document=Document(\n        fields=[\n            Field(\n                name=\"artist\",\n                type=\"string\",\n                indexing=[\"attribute\", \"summary\"],\n            ),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"attribute\", \"summary\"],\n            ),\n            Field(\n                name=\"timestamp\",\n                type=\"long\",\n                indexing=[\"attribute\", \"summary\"],\n                attribute=[\"fast-access\"],\n            ),\n        ]\n    ),\n)\n</pre> from vespa.package import Document, Field, Schema, ApplicationPackage  application_name = \"music\" music_schema = Schema(     name=application_name,     document=Document(         fields=[             Field(                 name=\"artist\",                 type=\"string\",                 indexing=[\"attribute\", \"summary\"],             ),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"attribute\", \"summary\"],             ),             Field(                 name=\"timestamp\",                 type=\"long\",                 indexing=[\"attribute\", \"summary\"],                 attribute=[\"fast-access\"],             ),         ]     ), ) In\u00a0[3]: Copied! <pre>from vespa.package import ServicesConfiguration\nfrom vespa.configuration.services import (\n    services,\n    container,\n    search,\n    document_api,\n    document_processing,\n    content,\n    redundancy,\n    documents,\n    document,\n    node,\n    nodes,\n)\n\n# Create a ServicesConfiguration with document-expiry set to 1 day (timestamp &gt; now() - 86400)\nservices_config = ServicesConfiguration(\n    application_name=application_name,\n    services_config=services(\n        container(\n            search(),\n            document_api(),\n            document_processing(),\n            id=f\"{application_name}_container\",\n            version=\"1.0\",\n        ),\n        content(\n            redundancy(\"1\"),\n            documents(\n                document(\n                    type=application_name,\n                    mode=\"index\",\n                    # Note that the selection-expression does not need to be escaped, as it will be automatically escaped during xml-serialization\n                    selection=\"music.timestamp &gt; now() - 86400\",\n                ),\n                garbage_collection=\"true\",\n            ),\n            nodes(node(distribution_key=\"0\", hostalias=\"node1\")),\n            id=f\"{application_name}_content\",\n            version=\"1.0\",\n        ),\n    ),\n)\napplication_package = ApplicationPackage(\n    name=application_name,\n    schema=[music_schema],\n    services_config=services_config,\n)\n</pre> from vespa.package import ServicesConfiguration from vespa.configuration.services import (     services,     container,     search,     document_api,     document_processing,     content,     redundancy,     documents,     document,     node,     nodes, )  # Create a ServicesConfiguration with document-expiry set to 1 day (timestamp &gt; now() - 86400) services_config = ServicesConfiguration(     application_name=application_name,     services_config=services(         container(             search(),             document_api(),             document_processing(),             id=f\"{application_name}_container\",             version=\"1.0\",         ),         content(             redundancy(\"1\"),             documents(                 document(                     type=application_name,                     mode=\"index\",                     # Note that the selection-expression does not need to be escaped, as it will be automatically escaped during xml-serialization                     selection=\"music.timestamp &gt; now() - 86400\",                 ),                 garbage_collection=\"true\",             ),             nodes(node(distribution_key=\"0\", hostalias=\"node1\")),             id=f\"{application_name}_content\",             version=\"1.0\",         ),     ), ) application_package = ApplicationPackage(     name=application_name,     schema=[music_schema],     services_config=services_config, ) <p>There are some useful gotchas to keep in mind when constructing the <code>ServiceConfiguration</code> object.</p> <p>First, let's establish a common vocabulary through an example. Consider the following <code>services.xml</code> file, which is what we are actually representing with the <code>ServiceConfiguration</code> object from the previous cell:</p> <pre>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\n&lt;services&gt;\n  &lt;container id=\"music_container\" version=\"1.0\"&gt;\n    &lt;search&gt;&lt;/search&gt;\n    &lt;document-api&gt;&lt;/document-api&gt;\n    &lt;document-processing&gt;&lt;/document-processing&gt;\n  &lt;/container&gt;\n  &lt;content id=\"music_content\" version=\"1.0\"&gt;\n    &lt;redundancy&gt;1&lt;/redundancy&gt;\n    &lt;documents garbage-collection=\"true\"&gt;\n      &lt;document type=\"music\" mode=\"index\" selection=\"music.timestamp &amp;gt; now() - 86400\"&gt;&lt;/document&gt;\n    &lt;/documents&gt;\n    &lt;nodes&gt;\n      &lt;node distribution-key=\"0\" hostalias=\"node1\"&gt;&lt;/node&gt;\n    &lt;/nodes&gt;\n  &lt;/content&gt;\n&lt;/services&gt;\n</pre> <p>In this example, <code>services</code>, <code>container</code>, <code>search</code>, <code>document-api</code>, <code>document-processing</code>, <code>content</code>, <code>redundancy</code>, <code>documents</code>, <code>document</code>, and <code>nodes</code> are tags. The <code>id</code>, <code>version</code>, <code>type</code>, <code>mode</code>, <code>selection</code>, <code>distribution-key</code>, <code>hostalias</code>, and <code>garbage-collection</code> are attributes, with a corresponding value.</p> In\u00a0[4]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=application_package)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy(application_package=application_package) <pre>Waiting for configuration server, 0/60 seconds...\nWaiting for application to come up, 0/300 seconds.\nApplication is up!\nFinished deployment.\n</pre> <p><code>app</code> now holds a reference to a Vespa instance. see this notebook for details on authenticating to Vespa Cloud.</p> In\u00a0[5]: Copied! <pre>import time\n\ndocs_to_feed = [\n    {\n        \"id\": \"1\",\n        \"fields\": {\n            \"artist\": \"Snoop Dogg\",\n            \"title\": \"Gin and Juice\",\n            \"timestamp\": int(time.time()) - 86401,\n        },\n    },\n    {\n        \"id\": \"2\",\n        \"fields\": {\n            \"artist\": \"Dr.Dre\",\n            \"title\": \"Still D.R.E\",\n            \"timestamp\": int(time.time()),\n        },\n    },\n]\n</pre> import time  docs_to_feed = [     {         \"id\": \"1\",         \"fields\": {             \"artist\": \"Snoop Dogg\",             \"title\": \"Gin and Juice\",             \"timestamp\": int(time.time()) - 86401,         },     },     {         \"id\": \"2\",         \"fields\": {             \"artist\": \"Dr.Dre\",             \"title\": \"Still D.R.E\",             \"timestamp\": int(time.time()),         },     }, ] In\u00a0[6]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Error when feeding document {id}: {response.get_json()}\")\n\n\napp.feed_iterable(docs_to_feed, schema=application_name, callback=callback)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Error when feeding document {id}: {response.get_json()}\")   app.feed_iterable(docs_to_feed, schema=application_name, callback=callback) In\u00a0[7]: Copied! <pre>visit_results = []\nfor slice_ in app.visit(\n    schema=application_name,\n    content_cluster_name=f\"{application_name}_content\",\n    timeout=\"5s\",\n):\n    for response in slice_:\n        visit_results.append(response.json)\nvisit_results\n</pre> visit_results = [] for slice_ in app.visit(     schema=application_name,     content_cluster_name=f\"{application_name}_content\",     timeout=\"5s\", ):     for response in slice_:         visit_results.append(response.json) visit_results Out[7]: <pre>[{'pathId': '/document/v1/music/music/docid/',\n  'documents': [{'id': 'id:music:music::2',\n    'fields': {'artist': 'Dr.Dre',\n     'title': 'Still D.R.E',\n     'timestamp': 1727428957}}],\n  'documentCount': 1}]</pre> <p>We can see that the document with the timestamp of 24 hours ago is not returned by the query, while the document with the current timestamp is returned.</p> In\u00a0[8]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove() In\u00a0[9]: Copied! <pre>from pathlib import Path\nimport requests\nfrom vespa.deployment import VespaDocker\n\n# Download the model if it doesn't exist\nurl = \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/resolve/main/onnx/model.onnx\"\nlocal_model_path = \"model/model.onnx\"\nif not Path(local_model_path).exists():\n    print(\"Downloading the mxbai-rerank model...\")\n    r = requests.get(url)\n    Path(local_model_path).parent.mkdir(parents=True, exist_ok=True)\n    with open(local_model_path, \"wb\") as f:\n        f.write(r.content)\n        print(f\"Downloaded model to {local_model_path}\")\nelse:\n    print(\"Model already exists, skipping download.\")\n</pre> from pathlib import Path import requests from vespa.deployment import VespaDocker  # Download the model if it doesn't exist url = \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/resolve/main/onnx/model.onnx\" local_model_path = \"model/model.onnx\" if not Path(local_model_path).exists():     print(\"Downloading the mxbai-rerank model...\")     r = requests.get(url)     Path(local_model_path).parent.mkdir(parents=True, exist_ok=True)     with open(local_model_path, \"wb\") as f:         f.write(r.content)         print(f\"Downloaded model to {local_model_path}\") else:     print(\"Model already exists, skipping download.\") <pre>Model already exists, skipping download.\n</pre> In\u00a0[10]: Copied! <pre>from vespa.package import (\n    OnnxModel,\n    RankProfile,\n    Schema,\n    ApplicationPackage,\n    Field,\n    FieldSet,\n    Function,\n    FirstPhaseRanking,\n    Document,\n)\n\n\napplication_name = \"requestthreads\"\n\n# Define the reranking, as we will use it for two different rank profiles\nreranking = FirstPhaseRanking(\n    keep_rank_count=8,\n    expression=\"sigmoid(onnx(crossencoder).logits{d0:0,d1:0})\",\n)\n\n# Define the schema\nschema = Schema(\n    name=\"doc\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"index\", \"summary\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"body_tokens\",\n                type=\"tensor&lt;float&gt;(d0[512])\",\n                indexing=[\n                    \"input text\",\n                    \"embed tokenizer\",\n                    \"attribute\",\n                    \"summary\",\n                ],\n                is_document_field=False,  # Indicates a synthetic field\n            ),\n        ],\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n    models=[\n        OnnxModel(\n            model_name=\"crossencoder\",\n            model_file_path=f\"{local_model_path}\",\n            inputs={\n                \"input_ids\": \"input_ids\",\n                \"attention_mask\": \"attention_mask\",\n            },\n            outputs={\"logits\": \"logits\"},\n        )\n    ],\n    rank_profiles=[\n        RankProfile(name=\"bm25\", first_phase=\"bm25(text)\"),\n        RankProfile(\n            name=\"reranking\",\n            inherits=\"default\",\n            inputs=[(\"query(q)\", \"tensor&lt;float&gt;(d0[64])\")],\n            functions=[\n                Function(\n                    name=\"input_ids\",\n                    expression=\"customTokenInputIds(1, 2, 512, query(q), attribute(body_tokens))\",\n                ),\n                Function(\n                    name=\"attention_mask\",\n                    expression=\"tokenAttentionMask(512, query(q), attribute(body_tokens))\",\n                ),\n            ],\n            first_phase=reranking,\n            summary_features=[\n                \"query(q)\",\n                \"input_ids\",\n                \"attention_mask\",\n                \"onnx(crossencoder).logits\",\n            ],\n        ),\n        RankProfile(\n            name=\"one-thread-profile\",\n            first_phase=reranking,\n            inherits=\"reranking\",\n            num_threads_per_search=1,\n        ),\n    ],\n)\n</pre> from vespa.package import (     OnnxModel,     RankProfile,     Schema,     ApplicationPackage,     Field,     FieldSet,     Function,     FirstPhaseRanking,     Document, )   application_name = \"requestthreads\"  # Define the reranking, as we will use it for two different rank profiles reranking = FirstPhaseRanking(     keep_rank_count=8,     expression=\"sigmoid(onnx(crossencoder).logits{d0:0,d1:0})\", )  # Define the schema schema = Schema(     name=\"doc\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"index\", \"summary\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"body_tokens\",                 type=\"tensor(d0[512])\",                 indexing=[                     \"input text\",                     \"embed tokenizer\",                     \"attribute\",                     \"summary\",                 ],                 is_document_field=False,  # Indicates a synthetic field             ),         ],     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],     models=[         OnnxModel(             model_name=\"crossencoder\",             model_file_path=f\"{local_model_path}\",             inputs={                 \"input_ids\": \"input_ids\",                 \"attention_mask\": \"attention_mask\",             },             outputs={\"logits\": \"logits\"},         )     ],     rank_profiles=[         RankProfile(name=\"bm25\", first_phase=\"bm25(text)\"),         RankProfile(             name=\"reranking\",             inherits=\"default\",             inputs=[(\"query(q)\", \"tensor(d0[64])\")],             functions=[                 Function(                     name=\"input_ids\",                     expression=\"customTokenInputIds(1, 2, 512, query(q), attribute(body_tokens))\",                 ),                 Function(                     name=\"attention_mask\",                     expression=\"tokenAttentionMask(512, query(q), attribute(body_tokens))\",                 ),             ],             first_phase=reranking,             summary_features=[                 \"query(q)\",                 \"input_ids\",                 \"attention_mask\",                 \"onnx(crossencoder).logits\",             ],         ),         RankProfile(             name=\"one-thread-profile\",             first_phase=reranking,             inherits=\"reranking\",             num_threads_per_search=1,         ),     ], ) In\u00a0[11]: Copied! <pre>from vespa.configuration.services import *\nfrom vespa.package import ServicesConfiguration\n\n# Define services configuration with persearch threads set to 4\nservices_config = ServicesConfiguration(\n    application_name=f\"{application_name}\",\n    services_config=services(\n        container(id=f\"{application_name}_default\", version=\"1.0\")(\n            component(\n                model(\n                    url=\"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/raw/main/tokenizer.json\"\n                ),\n                id=\"tokenizer\",\n                type=\"hugging-face-tokenizer\",\n            ),\n            document_api(),\n            search(),\n        ),\n        content(id=f\"{application_name}\", version=\"1.0\")(\n            min_redundancy(\"1\"),\n            documents(document(type=\"doc\", mode=\"index\")),\n            engine(\n                proton(\n                    tuning(\n                        searchnode(requestthreads(persearch(\"4\"))),\n                    ),\n                ),\n            ),\n        ),\n        version=\"1.0\",\n        minimum_required_vespa_version=\"8.311.28\",\n    ),\n)\n</pre> from vespa.configuration.services import * from vespa.package import ServicesConfiguration  # Define services configuration with persearch threads set to 4 services_config = ServicesConfiguration(     application_name=f\"{application_name}\",     services_config=services(         container(id=f\"{application_name}_default\", version=\"1.0\")(             component(                 model(                     url=\"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/raw/main/tokenizer.json\"                 ),                 id=\"tokenizer\",                 type=\"hugging-face-tokenizer\",             ),             document_api(),             search(),         ),         content(id=f\"{application_name}\", version=\"1.0\")(             min_redundancy(\"1\"),             documents(document(type=\"doc\", mode=\"index\")),             engine(                 proton(                     tuning(                         searchnode(requestthreads(persearch(\"4\"))),                     ),                 ),             ),         ),         version=\"1.0\",         minimum_required_vespa_version=\"8.311.28\",     ), ) <p>Now, we are ready to deploy our application-package with the defined <code>ServiceConfiguration</code>.</p> In\u00a0[12]: Copied! <pre>app_package = ApplicationPackage(\n    name=f\"{application_name}\",\n    schema=[schema],\n    services_config=services_config,\n)\n</pre> app_package = ApplicationPackage(     name=f\"{application_name}\",     schema=[schema],     services_config=services_config, ) In\u00a0[13]: Copied! <pre>app_package.to_files(\"deleteme\")\n</pre> app_package.to_files(\"deleteme\") In\u00a0[14]: Copied! <pre>vespa_docker = VespaDocker(port=8089)\napp = vespa_docker.deploy(application_package=app_package)\n</pre> vespa_docker = VespaDocker(port=8089) app = vespa_docker.deploy(application_package=app_package) <pre>Waiting for configuration server, 0/60 seconds...\nWaiting for application to come up, 0/300 seconds.\nWaiting for application to come up, 5/300 seconds.\nWaiting for application to come up, 10/300 seconds.\nWaiting for application to come up, 15/300 seconds.\nWaiting for application to come up, 20/300 seconds.\nWaiting for application to come up, 25/300 seconds.\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[15]: Copied! <pre>sample_docs = [\n    {\"id\": i, \"fields\": {\"text\": text}}\n    for i, text in enumerate(\n        [\n            \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature. The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird'. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n            \"was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961. Jane Austen was an English novelist known primarily for her six major novels, \",\n            \"which interpret, critique and comment upon the British landed gentry at the end of the 18th century. The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, \",\n            \"is among the most popular and critically acclaimed books of the modern era. 'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\",\n        ]\n    )\n]\napp.feed_iterable(sample_docs, schema=\"doc\")\n\n# Define the query body\nquery_body = {\n    \"yql\": \"select * from sources * where userQuery();\",\n    \"query\": \"who wrote to kill a mockingbird?\",\n    \"timeout\": \"10s\",\n    \"input.query(q)\": \"embed(tokenizer, @query)\",\n    \"presentation.timing\": \"true\",\n}\n\n# Warm-up query\napp.query(body=query_body)\nquery_body_reranking = {\n    **query_body,\n    \"ranking.profile\": \"reranking\",\n}\n# Query with default persearch threads (set to 4)\nwith app.syncio() as sess:\n    response_default = app.query(body=query_body_reranking)\n\n# Query with num-threads-per-search overridden to 1\nquery_body_one_thread = {\n    **query_body,\n    \"ranking.profile\": \"one-thread-profile\",\n    # \"ranking.matching.numThreadsPerSearch\": 1, Could potentiall also set numThreadsPerSearch in query parameters.\n}\nwith app.syncio() as sess:\n    response_one_thread = sess.query(body=query_body_one_thread)\n\n# Extract query times\ntiming_default = response_default.json[\"timing\"][\"querytime\"]\ntiming_one_thread = response_one_thread.json[\"timing\"][\"querytime\"]\n# Beautifully formatted statement of - num threads and ratio of query times\nprint(f\"Query time with 4 threads: {timing_default:.2f}s\")\nprint(f\"Query time with 1 thread: {timing_one_thread:.2f}s\")\nratio = timing_one_thread / timing_default\nprint(f\"4 threads is approximately {ratio:.2f}x faster than 1 thread\")\n</pre> sample_docs = [     {\"id\": i, \"fields\": {\"text\": text}}     for i, text in enumerate(         [             \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature. The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird'. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",             \"was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961. Jane Austen was an English novelist known primarily for her six major novels, \",             \"which interpret, critique and comment upon the British landed gentry at the end of the 18th century. The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, \",             \"is among the most popular and critically acclaimed books of the modern era. 'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\",         ]     ) ] app.feed_iterable(sample_docs, schema=\"doc\")  # Define the query body query_body = {     \"yql\": \"select * from sources * where userQuery();\",     \"query\": \"who wrote to kill a mockingbird?\",     \"timeout\": \"10s\",     \"input.query(q)\": \"embed(tokenizer, @query)\",     \"presentation.timing\": \"true\", }  # Warm-up query app.query(body=query_body) query_body_reranking = {     **query_body,     \"ranking.profile\": \"reranking\", } # Query with default persearch threads (set to 4) with app.syncio() as sess:     response_default = app.query(body=query_body_reranking)  # Query with num-threads-per-search overridden to 1 query_body_one_thread = {     **query_body,     \"ranking.profile\": \"one-thread-profile\",     # \"ranking.matching.numThreadsPerSearch\": 1, Could potentiall also set numThreadsPerSearch in query parameters. } with app.syncio() as sess:     response_one_thread = sess.query(body=query_body_one_thread)  # Extract query times timing_default = response_default.json[\"timing\"][\"querytime\"] timing_one_thread = response_one_thread.json[\"timing\"][\"querytime\"] # Beautifully formatted statement of - num threads and ratio of query times print(f\"Query time with 4 threads: {timing_default:.2f}s\") print(f\"Query time with 1 thread: {timing_one_thread:.2f}s\") ratio = timing_one_thread / timing_default print(f\"4 threads is approximately {ratio:.2f}x faster than 1 thread\") <pre>Query time with 4 threads: 0.72s\nQuery time with 1 thread: 1.24s\n4 threads is approximately 1.72x faster than 1 thread\n</pre> In\u00a0[16]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"advanced-configuration.html#advanced-configuration","title":"Advanced Configuration\u00b6","text":"<p>Vespa support a wide range of configuration options to customize the behavior of the system through the <code>services.xml</code>-file. Until pyvespa version 0.50.0, only a limited subset of these configurations were available in pyvespa.</p> <p>Now, we have added support for passing a <code>ServiceConfiguration</code> object to your <code>ApplicationPackage</code> that allows you to define any configuration you want. This notebook demonstrates how to use this new feature if you have the need for more advanced configurations.</p> <p>Note that it is not required to provide a <code>ServiceConfiguration</code> feature, and if not passed, the default configuration will still be created for you.</p> <p>There are some slight differences in which configuration options are available when running self-hosted (Docker) and when running on the cloud (Vespa Cloud). For details, see Vespa Cloud services.xml-reference This notebook demonstrates how to use the <code>ServiceConfiguration</code> object to configure a Vespa application for some common use cases, with options that are available in both environments.</p>"},{"location":"advanced-configuration.html#example-1-configure-document-expiry","title":"Example 1 - Configure document-expiry\u00b6","text":"<p>As an example of a common use case for advanced configuration, we will configure document-expiry. This feature allows you to set a time-to-live for documents in your Vespa application. This is useful when you have documents that are only relevant for a certain period of time, and you want to avoid serving stale data.</p> <p>For reference, see the docs on document-expiry.</p>"},{"location":"advanced-configuration.html#define-a-schema","title":"Define a schema\u00b6","text":"<p>We define a simple schema, with a timestamp field that we will use in the document selection expression to set the document-expiry.</p> <p>Note that the fields that are referenced in the selection expression should be attributes(in-memory).</p> <p>Also, either the fields should be set with <code>fast-access</code> or the number of searchable copies in the content cluster should be the same as the redundancy. Otherwise, the document selection maintenance will be slow and have a major performance impact on the system.</p>"},{"location":"advanced-configuration.html#the-serviceconfiguration-object","title":"The <code>ServiceConfiguration</code> object\u00b6","text":"<p>The <code>ServiceConfiguration</code> object allows you to define any configuration you want in the <code>services.xml</code> file.</p> <p>The syntax is as follows:</p>"},{"location":"advanced-configuration.html#tag-names","title":"Tag names\u00b6","text":"<p>All tags as referenced in the Vespa documentation are available in <code>vespa.configuration.services</code> module with the following modifications:</p> <ul> <li>All <code>-</code> in the tag names are replaced by <code>_</code> to avoid conflicts with Python syntax.</li> <li>Some tags that are Python reserved words (or commonly used objects) are constructed by adding a <code>_</code> at the end of the tag name. These are:<ul> <li><code>type_</code></li> <li><code>class_</code></li> <li><code>for_</code></li> <li><code>time_</code></li> <li><code>io_</code></li> </ul> </li> </ul> <p>Only valid tags are exported by the <code>vespa.configuration.services</code> module.</p>"},{"location":"advanced-configuration.html#attributes","title":"Attributes\u00b6","text":"<ul> <li>any attribute can be passed to the tag constructor (no validation at construction time).</li> <li>The attribute name should be the same as in the Vespa documentation, but with <code>-</code> replaced by <code>_</code>. For example, the <code>garbage-collection</code> attribute in the <code>query</code> tag should be passed as <code>garbage_collection</code>.</li> <li>In case the attribute name is a Python reserved word, the same rule as for the tag names applies (add <code>_</code> at the end). An example of this is the <code>global</code> attribute which should be passed as <code>global_</code>.</li> <li>Some attributes, such as <code>id</code>, in the <code>container</code> tag, are mandatory and should be passed as positional arguments to the tag constructor.</li> </ul>"},{"location":"advanced-configuration.html#values","title":"Values\u00b6","text":"<ul> <li>The value of an attribute can be a string, an integer, or a boolean. For types <code>bool</code> and <code>int</code>, the value is converted to a string (lowercased for <code>bool</code>). If you need to pass a float, you should convert it to a string before passing it to the tag constructor, e.g. <code>container(version=\"1.0\")</code>.</li> <li>Note that we are not escaping the values. In the xml file, the value of the <code>selection</code> attribute in the <code>document</code> tag is <code>music.timestamp &amp;gt; now() - 86400</code>. (<code>&amp;gt;</code> is the escaped form of <code>&gt;</code>.) When passing this value to the <code>document</code> tag constructor in python, we should not escape the <code>&gt;</code> character, i.e. <code>document(selection=\"music.timestamp &gt; now() - 86400\")</code>.</li> </ul>"},{"location":"advanced-configuration.html#deploy-the-vespa-application","title":"Deploy the Vespa application\u00b6","text":"<p>Deploy <code>package</code> on the local machine using Docker, without leaving the notebook, by creating an instance of VespaDocker. <code>VespaDocker</code> connects to the local Docker daemon socket and starts the Vespa docker image.</p> <p>If this step fails, please check that the Docker daemon is running, and that the Docker daemon socket can be used by clients (Configurable under advanced settings in Docker Desktop).</p>"},{"location":"advanced-configuration.html#feeding-documents-to-vespa","title":"Feeding documents to Vespa\u00b6","text":"<p>Now, let us feed some documents to Vespa. We will feed one document with a timestamp of 24 hours (+1 sec (86401)) ago and another document with a timestamp of the current time. We will then query the documents to check verify that the document-expiry is working as expected.</p>"},{"location":"advanced-configuration.html#verify-document-expiry-through-visiting","title":"Verify document expiry through visiting\u00b6","text":"<p>Visiting is a feature to efficiently get or process a set of documents, identified by a document selection expression. Here is how you can use visiting in pyvespa:</p>"},{"location":"advanced-configuration.html#clean-up","title":"Clean up\u00b6","text":""},{"location":"advanced-configuration.html#example-2-configuring-requestthreads-per-search","title":"Example 2 - Configuring <code>requestthreads</code> per search\u00b6","text":"<p>In Vespa, there are several configuration options that might be tuned to optimize the serving latency of your application. For an overview, see the Vespa documentation - Vespa Serving Scaling Guide. An example of a configuration that one might want to tune is the <code>requestthreads</code> <code>persearch</code> parameter. This parameter controls the number of search threads that are used to handle each search on the content nodes. The default value is 1.</p> <p>For some applications, where a significant portion of the work per query is linear with the number of documents, increasing the number of <code>requestthreads</code> <code>persearch</code> can improve the serving latency, as it allows more parallelism in the search phase.</p> <p>Examples of potentially expensive work that scales linearly with the number of documents, and thus are likely to benefit from increasing <code>requestthreads</code> <code>persearch</code> are: - Xgboost inference with a large GDBT-model - ONNX inference, e.g with a crossencoder. - MaxSim-operations for late interaction scoring, as in ColBERT and ColPali. - Exact nearest neighbor search.</p> <p>Example of query operators that are less likely to benefit from increasing <code>requestthreads</code> <code>persearch</code> are: - <code>wand</code>/<code>weakAnd</code>, see Using wand with Vespa. - Approximate nearest neighbor search with HNSW.</p> <p>In this example, we will demonstrate an example of configuring <code>requestthreads</code> <code>persearch</code> to 4 for an application where a Crossencoder is used in first-phase ranking. The demo is based on the Cross-encoders for global reranking guide, but here we will use a cross-encoder in first-phase instead of global-phase. First-phase and second-phase ranking are executed on the content nodes, while global-phase ranking is executed on the container node. See Phased ranking for more details.</p>"},{"location":"advanced-configuration.html#download-the-crossencoder-model","title":"Download the crossencoder-model\u00b6","text":""},{"location":"advanced-configuration.html#define-a-schema","title":"Define a schema\u00b6","text":""},{"location":"advanced-configuration.html#define-the-servicesconfiguration","title":"Define the ServicesConfiguration\u00b6","text":"<p>Note that the ServicesConfiguration may be used to define any configuration in the <code>services.xml</code> file. In this example, we are only configuring the <code>requestthreads</code> <code>persearch</code> parameter, but you can use the same approach to configure any other parameter.</p> <p>For a full reference of the available configuration options, see the Vespa documentation - services.xml.</p>"},{"location":"advanced-configuration.html#deploy-the-application-package","title":"Deploy the application package\u00b6","text":""},{"location":"advanced-configuration.html#feed-some-sample-documents","title":"Feed some sample documents\u00b6","text":""},{"location":"advanced-configuration.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"advanced-configuration.html#next-steps","title":"Next steps\u00b6","text":"<p>This is just an intro into to the advanced configuration options available in Vespa. For more details, see the Vespa documentation.</p>"},{"location":"application-packages.html","title":"Application packages","text":"Refer to troubleshooting     for any problem when running this guide.  In\u00a0[\u00a0]: Copied! <pre>!pip3 install pyvespa\n</pre> !pip3 install pyvespa <p>By exporting to disk, one can see the generated files:</p> In\u00a0[50]: Copied! <pre>import os\nimport tempfile\nfrom pathlib import Path\nfrom vespa.package import ApplicationPackage\n\napp_name = \"myschema\"\napp_package = ApplicationPackage(name=app_name, create_query_profile_by_default=False)\n\ntemp_dir = tempfile.TemporaryDirectory()\napp_package.to_files(temp_dir.name)\n\nfor p in Path(temp_dir.name).rglob(\"*\"):\n    if p.is_file():\n        print(p)\n</pre> import os import tempfile from pathlib import Path from vespa.package import ApplicationPackage  app_name = \"myschema\" app_package = ApplicationPackage(name=app_name, create_query_profile_by_default=False)  temp_dir = tempfile.TemporaryDirectory() app_package.to_files(temp_dir.name)  for p in Path(temp_dir.name).rglob(\"*\"):     if p.is_file():         print(p) <pre>/var/folders/9_/z105jyln7jz8h2vwsrjb7kxh0000gp/T/tmp6geo2dpg/services.xml\n/var/folders/9_/z105jyln7jz8h2vwsrjb7kxh0000gp/T/tmp6geo2dpg/schemas/myschema.sd\n</pre> In\u00a0[51]: Copied! <pre>os.environ[\"TMP_APP_DIR\"] = temp_dir.name\nos.environ[\"APP_NAME\"] = app_name\n\n!cat $TMP_APP_DIR/schemas/$APP_NAME.sd\n</pre> os.environ[\"TMP_APP_DIR\"] = temp_dir.name os.environ[\"APP_NAME\"] = app_name  !cat $TMP_APP_DIR/schemas/$APP_NAME.sd <pre>schema myschema {\r\n    document myschema {\r\n    }\r\n}</pre> <p>Configure the schema with fields, fieldsets and a ranking function:</p> In\u00a0[52]: Copied! <pre>from vespa.package import Field, FieldSet, RankProfile\n\napp_package.schema.add_fields(\n    Field(name=\"id\", type=\"string\", indexing=[\"attribute\", \"summary\"]),\n    Field(\n        name=\"title\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n    ),\n    Field(\n        name=\"body\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n    ),\n)\n\napp_package.schema.add_field_set(FieldSet(name=\"default\", fields=[\"title\", \"body\"]))\n\napp_package.schema.add_rank_profile(\n    RankProfile(name=\"default\", first_phase=\"bm25(title) + bm25(body)\")\n)\n</pre> from vespa.package import Field, FieldSet, RankProfile  app_package.schema.add_fields(     Field(name=\"id\", type=\"string\", indexing=[\"attribute\", \"summary\"]),     Field(         name=\"title\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"     ),     Field(         name=\"body\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"     ), )  app_package.schema.add_field_set(FieldSet(name=\"default\", fields=[\"title\", \"body\"]))  app_package.schema.add_rank_profile(     RankProfile(name=\"default\", first_phase=\"bm25(title) + bm25(body)\") ) <p>Export the application package again, show schema:</p> In\u00a0[53]: Copied! <pre>app_package.to_files(temp_dir.name)\n\n!cat $TMP_APP_DIR/schemas/$APP_NAME.sd\n</pre> app_package.to_files(temp_dir.name)  !cat $TMP_APP_DIR/schemas/$APP_NAME.sd <pre>schema myschema {\r\n    document myschema {\r\n        field id type string {\r\n            indexing: attribute | summary\r\n        }\r\n        field title type string {\r\n            indexing: index | summary\r\n            index: enable-bm25\r\n        }\r\n        field body type string {\r\n            indexing: index | summary\r\n            index: enable-bm25\r\n        }\r\n    }\r\n    fieldset default {\r\n        fields: title, body\r\n    }\r\n    rank-profile default {\r\n        first-phase {\r\n            expression {\r\n                bm25(title) + bm25(body)\r\n            }\r\n        }\r\n    }\r\n}</pre> In\u00a0[54]: Copied! <pre>!cat $TMP_APP_DIR/services.xml\n</pre> !cat $TMP_APP_DIR/services.xml <pre>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\r\n&lt;services version=\"1.0\"&gt;\r\n    &lt;container id=\"myschema_container\" version=\"1.0\"&gt;\r\n        &lt;search&gt;&lt;/search&gt;\r\n        &lt;document-api&gt;&lt;/document-api&gt;\r\n    &lt;/container&gt;\r\n    &lt;content id=\"myschema_content\" version=\"1.0\"&gt;\r\n        &lt;redundancy reply-after=\"1\"&gt;1&lt;/redundancy&gt;\r\n        &lt;documents&gt;\r\n            &lt;document type=\"myschema\" mode=\"index\"&gt;&lt;/document&gt;\r\n        &lt;/documents&gt;\r\n        &lt;nodes&gt;\r\n            &lt;node distribution-key=\"0\" hostalias=\"node1\"&gt;&lt;/node&gt;\r\n        &lt;/nodes&gt;\r\n    &lt;/content&gt;\r\n&lt;/services&gt;</pre> <p>Observe:</p> <ul> <li>A content cluster (this is where the index is stored) called <code>myschema_content</code> is created. This is information not normally needed, unless using delete_all_docs to quickly remove all documents from a schema</li> </ul> In\u00a0[55]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_container = VespaDocker()\nvespa_connection = vespa_container.deploy(application_package=app_package)\n</pre> from vespa.deployment import VespaDocker  vespa_container = VespaDocker() vespa_connection = vespa_container.deploy(application_package=app_package) <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nWaiting for application status, 0/300 seconds...\nWaiting for application status, 5/300 seconds...\nWaiting for application status, 10/300 seconds...\nWaiting for application status, 15/300 seconds...\nWaiting for application status, 20/300 seconds...\nWaiting for application status, 25/300 seconds...\nFinished deployment.\n</pre> In\u00a0[56]: Copied! <pre>%%sh\ncat &lt;&lt; EOF &gt; $TMP_APP_DIR/services.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;services version=\"1.0\"&gt;\n    &lt;container id=\"${APP_NAME}_container\" version=\"1.0\"&gt;\n        &lt;search&gt;&lt;/search&gt;\n        &lt;document-api&gt;&lt;/document-api&gt;\n    &lt;/container&gt;\n    &lt;content id=\"${APP_NAME}_content\" version=\"1.0\"&gt;\n        &lt;redundancy reply-after=\"1\"&gt;1&lt;/redundancy&gt;\n        &lt;documents&gt;\n            &lt;document type=\"${APP_NAME}\" mode=\"index\"&gt;&lt;/document&gt;\n        &lt;/documents&gt;\n        &lt;nodes&gt;\n            &lt;node distribution-key=\"0\" hostalias=\"node1\"&gt;&lt;/node&gt;\n        &lt;/nodes&gt;\n        &lt;tuning&gt;\n            &lt;resource-limits&gt;\n                &lt;disk&gt;0.90&lt;/disk&gt;\n            &lt;/resource-limits&gt;\n        &lt;/tuning&gt;\n    &lt;/content&gt;\n&lt;/services&gt;\nEOF\n</pre> %%sh cat &lt;&lt; EOF &gt; $TMP_APP_DIR/services.xml  1 0.90  EOF <p>The resource-limits in <code>tuning/resource-limits/disk</code> configuration setting allows a higher disk usage.</p> <p>Deploy using the exported files:</p> In\u00a0[57]: Copied! <pre>vespa_connection = vespa_container.deploy_from_disk(\n    application_name=app_name, application_root=temp_dir.name\n)\n</pre> vespa_connection = vespa_container.deploy_from_disk(     application_name=app_name, application_root=temp_dir.name ) <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nWaiting for application status, 0/300 seconds...\nWaiting for application status, 5/300 seconds...\nFinished deployment.\n</pre> <p>One can also export a deployable zip-file, which can be deployed using the Vespa Cloud Console:</p> In\u00a0[58]: Copied! <pre>Path.mkdir(Path(temp_dir.name) / \"zip\", exist_ok=True, parents=True)\napp_package.to_zipfile(temp_dir.name + \"/zip/application.zip\")\n\n! find \"$TMP_APP_DIR/zip\" -type f\n</pre> Path.mkdir(Path(temp_dir.name) / \"zip\", exist_ok=True, parents=True) app_package.to_zipfile(temp_dir.name + \"/zip/application.zip\")  ! find \"$TMP_APP_DIR/zip\" -type f <pre>/var/folders/9_/z105jyln7jz8h2vwsrjb7kxh0000gp/T/tmp6geo2dpg/zip/application.zip\r\n</pre> In\u00a0[59]: Copied! <pre>temp_dir.cleanup()\nvespa_container.container.stop()\nvespa_container.container.remove()\n</pre> temp_dir.cleanup() vespa_container.container.stop() vespa_container.container.remove()"},{"location":"application-packages.html#application-packages","title":"Application packages\u00b6","text":"<p>Vespa is configured using an application package. Pyvespa provides an API to generate a deployable application package. An application package has at a minimum a schema and services.xml.</p> <p>NOTE: pyvespa generally does not support all indexing options in Vespa - it is made for easy experimentation. To configure setting an unsupported indexing option (or any other unsupported option), export the application package like above, modify the schema or other files and deploy the application package from the directory, or as a zipped file. Find more details at the end of this notebook.</p>"},{"location":"application-packages.html#schema","title":"Schema\u00b6","text":"<p>A schema is created with the same name as the application package:</p>"},{"location":"application-packages.html#services","title":"Services\u00b6","text":"<p><code>services.xml</code> configures container and content clusters - see the Vespa Overview. This is a file you will normally not change or need to know much about:</p>"},{"location":"application-packages.html#deploy","title":"Deploy\u00b6","text":"<p>After completing the code for the fields and ranking, deploy the application into a Docker container - the container is started by pyvespa:</p>"},{"location":"application-packages.html#deploy-from-modified-files","title":"Deploy from modified files\u00b6","text":"<p>To add configuration the the schema, which is not supported by the pyvespa code, export the files, modify, then deploy by using <code>deploy_from_disk</code>. This example adds custom configuration to the <code>services.xml</code> file above and deploys it:</p>"},{"location":"application-packages.html#cleanup","title":"Cleanup\u00b6","text":"<p>Remove the container resources and temporary application package file export:</p>"},{"location":"application-packages.html#next-step-deploy-feed-and-query","title":"Next step: Deploy, feed and query\u00b6","text":"<p>Once the schema is ready for deployment, decide deployment option and deploy the application package:</p> <ul> <li>Deploy to local container</li> <li>Deploy to Vespa Cloud</li> </ul> <p>Use the guides on the pyvespa site to feed and query data.</p>"},{"location":"authenticating-to-vespa-cloud.html","title":"Authenticating to Vespa Cloud","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Pre-requisite: Create a tenant at cloud.vespa.ai, save the tenant name.</p> <p></p> In\u00a0[1]: Copied! <pre>!pip3 install pyvespa vespacli\n</pre> !pip3 install pyvespa vespacli <p>For background context, it is useful to read the Vespa Cloud Security Guide.</p> Control-plane Data-plane Comments Deploy application \u2705 \u274c Modify application (re-deploy) \u2705 \u274c Add or modify data-plane certs or token(s) \u2705 \u274c Feed data \u274c \u2705 Query data \u274c \u2705 Delete data \u274c \u2705 Visiting \u274c \u2705 Monitoring \u274c \u2705 Get application package \u2705 \u274c vespa auth login \u2705 \u274c Interactive control-plane login in browser vespa auth api-key \u2705 \u274c Headless control-plane authentication with an API key generated in the Vespa Cloud console vespa auth cert \u274c \u2705 Used to generate a certificate for a data-plane connection VespaCloud \u2705 \u274c `VespaCloud` is a control-plane connection to Vespa Cloud VespaDocker \u2705 \u274c `VespaDocker` is a control-plane connection to a Vespa server running in Docker Vespa \u274c \u2705 `Vespa` is a data-plane connection to an existing Vespa application In\u00a0[2]: Copied! <pre># Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n# Replace with your application name (does not need to exist yet)\napplication = \"authnotebook\"\n</pre> # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\" # Replace with your application name (does not need to exist yet) application = \"authnotebook\" In\u00a0[3]: Copied! <pre>from vespa.package import ApplicationPackage, Field, Schema, Document\n\nschema_name = \"doc\"\n\nschema = Schema(\n    name=schema_name,\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"index\", \"summary\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"body\",\n                type=\"string\",\n                indexing=[\"index\", \"summary\"],\n                index=\"enable-bm25\",\n            ),\n        ]\n    ),\n)\n\npackage = ApplicationPackage(name=application, schema=[schema])\n</pre> from vespa.package import ApplicationPackage, Field, Schema, Document  schema_name = \"doc\"  schema = Schema(     name=schema_name,     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"index\", \"summary\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"body\",                 type=\"string\",                 indexing=[\"index\", \"summary\"],                 index=\"enable-bm25\",             ),         ]     ), )  package = ApplicationPackage(name=application, schema=[schema]) In\u00a0[4]: Copied! <pre>from vespa.deployment import VespaCloud\nfrom vespa.application import Vespa\nimport os\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,  # Note that the name cannot contain the characters `-` or `_`.\n    application=application,\n    key_content=key,  # Prefer to use  key_location=\"&lt;path-to-key-file.pem&gt;\"\n    application_package=package,\n)\n</pre> from vespa.deployment import VespaCloud from vespa.application import Vespa import os  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly   vespa_cloud = VespaCloud(     tenant=tenant_name,  # Note that the name cannot contain the characters `-` or `_`.     application=application,     key_content=key,  # Prefer to use  key_location=\"\"     application_package=package, ) <pre>Setting application...\nRunning: vespa config set application vespa-team.authnotebook\nSetting target cloud...\nRunning: vespa config set target cloud\n\nApi-key found for control plane access. Using api-key.\n</pre> <p>When you have authenticated to the control-plane of Vespa Cloud, key/cert for data-plane authentication will be generated automatically for you, if none exists.</p> <p>The <code>data-plane-public-cert.pem</code> will be added to the application package (in <code>/security/clients.pem</code> directory) that will be deployed. You should keep them safe, as any app or users that need data-plane access to your Vespa application will need them.</p> <p>For <code>dev</code>-deployments, we allow redeploying an application with a different key/cert than the previous deployment. For <code>prod</code>-deployments however, this is not allowed, and will require a <code>validation-overrides</code>-specification in the application package.</p> <p>The following will upload the application package to Vespa Cloud Dev Zone (<code>aws-us-east-1c</code>), read more about Vespa Zones. The Vespa Cloud Dev Zone is considered as a sandbox environment where resources are down-scaled and idle deployments are expired automatically. For information about production deployments, see the following docs.</p> <p>Note: Deployments to dev and perf expire after 14 days of inactivity, i.e., 14 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 14 more days.</p> In\u00a0[5]: Copied! <pre>app: Vespa = vespa_cloud.deploy()\n</pre> app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 1 of dev-aws-us-east-1c for vespa-team.authnotebook. This may take a few minutes the first time.\nINFO    [06:35:26]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:35:27]  Using CA signed certificate version 1\nINFO    [06:35:27]  Using 1 nodes in container cluster 'authnotebook_container'\nINFO    [06:35:30]  Session 309490 for tenant 'vespa-team' prepared, but activation failed: 1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:35:33]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:35:33]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:35:42]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:35:42]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:35:52]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:35:52]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:36:03]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:36:03]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:36:14]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:36:14]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:36:22]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:36:22]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:36:33]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:36:33]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:36:42]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:36:42]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:36:52]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:36:53]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:37:03]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:37:03]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:37:12]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:37:12]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:37:22]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:37:22]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:37:33]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:37:33]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:37:43]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:37:43]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:37:53]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:37:54]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:38:03]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:38:03]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:38:12]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:38:12]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:38:22]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:38:22]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:38:33]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:38:34]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:38:42]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:38:43]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:38:52]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:38:53]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:39:02]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:39:03]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:39:12]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:39:13]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:39:22]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:39:22]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:39:34]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:39:35]  Session 309490 for vespa-team.authnotebook.default activated\nINFO    [06:39:56]  ######## Details for all nodes ########\nINFO    [06:39:56]  h98612b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:39:56]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:39:56]  --- storagenode on port 19102 has not started \nINFO    [06:39:56]  --- searchnode on port 19107 has not started \nINFO    [06:39:56]  --- distributor on port 19111 has not started \nINFO    [06:39:56]  --- metricsproxy-container on port 19092 has not started \nINFO    [06:39:56]  h97566a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:39:56]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:39:56]  --- logserver-container on port 4080 has not started \nINFO    [06:39:56]  --- metricsproxy-container on port 19092 has not started \nINFO    [06:39:56]  h98840a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:39:56]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:39:56]  --- container on port 4080 has not started \nINFO    [06:39:56]  --- metricsproxy-container on port 19092 has not started \nINFO    [06:39:56]  h98621d.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:39:56]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:39:56]  --- container-clustercontroller on port 19050 has not started \nINFO    [06:39:56]  --- metricsproxy-container on port 19092 has not started \nINFO    [06:40:33]  Found endpoints:\nINFO    [06:40:33]  - dev.aws-us-east-1c\nINFO    [06:40:33]   |-- https://ea8555a9.c6970ada.z.vespa-app.cloud/ (cluster 'authnotebook_container')\nINFO    [06:40:33]  Deployment complete!\nOnly region: aws-us-east-1c available in dev environment.\nFound mtls endpoint for authnotebook_container\nURL: https://ea8555a9.c6970ada.z.vespa-app.cloud/\nApplication is up!\n</pre> <p>If the deployment failed, it is possible you forgot to add the key in the Vespa Cloud Console in the <code>vespa auth api-key</code> step above.</p> <p>If you can authenticate, you should see lines like the following</p> <pre><code> Deployment started in run 1 of dev-aws-us-east-1c for mytenant.authdemo.\n</code></pre> <p>The deployment takes a few minutes the first time while Vespa Cloud sets up the resources for your Vespa application</p> <p><code>app</code> now holds a reference to a Vespa instance. We can access the mTLS protected endpoint name using the control-plane (vespa_cloud) instance. This endpoint we can query and feed to (data plane access) using the mTLS certificate generated in previous steps.</p> In\u00a0[6]: Copied! <pre>mtls_endpoint = vespa_cloud.get_mtls_endpoint()\nmtls_endpoint\n</pre> mtls_endpoint = vespa_cloud.get_mtls_endpoint() mtls_endpoint <pre>Found mtls endpoint for authnotebook_container\nURL: https://ea8555a9.c6970ada.z.vespa-app.cloud/\n</pre> Out[6]: <pre>'https://ea8555a9.c6970ada.z.vespa-app.cloud/'</pre> In\u00a0[7]: Copied! <pre>from vespa.package import AuthClient, Parameter\n\nCLIENT_TOKEN_ID = \"pyvespa_integration\"\n# Same as token name from the Vespa Cloud Console\nauth_clients = [\n    AuthClient(\n        id=\"mtls\",  # Note that you still need to include the mtls client.\n        permissions=[\"read\", \"write\"],\n        parameters=[Parameter(\"certificate\", {\"file\": \"security/clients.pem\"})],\n    ),\n    AuthClient(\n        id=\"token\",\n        permissions=[\"read\"],\n        parameters=[Parameter(\"token\", {\"id\": CLIENT_TOKEN_ID})],\n    ),\n]\n\napp_package = ApplicationPackage(\n    name=application, schema=[schema], auth_clients=auth_clients\n)\n</pre> from vespa.package import AuthClient, Parameter  CLIENT_TOKEN_ID = \"pyvespa_integration\" # Same as token name from the Vespa Cloud Console auth_clients = [     AuthClient(         id=\"mtls\",  # Note that you still need to include the mtls client.         permissions=[\"read\", \"write\"],         parameters=[Parameter(\"certificate\", {\"file\": \"security/clients.pem\"})],     ),     AuthClient(         id=\"token\",         permissions=[\"read\"],         parameters=[Parameter(\"token\", {\"id\": CLIENT_TOKEN_ID})],     ), ]  app_package = ApplicationPackage(     name=application, schema=[schema], auth_clients=auth_clients ) <p>Notice that we added the <code>read</code> and <code>write</code> permissions to mtls clients, and only <code>read</code> to the token client.</p> <p>Make sure to restrict the permissions to suit your needs.</p> <p>Now, we can deploy a new instance of the application package with the new auth-client added:</p>      See Tenants, apps, instances     for details on terminology for Vespa Cloud.  In\u00a0[8]: Copied! <pre>instance = \"token\"\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=application,\n    key_content=key,\n    application_package=app_package,\n)\napp = vespa_cloud.deploy(instance=instance)\n</pre> instance = \"token\"  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=application,     key_content=key,     application_package=app_package, ) app = vespa_cloud.deploy(instance=instance) <pre>Setting application...\nRunning: vespa config set application vespa-team.authnotebook\nSetting target cloud...\nRunning: vespa config set target cloud\n\nApi-key found for control plane access. Using api-key.\nDeployment started in run 60 of dev-aws-us-east-1c for vespa-team.authnotebook.token. This may take a few minutes the first time.\nINFO    [06:40:38]  Deploying platform version 8.408.12 and application dev build 54 for dev-aws-us-east-1c of token ...\nINFO    [06:40:39]  Using CA signed certificate version 1\nINFO    [06:40:39]  Using 1 nodes in container cluster 'authnotebook_container'\nWARNING [06:40:41]  Auto-overriding validation which would be disallowed in production: certificate-removal: Data plane certificate(s) from cluster 'authnotebook_container' is removed (removed certificates: [CN=cloud.vespa.example]) This can cause client connection issues.. To allow this add &lt;allow until='yyyy-mm-dd'&gt;certificate-removal&lt;/allow&gt; to validation-overrides.xml, see https://docs.vespa.ai/en/reference/validation-overrides.html\nINFO    [06:40:42]  Session 309492 for tenant 'vespa-team' prepared and activated.\nINFO    [06:40:43]  ######## Details for all nodes ########\nINFO    [06:40:43]  h97526a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:40:43]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:40:43]  --- storagenode on port 19102 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  --- searchnode on port 19107 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  --- distributor on port 19111 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  --- metricsproxy-container on port 19092 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  h97566b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:40:43]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:40:43]  --- logserver-container on port 4080 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  --- metricsproxy-container on port 19092 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  h97538e.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:40:43]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:40:43]  --- container-clustercontroller on port 19050 has config generation 309492, wanted is 309492\nINFO    [06:40:43]  --- metricsproxy-container on port 19092 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  h97567a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:40:43]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:40:43]  --- container on port 4080 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  --- metricsproxy-container on port 19092 has config generation 309488, wanted is 309492\nINFO    [06:40:53]  Found endpoints:\nINFO    [06:40:53]  - dev.aws-us-east-1c\nINFO    [06:40:53]   |-- https://ab50e0c2.c6970ada.z.vespa-app.cloud/ (cluster 'authnotebook_container')\nINFO    [06:40:53]  Deployment of new application complete!\nOnly region: aws-us-east-1c available in dev environment.\nFound mtls endpoint for authnotebook_container\nURL: https://ab50e0c2.c6970ada.z.vespa-app.cloud/\nApplication is up!\n</pre> <p>Note that the connection that will be returned by default, will be the mTLS connection. If you want to get a connection using token-based authentication, you can do it like this:</p> In\u00a0[9]: Copied! <pre>token_app = vespa_cloud.get_application(\n    instance=instance,\n    endpoint_type=\"token\",\n    vespa_cloud_secret_token=os.getenv(\"VESPA_CLOUD_SECRET_TOKEN\"),\n)\n</pre> token_app = vespa_cloud.get_application(     instance=instance,     endpoint_type=\"token\",     vespa_cloud_secret_token=os.getenv(\"VESPA_CLOUD_SECRET_TOKEN\"), ) <pre>Only region: aws-us-east-1c available in dev environment.\nFound token endpoint for authnotebook_container\nURL: https://c7f94a93.c6970ada.z.vespa-app.cloud/\nApplication is up!\n</pre> In\u00a0[10]: Copied! <pre>token_app.get_application_status()\n</pre> token_app.get_application_status() Out[10]: <pre>&lt;Response [200]&gt;</pre> <p>Note that a Vespa application creates a separate URL endpoint for each auth-client added. Here is how you can retrieve the URL for the token endpoint:</p> In\u00a0[11]: Copied! <pre>token_endpoint = vespa_cloud.get_token_endpoint(instance=instance)\ntoken_endpoint\n</pre> token_endpoint = vespa_cloud.get_token_endpoint(instance=instance) token_endpoint <pre>Found token endpoint for authnotebook_container\nURL: https://c7f94a93.c6970ada.z.vespa-app.cloud/\n</pre> Out[11]: <pre>'https://c7f94a93.c6970ada.z.vespa-app.cloud/'</pre> In\u00a0[12]: Copied! <pre>import os\n\n# Get user home directory\nhome = os.path.expanduser(\"~\")\n# Vespa key/cert directory\napp_dir = f\"{home}/.vespa/{tenant_name}.{application}.default/\"\n\ncert_path = f\"{app_dir}/data-plane-public-cert.pem\"\nkey_path = f\"{app_dir}/data-plane-private-key.pem\"\n</pre> import os  # Get user home directory home = os.path.expanduser(\"~\") # Vespa key/cert directory app_dir = f\"{home}/.vespa/{tenant_name}.{application}.default/\"  cert_path = f\"{app_dir}/data-plane-public-cert.pem\" key_path = f\"{app_dir}/data-plane-private-key.pem\" In\u00a0[13]: Copied! <pre>from vespa.application import Vespa\n\napp = Vespa(url=mtls_endpoint, cert=cert_path, key=key_path)\napp.get_application_status()\n</pre> from vespa.application import Vespa  app = Vespa(url=mtls_endpoint, cert=cert_path, key=key_path) app.get_application_status() Out[13]: <pre>&lt;Response [200]&gt;</pre> In\u00a0[14]: Copied! <pre>import requests\n\nsession = requests.Session()\nsession.cert = (cert_path, key_path)\nurl = f\"{mtls_endpoint}/document/v1/doc/doc/docid/1\"\ndata = {\n    \"fields\": {\n        \"id\": \"id:doc:doc::1\",\n        \"title\": \"the title\",\n        \"body\": \"the body\",\n    }\n}\nresp = session.post(url, json=data).json()\nresp\n</pre> import requests  session = requests.Session() session.cert = (cert_path, key_path) url = f\"{mtls_endpoint}/document/v1/doc/doc/docid/1\" data = {     \"fields\": {         \"id\": \"id:doc:doc::1\",         \"title\": \"the title\",         \"body\": \"the body\",     } } resp = session.post(url, json=data).json() resp Out[14]: <pre>{'pathId': '/document/v1/doc/doc/docid/1', 'id': 'id:doc:doc::1'}</pre> In\u00a0[15]: Copied! <pre>app = Vespa(\n    url=token_endpoint, vespa_cloud_secret_token=os.getenv(\"VESPA_CLOUD_SECRET_TOKEN\")\n)\napp.get_application_status()\n</pre> app = Vespa(     url=token_endpoint, vespa_cloud_secret_token=os.getenv(\"VESPA_CLOUD_SECRET_TOKEN\") ) app.get_application_status() Out[15]: <pre>&lt;Response [200]&gt;</pre> <p>Token authentication provides an even more convenient way to authenticate to the data-plane, as you do not need to handle key/cert files, and can just add the token to the HTTP header, as shown in the example below.</p> <pre>curl -H \"Authorization: Bearer $TOKEN\" https://{endpoint}/document/v1/{document-type}/{document-id}\n</pre> In\u00a0[16]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete() <pre>Deactivated vespa-team.authnotebook in dev.aws-us-east-1c\nDeleted instance vespa-team.authnotebook.default\n</pre>"},{"location":"authenticating-to-vespa-cloud.html#authenticating-to-vespa-cloud","title":"Authenticating to Vespa Cloud\u00b6","text":"<p>Security is a top priority for the Vespa Team. We understand that as a newcomer to Vespa, the different authentication methods may not always be immediately clear.</p> <p>This notebook is intended to provide some clarity on the different authentication methods needed when interacting with Vespa Cloud for different purposes.</p>"},{"location":"authenticating-to-vespa-cloud.html#install","title":"Install\u00b6","text":"<p>Install pyvespa &gt;= 0.45 and the Vespa CLI.</p>"},{"location":"authenticating-to-vespa-cloud.html#control-plane-vs-data-plane","title":"Control-plane vs Data-plane\u00b6","text":"<p>This may be self-explanatory for some, but it is worth mentioning that Vespa Cloud has two main components: the control-plane and the data-plane, which provide access to different functionalities.</p>"},{"location":"authenticating-to-vespa-cloud.html#defining-your-application","title":"Defining your application\u00b6","text":"<p>To initialize a connection to Vespa Cloud, you need to define your tenant name and application name.</p>"},{"location":"authenticating-to-vespa-cloud.html#defining-your-application-package","title":"Defining your application package\u00b6","text":"<p>An application package is the whole Vespa application configuration. It can either be constructed directly from python (as we will do below) or initalized from a path, for example by cloning a sample application from the Vespa sample apps.</p>      Tip: You can use the command vespa clone album-recommendation my-app to clone a single sample app if you have the Vespa CLI installed.  <p>For this guide, we will create a minimal application package. See other guides for more complex examples.</p>"},{"location":"authenticating-to-vespa-cloud.html#control-plane-authentication","title":"Control-plane authentication\u00b6","text":"<p>Next, we need to authenticate to the Vespa Cloud control-plane. There are two ways to authenticate to the control-plane:</p>"},{"location":"authenticating-to-vespa-cloud.html#1-interactive-login","title":"1. Interactive login:\u00b6","text":"<p>This is the recommended way to authenticate to the control-plane. It opens a browser window for you to authenticate with either google or github.</p> <p>This method does not work on windows, currently. You can run <code>vespa auth login</code> in a terminal to authenticate first, and then use this method (which will then reuse the generated token).</p> <p>(We will not run this method here, as the notebook is run in CI, but you should run it in your local environment)</p> <pre>from vespa.deployment import VespaCloud\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=application,\n    application_package=package, # Could also initialize from application_root (path to application package)\n)\n</pre> <p>You should see something similar to this:</p> <pre><code>log\nChecking for access token in auth.json...\nAccess token expired. Please re-authenticate.\nYour Device Confirmation code is: DRDT-ZZDC\nAutomatically open confirmation page in your default browser? [Y/n] y\nOpened link in your browser: https://vespa.auth0.com/activate?user_code=DRDT-ZZDC\nWaiting for login to complete in browser ... done;1m\u28ef\nSuccess: Logged in\n auth.json created at /Users/thomas/.vespa/auth.json\nSuccessfully obtained access token for control plane access.\n</code></pre>"},{"location":"authenticating-to-vespa-cloud.html#2-api-key-authentication","title":"2. API-key authentication\u00b6","text":"<p>This is a headless way to authenticate to the control-plane.</p> <p>Note that the key must be generated, either with <code>vespa auth api-key</code> or in the Vespa Cloud console directly.</p>"},{"location":"authenticating-to-vespa-cloud.html#deploy-to-vespa-cloud","title":"Deploy to Vespa Cloud\u00b6","text":"<p>The app is now defined and ready to deploy to Vespa Cloud.</p> <p>Deploy <code>package</code> to Vespa Cloud, by creating an instance of VespaCloud:</p>"},{"location":"authenticating-to-vespa-cloud.html#data-plane-authentication","title":"Data-plane authentication\u00b6","text":"<p>As we have mentioned, there are two ways to authenticate to the data-plane:</p>"},{"location":"authenticating-to-vespa-cloud.html#1-mtls-certificate-authentication","title":"1. mTLS - Certificate authentication\u00b6","text":"<p>This is the default way to authenticate to the data-plane. It uses the certificate which was added to the application package upon deployment.</p>"},{"location":"authenticating-to-vespa-cloud.html#2-token-based-authentication","title":"2. Token-based authentication\u00b6","text":"<p>A more convenient way to authenticate to the data-plane is to use a token. A token must be generated in the Vespa Cloud console. For more details, see the Security Guide</p> <p></p> <p>Set a reasonable expiry, and copy the token to a safe place, such as for instance a passwordmanager. You will not be able to see it again.</p> <p>After the token is generated, you need to add it as an auth-client to the application you want to access.</p> <p>In pyvespa, this is done by adding the AuthClients to the application package:</p> <p>NB! - The method below applies to <code>dev</code></p> <p>The approach described above applies to <code>dev</code>-deployments. For <code>prod</code>-deployments, it is a little more complex, and you need to add the <code>AuthClients</code> to your application package like this:</p> <pre>from vespa.package import ContainerCluster\n\nauth_clients = [\n            AuthClient(\n                id=\"mtls\",\n                permissions=[\"read\"],\n                parameters=[Parameter(\"certificate\", {\"file\": \"security/clients.pem\"})],\n            ),\n            AuthClient(\n                id=\"token\",\n                permissions=[\"read\"], # Set the permissions you need\n                parameters=[Parameter(\"token\", {\"id\": CLIENT_TOKEN_ID})],\n            ),\n        ]\n# Add prod deployment config\nprod_region = \"aws-us-east-1c\"\nclusters = [\n    ContentCluster(\n        id=f\"{schema_name}_content\",\n        nodes=Nodes(count=\"2\"),\n        document_name=schema_name,\n        min_redundancy=\"2\",\n    ),\n    ContainerCluster(\n        id=f\"{schema_name}_container\",\n        nodes=Nodes(count=\"2\"),\n        auth_clients=auth_clients, # Note that the auth_clients are added here for prod deployments\n    ),\n]\nprod_region = \"aws-us-east-1c\"\ndeployment_config = DeploymentConfiguration(\n    environment=\"prod\", regions=[prod_region]\n)\napp_package = ApplicationPackage(name=application, schema=[schema], clusters=clusters, deployment=deployment_config)\n</pre> <p>See Application Package reference for more details.</p>"},{"location":"authenticating-to-vespa-cloud.html#re-connecting-to-a-deployed-application","title":"Re-connecting to a deployed application\u00b6","text":"<p>To connect to a deployed application, you can use the <code>Vespa</code> class, which is a data-plane connection to an existing Vespa application.</p> <p>The <code>Vespa</code> class requires the endpoint URL.</p> <p>Note that this class can also be instantiated without authentication, typically used if connecting to an instance running in Docker, see VespaDocker.</p>"},{"location":"authenticating-to-vespa-cloud.html#connecting-using-mtls","title":"Connecting using mTLS\u00b6","text":"<p>To connect to the Vespa application using mTLS, you must pass <code>key</code> and <code>cert</code> to the <code>Vespa</code> class. Both should be a path to the respective files, matching the cert that was added to the application package upon deployment.</p> <p>A common error is to try to regenerate the key/cert after deployment, causing a mismatch between the key/cert you are trying to authenticate with, and the cert added to the application package.</p>"},{"location":"authenticating-to-vespa-cloud.html#using-requests","title":"Using <code>requests</code>\u00b6","text":"<p>It is often overlooked that all interactions with Vespa are through HTTP-api calls, so you are free to use any HTTP client you like.</p> <p>Below is an example of how to use the <code>requests</code> library to interact with Vespa, using <code>key</code> and <code>cert</code> for authentication, and the /document/v1/ endpoint to feed data to Vespa.</p>"},{"location":"authenticating-to-vespa-cloud.html#connecting-using-token","title":"Connecting using token\u00b6","text":"<p>To connect to the Vespa application using a token, you must pass the token value to the <code>Vespa</code> class as <code>vespa_cloud_secret_token</code>.</p>"},{"location":"authenticating-to-vespa-cloud.html#using-curl","title":"Using cURL\u00b6","text":""},{"location":"authenticating-to-vespa-cloud.html#next-steps","title":"Next steps\u00b6","text":"<p>This was a guide to the different authentication methods when interacting with Vespa Cloud for different purposes.</p> <p>Try to deploy a frontend as interface to your Vespa application.</p> <p>Example of some providers are:</p> <ul> <li>Cloudflare Workers, see also https://cloud.vespa.ai/en/security/cloudflare-workers.html</li> <li>Vercel</li> <li>Railway etc.</li> </ul>"},{"location":"authenticating-to-vespa-cloud.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"evaluating-vespa-application-cloud.html","title":"Evaluating a Vespa Application","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Pre-requisite: Create a tenant at cloud.vespa.ai, save the tenant name.</p> <p></p> In\u00a0[1]: Copied! <pre>#!pip3 install pyvespa vespacli datasets pandas\n</pre> #!pip3 install pyvespa vespacli datasets pandas In\u00a0[2]: Copied! <pre># Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n# Replace with your application name (does not need to exist yet)\napplication = \"evaluation\"\nschema_name = \"doc\"\n</pre> # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\" # Replace with your application name (does not need to exist yet) application = \"evaluation\" schema_name = \"doc\" In\u00a0[3]: Copied! <pre>from vespa.package import (\n    ApplicationPackage,\n    Field,\n    Schema,\n    Document,\n    HNSW,\n    RankProfile,\n    Component,\n    Parameter,\n    FieldSet,\n    GlobalPhaseRanking,\n    Function,\n)\n\npackage = ApplicationPackage(\n    name=application,\n    schema=[\n        Schema(\n            name=schema_name,\n            document=Document(\n                fields=[\n                    Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n                    Field(\n                        name=\"text\",\n                        type=\"string\",\n                        indexing=[\"index\", \"summary\"],\n                        index=\"enable-bm25\",\n                        bolding=True,\n                    ),\n                    Field(\n                        name=\"embedding\",\n                        type=\"tensor&lt;float&gt;(x[384])\",\n                        indexing=[\n                            \"input text\",\n                            \"embed\",  # uses default model\n                            \"index\",\n                            \"attribute\",\n                        ],\n                        ann=HNSW(distance_metric=\"angular\"),\n                        is_document_field=False,\n                    ),\n                ]\n            ),\n            fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n            rank_profiles=[\n                RankProfile(\n                    name=\"bm25\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    functions=[Function(name=\"bm25text\", expression=\"bm25(text)\")],\n                    first_phase=\"bm25text\",\n                    match_features=[\"bm25text\"],\n                ),\n                RankProfile(\n                    name=\"semantic\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    functions=[\n                        Function(\n                            name=\"cos_sim\", expression=\"closeness(field, embedding)\"\n                        )\n                    ],\n                    first_phase=\"cos_sim\",\n                    match_features=[\"cos_sim\"],\n                ),\n                RankProfile(\n                    name=\"fusion\",\n                    inherits=\"bm25\",\n                    functions=[\n                        Function(\n                            name=\"cos_sim\", expression=\"closeness(field, embedding)\"\n                        )\n                    ],\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    first_phase=\"cos_sim\",\n                    global_phase=GlobalPhaseRanking(\n                        expression=\"reciprocal_rank_fusion(bm25text, closeness(field, embedding))\",\n                        rerank_count=1000,\n                    ),\n                    match_features=[\"cos_sim\", \"bm25text\"],\n                ),\n                RankProfile(\n                    name=\"atan_norm\",\n                    inherits=\"bm25\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    functions=[\n                        Function(\n                            name=\"scale\",\n                            args=[\"val\"],\n                            expression=\"2*atan(val)/(3.14159)\",\n                        ),\n                        Function(\n                            name=\"normalized_bm25\", expression=\"scale(bm25(text))\"\n                        ),\n                        Function(\n                            name=\"cos_sim\", expression=\"closeness(field, embedding)\"\n                        ),\n                    ],\n                    first_phase=\"normalized_bm25\",\n                    global_phase=GlobalPhaseRanking(\n                        expression=\"normalize_linear(normalized_bm25) + normalize_linear(cos_sim)\",\n                        rerank_count=1000,\n                    ),\n                    match_features=[\"cos_sim\", \"normalized_bm25\"],\n                ),\n            ],\n        )\n    ],\n    components=[\n        Component(\n            id=\"e5\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    \"transformer-model\",\n                    {\n                        \"model-id\": \"e5-small-v2\"\n                    },  # in vespa cloud, we can use the model-id for selected models, see https://cloud.vespa.ai/en/model-hub\n                ),\n                Parameter(\n                    \"tokenizer-model\",\n                    {\"model-id\": \"e5-base-v2-vocab\"},\n                ),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import (     ApplicationPackage,     Field,     Schema,     Document,     HNSW,     RankProfile,     Component,     Parameter,     FieldSet,     GlobalPhaseRanking,     Function, )  package = ApplicationPackage(     name=application,     schema=[         Schema(             name=schema_name,             document=Document(                 fields=[                     Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),                     Field(                         name=\"text\",                         type=\"string\",                         indexing=[\"index\", \"summary\"],                         index=\"enable-bm25\",                         bolding=True,                     ),                     Field(                         name=\"embedding\",                         type=\"tensor(x[384])\",                         indexing=[                             \"input text\",                             \"embed\",  # uses default model                             \"index\",                             \"attribute\",                         ],                         ann=HNSW(distance_metric=\"angular\"),                         is_document_field=False,                     ),                 ]             ),             fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],             rank_profiles=[                 RankProfile(                     name=\"bm25\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     functions=[Function(name=\"bm25text\", expression=\"bm25(text)\")],                     first_phase=\"bm25text\",                     match_features=[\"bm25text\"],                 ),                 RankProfile(                     name=\"semantic\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     functions=[                         Function(                             name=\"cos_sim\", expression=\"closeness(field, embedding)\"                         )                     ],                     first_phase=\"cos_sim\",                     match_features=[\"cos_sim\"],                 ),                 RankProfile(                     name=\"fusion\",                     inherits=\"bm25\",                     functions=[                         Function(                             name=\"cos_sim\", expression=\"closeness(field, embedding)\"                         )                     ],                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     first_phase=\"cos_sim\",                     global_phase=GlobalPhaseRanking(                         expression=\"reciprocal_rank_fusion(bm25text, closeness(field, embedding))\",                         rerank_count=1000,                     ),                     match_features=[\"cos_sim\", \"bm25text\"],                 ),                 RankProfile(                     name=\"atan_norm\",                     inherits=\"bm25\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     functions=[                         Function(                             name=\"scale\",                             args=[\"val\"],                             expression=\"2*atan(val)/(3.14159)\",                         ),                         Function(                             name=\"normalized_bm25\", expression=\"scale(bm25(text))\"                         ),                         Function(                             name=\"cos_sim\", expression=\"closeness(field, embedding)\"                         ),                     ],                     first_phase=\"normalized_bm25\",                     global_phase=GlobalPhaseRanking(                         expression=\"normalize_linear(normalized_bm25) + normalize_linear(cos_sim)\",                         rerank_count=1000,                     ),                     match_features=[\"cos_sim\", \"normalized_bm25\"],                 ),             ],         )     ],     components=[         Component(             id=\"e5\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(                     \"transformer-model\",                     {                         \"model-id\": \"e5-small-v2\"                     },  # in vespa cloud, we can use the model-id for selected models, see https://cloud.vespa.ai/en/model-hub                 ),                 Parameter(                     \"tokenizer-model\",                     {\"model-id\": \"e5-base-v2-vocab\"},                 ),             ],         )     ], ) <p>Note that the name cannot have <code>-</code> or <code>_</code>.</p> In\u00a0[4]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=application,\n    key_content=os.getenv(\n        \"VESPA_TEAM_API_KEY\", None\n    ),  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Key is only used for CI/CD. Can be removed if logging in interactively  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=application,     key_content=os.getenv(         \"VESPA_TEAM_API_KEY\", None     ),  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=package, ) <pre>Setting application...\nRunning: vespa config set application vespa-team.evaluation\nSetting target cloud...\nRunning: vespa config set target cloud\n\nApi-key found for control plane access. Using api-key.\n</pre> <p>For more details on different authentication options and methods, see authenticating-to-vespa-cloud.</p> <p>The following will upload the application package to Vespa Cloud Dev Zone (<code>aws-us-east-1c</code>), read more about Vespa Zones. The Vespa Cloud Dev Zone is considered as a sandbox environment where resources are down-scaled and idle deployments are expired automatically. For information about production deployments, see the following method.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p> <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up. (Applications that for example refer to large onnx-models may take a bit longer.)</p> In\u00a0[5]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 4 of dev-aws-us-east-1c for vespa-team.evaluation. This may take a few minutes the first time.\nINFO    [14:00:58]  Deploying platform version 8.478.26 and application dev build 4 for dev-aws-us-east-1c of default ...\nINFO    [14:00:58]  Using CA signed certificate version 1\nINFO    [14:00:58]  Using 1 nodes in container cluster 'evaluation_container'\nINFO    [14:01:01]  Session 338115 for tenant 'vespa-team' prepared and activated.\nINFO    [14:01:01]  ######## Details for all nodes ########\nINFO    [14:01:01]  h113421f.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [14:01:01]  --- platform vespa/cloud-tenant-rhel8:8.478.26\nINFO    [14:01:01]  --- container-clustercontroller on port 19050 has config generation 338110, wanted is 338115\nINFO    [14:01:01]  --- metricsproxy-container on port 19092 has config generation 338110, wanted is 338115\nINFO    [14:01:01]  h113421e.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [14:01:01]  --- platform vespa/cloud-tenant-rhel8:8.478.26\nINFO    [14:01:01]  --- logserver-container on port 4080 has config generation 338110, wanted is 338115\nINFO    [14:01:01]  --- metricsproxy-container on port 19092 has config generation 338110, wanted is 338115\nINFO    [14:01:01]  h113501a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [14:01:01]  --- platform vespa/cloud-tenant-rhel8:8.478.26\nINFO    [14:01:01]  --- container on port 4080 has config generation 338110, wanted is 338115\nINFO    [14:01:01]  --- metricsproxy-container on port 19092 has config generation 338115, wanted is 338115\nINFO    [14:01:01]  h112930a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [14:01:01]  --- platform vespa/cloud-tenant-rhel8:8.478.26\nINFO    [14:01:01]  --- storagenode on port 19102 has config generation 338110, wanted is 338115\nINFO    [14:01:01]  --- searchnode on port 19107 has config generation 338115, wanted is 338115\nINFO    [14:01:01]  --- distributor on port 19111 has config generation 338110, wanted is 338115\nINFO    [14:01:01]  --- metricsproxy-container on port 19092 has config generation 338110, wanted is 338115\nINFO    [14:01:05]  Found endpoints:\nINFO    [14:01:05]  - dev.aws-us-east-1c\nINFO    [14:01:05]   |-- https://e8c40d27.ccc9bd09.z.vespa-app.cloud/ (cluster 'evaluation_container')\nINFO    [14:01:06]  Deployment of new application complete!\nOnly region: aws-us-east-1c available in dev environment.\nFound mtls endpoint for evaluation_container\nURL: https://e8c40d27.ccc9bd09.z.vespa-app.cloud/\nApplication is up!\n</pre> <p>If the deployment failed, it is possible you forgot to add the key in the Vespa Cloud Console in the <code>vespa auth api-key</code> step above.</p> <p>If you can authenticate, you should see lines like the following</p> <pre><code> Deployment started in run 1 of dev-aws-us-east-1c for mytenant.hybridsearch.\n</code></pre> <p>The deployment takes a few minutes the first time while Vespa Cloud sets up the resources for your Vespa application</p> <p><code>app</code> now holds a reference to a Vespa instance. We can access the mTLS protected endpoint name using the control-plane (vespa_cloud) instance. This endpoint we can query and feed to (data plane access) using the mTLS certificate generated in previous steps.</p> <p>See Authenticating to Vespa Cloud for details on using token authentication instead of mTLS.</p> In\u00a0[6]: Copied! <pre>from datasets import load_dataset\n\ndataset_id = \"zeta-alpha-ai/NanoMSMARCO\"\n\ndataset = load_dataset(dataset_id, \"corpus\", split=\"train\", streaming=True)\nvespa_feed = dataset.map(\n    lambda x: {\n        \"id\": x[\"_id\"],\n        \"fields\": {\"text\": x[\"text\"], \"id\": x[\"_id\"]},\n    }\n)\n</pre> from datasets import load_dataset  dataset_id = \"zeta-alpha-ai/NanoMSMARCO\"  dataset = load_dataset(dataset_id, \"corpus\", split=\"train\", streaming=True) vespa_feed = dataset.map(     lambda x: {         \"id\": x[\"_id\"],         \"fields\": {\"text\": x[\"text\"], \"id\": x[\"_id\"]},     } ) <p>Note that we are only evaluating rank strategies here, we consider it OK to use the <code>train</code> split for evaluation. If we were to make changes to our ranking strategies, such as adding weighting terms, or training ml models for ranking, we would suggest to adopt a <code>train</code>, <code>validation</code>, <code>test</code> split approach to avoid overfitting.</p> In\u00a0[7]: Copied! <pre>query_ds = load_dataset(dataset_id, \"queries\", split=\"train\")\nqrels = load_dataset(dataset_id, \"qrels\", split=\"train\")\n</pre> query_ds = load_dataset(dataset_id, \"queries\", split=\"train\") qrels = load_dataset(dataset_id, \"qrels\", split=\"train\") In\u00a0[8]: Copied! <pre>ids_to_query = dict(zip(query_ds[\"_id\"], query_ds[\"text\"]))\n</pre> ids_to_query = dict(zip(query_ds[\"_id\"], query_ds[\"text\"])) <p>Let us print the first 5 queries:</p> In\u00a0[9]: Copied! <pre>for idx, (qid, q) in enumerate(ids_to_query.items()):\n    print(f\"qid: {qid}, query: {q}\")\n    if idx == 5:\n        break\n</pre> for idx, (qid, q) in enumerate(ids_to_query.items()):     print(f\"qid: {qid}, query: {q}\")     if idx == 5:         break <pre>qid: 994479, query: which health care system provides all citizens or residents with equal access to health care services\nqid: 1009388, query: what's right in health care\nqid: 1088332, query: weather in oran\nqid: 265729, query: how long keep financial records\nqid: 1099433, query: how do hoa fees work\nqid: 200600, query: heels or heal\n</pre> In\u00a0[10]: Copied! <pre>relevant_docs = dict(zip(qrels[\"query-id\"], qrels[\"corpus-id\"]))\n</pre> relevant_docs = dict(zip(qrels[\"query-id\"], qrels[\"corpus-id\"])) <p>Let us print the first 5 query ids and their relevant documents:</p> In\u00a0[11]: Copied! <pre>for idx, (qid, doc_id) in enumerate(relevant_docs.items()):\n    print(f\"qid: {qid}, doc_id: {doc_id}\")\n    if idx == 5:\n        break\n</pre> for idx, (qid, doc_id) in enumerate(relevant_docs.items()):     print(f\"qid: {qid}, doc_id: {doc_id}\")     if idx == 5:         break <pre>qid: 994479, doc_id: 7275120\nqid: 1009388, doc_id: 7248824\nqid: 1088332, doc_id: 7094398\nqid: 265729, doc_id: 7369987\nqid: 1099433, doc_id: 7255675\nqid: 200600, doc_id: 7929603\n</pre> <p>We can see that this dataset only has one relevant document per query. The <code>VespaEvaluator</code> class handles this just fine, but you could also provide a set of relevant documents per query if there are multiple relevant docs.</p> <pre># multiple relevant docs per query\nqrels = {\n    \"q1\": {\"doc1\", \"doc2\"},\n    \"q2\": {\"doc3\", \"doc4\"},\n    # etc. \n}\n</pre> <p>Now we can feed to Vespa using <code>feed_iterable</code> which accepts any <code>Iterable</code> and an optional callback function where we can check the outcome of each operation. The application is configured to use embedding functionality, that produce a vector embedding using a concatenation of the title and the body input fields. This step may be resource intensive, depending on the model size.</p> <p>Read more about embedding inference in Vespa in the Accelerating Transformer-based Embedding Retrieval with Vespa blog post.</p> <p>Default node resources in Vespa Cloud have 2 v-cpu for the Dev Zone.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Error when feeding document {id}: {response.get_json()}\")\n\n\napp.feed_iterable(vespa_feed, schema=\"doc\", namespace=\"tutorial\", callback=callback)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Error when feeding document {id}: {response.get_json()}\")   app.feed_iterable(vespa_feed, schema=\"doc\", namespace=\"tutorial\", callback=callback) In\u00a0[14]: Copied! <pre>from vespa.evaluation import VespaEvaluator\n\n?VespaEvaluator\n</pre> from vespa.evaluation import VespaEvaluator  ?VespaEvaluator <pre>Init signature:\nVespaEvaluator(\n    queries: 'Dict[str, str]',\n    relevant_docs: 'Union[Dict[str, Set[str]], Dict[str, str]]',\n    vespa_query_fn: 'Callable[[str, int], dict]',\n    app: 'Vespa',\n    name: 'str' = '',\n    accuracy_at_k: 'List[int]' = [1, 3, 5, 10],\n    precision_recall_at_k: 'List[int]' = [1, 3, 5, 10],\n    mrr_at_k: 'List[int]' = [10],\n    ndcg_at_k: 'List[int]' = [10],\n    map_at_k: 'List[int]' = [100],\n    write_csv: 'bool' = False,\n    csv_dir: 'Optional[str]' = None,\n)\nDocstring:     \nEvaluate retrieval performance on a Vespa application.\n\nThis class:\n\n- Iterates over queries and issues them against your Vespa application.\n- Retrieves top-k documents per query (with k = max of your IR metrics).\n- Compares the retrieved documents with a set of relevant document ids.\n- Computes IR metrics: Accuracy@k, Precision@k, Recall@k, MRR@k, NDCG@k, MAP@k.\n- Logs vespa search times for each query.\n- Logs/returns these metrics.\n- Optionally writes out to CSV.\n\nExample usage::\n\n    from vespa.application import Vespa\n    from vespa.evaluation import VespaEvaluator\n\n    queries = {\n        \"q1\": \"What is the best GPU for gaming?\",\n        \"q2\": \"How to bake sourdough bread?\",\n        # ...\n    }\n    relevant_docs = {\n        \"q1\": {\"d12\", \"d99\"},\n        \"q2\": {\"d101\"},\n        # ...\n    }\n    # relevant_docs can also be a dict of query_id =&gt; single relevant doc_id\n    # relevant_docs = {\n    #     \"q1\": \"d12\",\n    #     \"q2\": \"d101\",\n    #     # ...\n    # }\n\n    def my_vespa_query_fn(query_text: str, top_k: int) -&gt; dict:\n        return {\n            \"yql\": 'select * from sources * where userInput(\"' + query_text + '\");',\n            \"hits\": top_k,\n            \"ranking\": \"your_ranking_profile\",\n        }\n\n    app = Vespa(url=\"http://localhost\", port=8080)\n\n    evaluator = VespaEvaluator(\n        queries=queries,\n        relevant_docs=relevant_docs,\n        vespa_query_fn=my_vespa_query_fn,\n        app=app,\n        name=\"test-run\",\n        accuracy_at_k=[1, 3, 5],\n        precision_recall_at_k=[1, 3, 5],\n        mrr_at_k=[10],\n        ndcg_at_k=[10],\n        map_at_k=[100],\n        write_csv=True\n    )\n\n    results = evaluator()\n    print(\"Primary metric:\", evaluator.primary_metric)\n    print(\"All results:\", results)\nInit docstring:\n:param queries: Dict of query_id =&gt; query text\n:param relevant_docs: Dict of query_id =&gt; set of relevant doc_ids (the user-specified part of `id:&lt;namespace&gt;:&lt;document-type&gt;:&lt;key/value-pair&gt;:&lt;user-specified&gt;` in Vespa, see https://docs.vespa.ai/en/documents.html#document-ids)\n:param vespa_query_fn: Callable, with signature: my_func(query:str, top_k: int)-&gt; dict: Given a query string and top_k, returns a Vespa query body (dict).\n:param app: A `vespa.application.Vespa` instance.\n:param name: A name or tag for this evaluation run.\n:param accuracy_at_k: list of k-values for Accuracy@k\n:param precision_recall_at_k: list of k-values for Precision@k and Recall@k\n:param mrr_at_k: list of k-values for MRR@k\n:param ndcg_at_k: list of k-values for NDCG@k\n:param map_at_k: list of k-values for MAP@k\n:param write_csv: If True, writes results to CSV\n:param csv_dir: Path in which to write the CSV file (default: current working dir).\nFile:           ~/Repos/pyvespa/vespa/evaluation.py\nType:           type\nSubclasses:     </pre> <p>We now have created the app, the queries, and the relevant documents. The only thing missing before we can initialize the <code>VespaEvaluator</code> is the ranking strategies we want to evaluate. Each of them is passed as <code>vespa_query_fn</code>.</p> <p>We will use the <code>vespa.querybuilder</code> module to create the queries. See reference doc and example notebook for more details on usage.</p> <p>This module is a Python wrapper around the Vespa Query Language (YQL), which is an alternative to providing the YQL query as a string directly.</p> In\u00a0[15]: Copied! <pre>import vespa.querybuilder as qb\n\n\ndef semantic_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": str(\n            qb.select(\"*\")\n            .from_(schema_name)\n            .where(\n                qb.nearestNeighbor(\n                    field=\"embedding\",\n                    query_vector=\"q\",\n                    annotations={\"targetHits\": 1000},\n                )\n            )\n        ),\n        \"query\": query_text,\n        \"ranking\": \"semantic\",\n        \"input.query(q)\": f\"embed({query_text})\",\n        \"hits\": top_k,\n    }\n\n\ndef bm25_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": \"select * from sources * where userQuery();\",  # provide the yql directly as a string\n        \"query\": query_text,\n        \"ranking\": \"bm25\",\n        \"hits\": top_k,\n    }\n\n\ndef fusion_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": str(\n            qb.select(\"*\")\n            .from_(schema_name)\n            .where(\n                qb.nearestNeighbor(\n                    field=\"embedding\",\n                    query_vector=\"q\",\n                    annotations={\"targetHits\": 1000},\n                )\n                | qb.userQuery(query_text)\n            )\n        ),\n        \"query\": query_text,\n        \"ranking\": \"fusion\",\n        \"input.query(q)\": f\"embed({query_text})\",\n        \"hits\": top_k,\n    }\n\n\ndef atan_norm_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": str(\n            qb.select(\"*\")\n            .from_(schema_name)\n            .where(\n                qb.nearestNeighbor(\n                    field=\"embedding\",\n                    query_vector=\"q\",\n                    annotations={\"targetHits\": 1000},\n                )\n                | qb.userQuery(query_text)\n            )\n        ),\n        \"query\": query_text,\n        \"ranking\": \"atan_norm\",\n        \"input.query(q)\": f\"embed({query_text})\",\n        \"hits\": top_k,\n    }\n</pre> import vespa.querybuilder as qb   def semantic_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": str(             qb.select(\"*\")             .from_(schema_name)             .where(                 qb.nearestNeighbor(                     field=\"embedding\",                     query_vector=\"q\",                     annotations={\"targetHits\": 1000},                 )             )         ),         \"query\": query_text,         \"ranking\": \"semantic\",         \"input.query(q)\": f\"embed({query_text})\",         \"hits\": top_k,     }   def bm25_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": \"select * from sources * where userQuery();\",  # provide the yql directly as a string         \"query\": query_text,         \"ranking\": \"bm25\",         \"hits\": top_k,     }   def fusion_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": str(             qb.select(\"*\")             .from_(schema_name)             .where(                 qb.nearestNeighbor(                     field=\"embedding\",                     query_vector=\"q\",                     annotations={\"targetHits\": 1000},                 )                 | qb.userQuery(query_text)             )         ),         \"query\": query_text,         \"ranking\": \"fusion\",         \"input.query(q)\": f\"embed({query_text})\",         \"hits\": top_k,     }   def atan_norm_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": str(             qb.select(\"*\")             .from_(schema_name)             .where(                 qb.nearestNeighbor(                     field=\"embedding\",                     query_vector=\"q\",                     annotations={\"targetHits\": 1000},                 )                 | qb.userQuery(query_text)             )         ),         \"query\": query_text,         \"ranking\": \"atan_norm\",         \"input.query(q)\": f\"embed({query_text})\",         \"hits\": top_k,     } In\u00a0[16]: Copied! <pre>from vespa.io import VespaQueryResponse\n\nresponse: VespaQueryResponse = app.query(\n    body=atan_norm_query_fn(\"how to bake a cake\", 3)\n)\n</pre> from vespa.io import VespaQueryResponse  response: VespaQueryResponse = app.query(     body=atan_norm_query_fn(\"how to bake a cake\", 3) ) In\u00a0[17]: Copied! <pre>response.get_json()\n</pre> response.get_json() Out[17]: <pre>{'root': {'id': 'toplevel',\n  'relevance': 1.0,\n  'fields': {'totalCount': 1911},\n  'coverage': {'coverage': 100,\n   'documents': 5043,\n   'full': True,\n   'nodes': 1,\n   'results': 1,\n   'resultsFull': 1},\n  'children': [{'id': 'id:tutorial:doc::1365411',\n    'relevance': 1.9456371356183069,\n    'source': 'evaluation_content',\n    'fields': {'matchfeatures': {'cos_sim': 0.6327433624922647,\n      'normalized_bm25': 0.9258703382800164},\n     'sddocname': 'doc',\n     'text': 'Cooked. 1  Beef: 2 &lt;hi&gt;to&lt;/hi&gt; 3 months. 2  Breads and &lt;hi&gt;cakes&lt;/hi&gt;: 3 months. 3  Casseroles: 3 months.  Chicken pieces: 4 1  months. Hard sausage (pepperoni): 1 &lt;hi&gt;to&lt;/hi&gt; 2 months.  Vegetable or meat soups and stews: 2 &lt;hi&gt;to&lt;/hi&gt; 3 months.',\n     'documentid': 'id:tutorial:doc::1365411',\n     'id': '1365411'}},\n   {'id': 'id:tutorial:doc::5144958',\n    'relevance': 1.9291597227244408,\n    'source': 'evaluation_content',\n    'fields': {'matchfeatures': {'cos_sim': 0.6306017579465462,\n      'normalized_bm25': 0.9296623243800929},\n     'sddocname': 'doc',\n     'text': 'Arrange 2 pounds of Italian sausages over the mixture, browned first if desired. &lt;hi&gt;To&lt;/hi&gt; brown, use &lt;hi&gt;a&lt;/hi&gt; heavy skillet over high heat and cook the sausages, turning often, for about eight minutes. After arranging the sausages over the vegetable mixture, put the &lt;hi&gt;baking&lt;/hi&gt; dish in an oven at 350 F and allow &lt;hi&gt;to&lt;/hi&gt; &lt;hi&gt;bake&lt;/hi&gt; for 45 minutes.',\n     'documentid': 'id:tutorial:doc::5144958',\n     'id': '5144958'}},\n   {'id': 'id:tutorial:doc::8204644',\n    'relevance': 1.8018643812627477,\n    'source': 'evaluation_content',\n    'fields': {'matchfeatures': {'cos_sim': 0.615268931172449,\n      'normalized_bm25': 0.9522807125125186},\n     'sddocname': 'doc',\n     'text': 'pl. bis\u00b7cuits. 1  &lt;hi&gt;A&lt;/hi&gt; small &lt;hi&gt;cake&lt;/hi&gt; of shortened bread leavened with &lt;hi&gt;baking&lt;/hi&gt; powder or soda. 2  Chiefly British &lt;hi&gt;a&lt;/hi&gt;. &lt;hi&gt;A&lt;/hi&gt; thin, crisp cracker. b. 3  &lt;hi&gt;A&lt;/hi&gt; hard, dry cracker given &lt;hi&gt;to&lt;/hi&gt; dogs as &lt;hi&gt;a&lt;/hi&gt; treat or dietary supplement. 4  &lt;hi&gt;A&lt;/hi&gt; thin, often oblong, waferlike piece of wood, glued into slots &lt;hi&gt;to&lt;/hi&gt; connect larger pieces of wood in &lt;hi&gt;a&lt;/hi&gt; joint. 5  &lt;hi&gt;A&lt;/hi&gt; pale brown.',\n     'documentid': 'id:tutorial:doc::8204644',\n     'id': '8204644'}}]}}</pre> In\u00a0[20]: Copied! <pre>all_results = {}\nfor evaluator_name, query_fn in [\n    (\"semantic\", semantic_query_fn),\n    (\"bm25\", bm25_query_fn),\n    (\"fusion\", fusion_query_fn),\n    (\"atan_norm\", atan_norm_query_fn),\n]:\n    print(f\"Evaluating {evaluator_name}...\")\n    evaluator = VespaEvaluator(\n        queries=ids_to_query,\n        relevant_docs=relevant_docs,\n        vespa_query_fn=query_fn,\n        app=app,\n        name=evaluator_name,\n        write_csv=True,  # optionally write metrics to CSV\n    )\n\n    results = evaluator.run()\n    all_results[evaluator_name] = results\n</pre> all_results = {} for evaluator_name, query_fn in [     (\"semantic\", semantic_query_fn),     (\"bm25\", bm25_query_fn),     (\"fusion\", fusion_query_fn),     (\"atan_norm\", atan_norm_query_fn), ]:     print(f\"Evaluating {evaluator_name}...\")     evaluator = VespaEvaluator(         queries=ids_to_query,         relevant_docs=relevant_docs,         vespa_query_fn=query_fn,         app=app,         name=evaluator_name,         write_csv=True,  # optionally write metrics to CSV     )      results = evaluator.run()     all_results[evaluator_name] = results <pre>Evaluating semantic...\nEvaluating bm25...\n</pre> <pre>/Users/thomas/.local/share/uv/python/cpython-3.10.14-macos-aarch64-none/lib/python3.10/json/decoder.py:353: RuntimeWarning: coroutine 'Vespa.feed_async_iterable.&lt;locals&gt;.run' was never awaited\n  obj, end = self.scan_once(s, idx)\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n</pre> <pre>Evaluating fusion...\nEvaluating atan_norm...\n</pre> In\u00a0[22]: Copied! <pre>import pandas as pd\n\nresults = pd.DataFrame(all_results)\n</pre> import pandas as pd  results = pd.DataFrame(all_results) In\u00a0[23]: Copied! <pre># take out all rows with \"searchtime\" to a separate dataframe\nsearchtime = results[results.index.str.contains(\"searchtime\")]\nresults = results[~results.index.str.contains(\"searchtime\")]\n\n\n# Highlight the maximum value in each row\ndef highlight_max(s):\n    is_max = s == s.max()\n    return [\"background-color: lightgreen; color: black;\" if v else \"\" for v in is_max]\n\n\n# Style the DataFrame: Highlight max values and format numbers to 4 decimals\nstyled_df = results.style.apply(highlight_max, axis=1).format(\"{:.4f}\")\nstyled_df\n</pre> # take out all rows with \"searchtime\" to a separate dataframe searchtime = results[results.index.str.contains(\"searchtime\")] results = results[~results.index.str.contains(\"searchtime\")]   # Highlight the maximum value in each row def highlight_max(s):     is_max = s == s.max()     return [\"background-color: lightgreen; color: black;\" if v else \"\" for v in is_max]   # Style the DataFrame: Highlight max values and format numbers to 4 decimals styled_df = results.style.apply(highlight_max, axis=1).format(\"{:.4f}\") styled_df Out[23]: semantic bm25 fusion atan_norm accuracy@1 0.3800 0.3000 0.4400 0.4400 accuracy@3 0.6400 0.6000 0.6800 0.7000 accuracy@5 0.7200 0.6600 0.7200 0.7400 accuracy@10 0.8200 0.7400 0.8000 0.8400 precision@1 0.3800 0.3000 0.4400 0.4400 recall@1 0.3800 0.3000 0.4400 0.4400 precision@3 0.2133 0.2000 0.2267 0.2333 recall@3 0.6400 0.6000 0.6800 0.7000 precision@5 0.1440 0.1320 0.1440 0.1480 recall@5 0.7200 0.6600 0.7200 0.7400 precision@10 0.0820 0.0740 0.0800 0.0840 recall@10 0.8200 0.7400 0.8000 0.8400 mrr@10 0.5309 0.4499 0.5532 0.5776 ndcg@10 0.6007 0.5204 0.6129 0.6409 map@100 0.5393 0.4592 0.5634 0.5853 <p>We can see that for this particular dataset, the hybrid strategy <code>atan_norm</code> is the best across all metrics.</p> In\u00a0[24]: Copied! <pre>results.plot(kind=\"bar\", figsize=(12, 6))\n</pre> results.plot(kind=\"bar\", figsize=(12, 6)) Out[24]: <pre>&lt;Axes: &gt;</pre> In\u00a0[25]: Copied! <pre># plot search time, add (ms) to the y-axis\n# convert to ms\nsearchtime = searchtime * 1000\nsearchtime.plot(kind=\"bar\", figsize=(12, 6)).set(ylabel=\"time (ms)\")\n</pre> # plot search time, add (ms) to the y-axis # convert to ms searchtime = searchtime * 1000 searchtime.plot(kind=\"bar\", figsize=(12, 6)).set(ylabel=\"time (ms)\") Out[25]: <pre>[Text(0, 0.5, 'time (ms)')]</pre> <p>We can see that both hybrid strategies, <code>fusion</code> and <code>atan_norm</code> strategy is a bit slower on average than pure <code>bm25</code> or <code>semantic</code>, as expected.</p> <p>Depending on the latency budget of your application, this is likely still an attractive trade-off.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete() <pre>Deactivated vespa-team.evaluation in dev.aws-us-east-1c\nDeleted instance vespa-team.evaluation.default\n</pre>"},{"location":"evaluating-vespa-application-cloud.html#evaluating-a-vespa-application","title":"Evaluating a Vespa Application\u00b6","text":"<p>We are often asked by users and customers what is the best retrieval and ranking strategy for a given use case. And even though we might sometimes have an intuition, we always recommend to set up experiments and do a proper quantitative evaluation.</p> <p>Models are temporary; Evals are forever.</p> <p>-Eugene Yan</p> <p>Without a proper evaluation setup, you run the risk of settling for <code>lgtm@10</code> (looks good to me @ 10).</p> <p>Then, if you deploy your application to users, you can be sure that you will get feedback of queries that does not produce relevant results. If you then try to optimize for that without knowing whether your tweaks are actually improving the overall quality of your search, you might end up with a system that is worse than the one you started with.</p> <p></p>"},{"location":"evaluating-vespa-application-cloud.html#so-what-can-you-do","title":"So, what can you do?\u00b6","text":"<p>You can set up a proper evaluation pipeline, where you can test different ranking strategies, and see how they perform on a set of evaluation queries that act as a proxy for your real users' queries. This way, you can make informed decisions about what works best for your use case. If you collect real user interactions, it could be even better, but it is important to also keep the evaluation pipeline light enough so that you can run it both during development and in CI pipelines (possibly at different scales).</p> <p>This guide will show how you easily can evaluate a Vespa application using pyvespa's <code>VespaEvaluator</code> class.</p> <p>We will define and compare 4 different ranking strategies in this guide:</p> <ol> <li><code>bm25</code> - Keyword-based retrieval and ranking - The solid baseline.</li> <li><code>semantic</code> - Vector search using cosine similarity (using https://huggingface.co/intfloat/e5-small-v2 for embeddings)</li> <li><code>fusion</code>- Hybrid search (semantic+keyword). Combining BM25 and Semantic with reciprocal rank fusion</li> <li><code>atan_norm</code> - Hybrid search, combining BM25 and Semantic with atan normalization as described in Aapo Tanskanen's Guidebook to the State-of-the-Art Embeddings and Information Retrieval (Originally proposed by Seo et al. (2022))</li> </ol>"},{"location":"evaluating-vespa-application-cloud.html#install","title":"Install\u00b6","text":"<p>Install pyvespa &gt;= 0.53.0 and the Vespa CLI. The Vespa CLI is used for data and control plane key management (Vespa Cloud Security Guide).</p>"},{"location":"evaluating-vespa-application-cloud.html#configure-application","title":"Configure application\u00b6","text":""},{"location":"evaluating-vespa-application-cloud.html#create-an-application-package","title":"Create an application package\u00b6","text":"<p>The application package has all the Vespa configuration files - create one from scratch:</p>"},{"location":"evaluating-vespa-application-cloud.html#deploy-to-vespa-cloud","title":"Deploy to Vespa Cloud\u00b6","text":"<p>The app is now defined and ready to deploy to Vespa Cloud.</p> <p>Deploy <code>package</code> to Vespa Cloud, by creating an instance of VespaCloud:</p>"},{"location":"evaluating-vespa-application-cloud.html#getting-your-evaluation-data","title":"Getting your evaluation data\u00b6","text":"<p>For evaluating information retrieval methods, in addition to the document corpus, we also need a set of queries and a mapping from queries to relevant documents.</p> <p>For this guide, we will use the NanoMSMARCO dataset, made available on huggingface by Zeta Alpha.</p> <p>This dataset is a subset of their \ud83c\udf7aNanoBEIR-collection, with 50 queries and up to 10K documents each.</p> <p>This is a great dataset for testing and evaluating information retrieval methods quickly, as it is small and easy to work with.</p> <p>Note that for almost any real-world use case, we would recommend you to create your own evaluation dataset. See Vespa blog post on how you can get help from an LLM for this.</p> <p>Note that creating 20-50 queries and annotating relevant documents for each query can be a good start and well worth the effort.</p>"},{"location":"evaluating-vespa-application-cloud.html#vespaevaluator","title":"VespaEvaluator\u00b6","text":"<p>The <code>VespaEvaluator</code> class is a high-level API that allows you to evaluate a Vespa application using a set of queries and a mapping from queries to relevant documents. It is inspired by SentenceTransformers <code>InformationRetrievalEvaluator</code> class.</p> <p>The difference it that <code>VespaEvaluator</code> works on a retrieval and ranking system (Vespa application) instead of a single model. Your application should be fed with the document corpus in advance, instead of taking in the document corpus.</p> <p>Let us take a look at its API and documentation:</p>"},{"location":"evaluating-vespa-application-cloud.html#run-a-test-query","title":"Run a test query\u00b6","text":"<p>Great, now we have deployed the application and fed the data. Let us run a test query to see if everything is working as expected.</p>"},{"location":"evaluating-vespa-application-cloud.html#looking-at-the-results","title":"Looking at the results\u00b6","text":""},{"location":"evaluating-vespa-application-cloud.html#looking-at-searchtimes","title":"Looking at searchtimes\u00b6","text":"<p>Ranking quality is not the only thing that matters. For many applications, search time is equally important.</p>"},{"location":"evaluating-vespa-application-cloud.html#conclusion-and-next-steps","title":"Conclusion and next steps\u00b6","text":"<p>We have shown how you can evaluate a Vespa application using pyvespa's <code>VespaEvaluator</code> class. We have defined and compared 4 different ranking strategies in terms of both ranking quality and searchtime latency.</p> <p>We hope this can provide you with a good starting point for evaluating your own Vespa application.</p> <p>If you are ready to advance, you can try to optimize the ranking strategies further, by for example weighing each of the terms in the <code>atan_norm</code> strategy differently (<code>a * normalize_linear(normalized_bm25) + (1-a) * normalize_linear(cos_sim)</code>) , or by adding a crossencoder for re-ranking the top-k results.</p>"},{"location":"evaluating-vespa-application-cloud.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"getting-started-pyvespa-cloud.html","title":"Hybrid Search - Quickstart on Vespa Cloud","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Pre-requisite: Create a tenant at cloud.vespa.ai, save the tenant name.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install pyvespa vespacli\n</pre> !pip3 install pyvespa vespacli In\u00a0[2]: Copied! <pre># Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n# Replace with your application name (does not need to exist yet)\napplication = \"hybridsearch\"\n</pre> # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\" # Replace with your application name (does not need to exist yet) application = \"hybridsearch\" In\u00a0[3]: Copied! <pre>from vespa.package import (\n    ApplicationPackage,\n    Field,\n    Schema,\n    Document,\n    HNSW,\n    RankProfile,\n    Component,\n    Parameter,\n    FieldSet,\n    GlobalPhaseRanking,\n    Function,\n)\n\npackage = ApplicationPackage(\n    name=application,\n    schema=[\n        Schema(\n            name=\"doc\",\n            document=Document(\n                fields=[\n                    Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n                    Field(\n                        name=\"title\",\n                        type=\"string\",\n                        indexing=[\"index\", \"summary\"],\n                        index=\"enable-bm25\",\n                    ),\n                    Field(\n                        name=\"body\",\n                        type=\"string\",\n                        indexing=[\"index\", \"summary\"],\n                        index=\"enable-bm25\",\n                        bolding=True,\n                    ),\n                    Field(\n                        name=\"embedding\",\n                        type=\"tensor&lt;float&gt;(x[384])\",\n                        indexing=[\n                            'input title . \" \" . input body',\n                            \"embed\",\n                            \"index\",\n                            \"attribute\",\n                        ],\n                        ann=HNSW(distance_metric=\"angular\"),\n                        is_document_field=False,\n                    ),\n                ]\n            ),\n            fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"body\"])],\n            rank_profiles=[\n                RankProfile(\n                    name=\"bm25\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    functions=[\n                        Function(name=\"bm25sum\", expression=\"bm25(title) + bm25(body)\")\n                    ],\n                    first_phase=\"bm25sum\",\n                ),\n                RankProfile(\n                    name=\"semantic\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    first_phase=\"closeness(field, embedding)\",\n                ),\n                RankProfile(\n                    name=\"fusion\",\n                    inherits=\"bm25\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    first_phase=\"closeness(field, embedding)\",\n                    global_phase=GlobalPhaseRanking(\n                        expression=\"reciprocal_rank_fusion(bm25sum, closeness(field, embedding))\",\n                        rerank_count=1000,\n                    ),\n                ),\n            ],\n        )\n    ],\n    components=[\n        Component(\n            id=\"e5\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    \"transformer-model\",\n                    {\n                        \"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\"\n                    },\n                ),\n                Parameter(\n                    \"tokenizer-model\",\n                    {\n                        \"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\"\n                    },\n                ),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import (     ApplicationPackage,     Field,     Schema,     Document,     HNSW,     RankProfile,     Component,     Parameter,     FieldSet,     GlobalPhaseRanking,     Function, )  package = ApplicationPackage(     name=application,     schema=[         Schema(             name=\"doc\",             document=Document(                 fields=[                     Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),                     Field(                         name=\"title\",                         type=\"string\",                         indexing=[\"index\", \"summary\"],                         index=\"enable-bm25\",                     ),                     Field(                         name=\"body\",                         type=\"string\",                         indexing=[\"index\", \"summary\"],                         index=\"enable-bm25\",                         bolding=True,                     ),                     Field(                         name=\"embedding\",                         type=\"tensor(x[384])\",                         indexing=[                             'input title . \" \" . input body',                             \"embed\",                             \"index\",                             \"attribute\",                         ],                         ann=HNSW(distance_metric=\"angular\"),                         is_document_field=False,                     ),                 ]             ),             fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"body\"])],             rank_profiles=[                 RankProfile(                     name=\"bm25\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     functions=[                         Function(name=\"bm25sum\", expression=\"bm25(title) + bm25(body)\")                     ],                     first_phase=\"bm25sum\",                 ),                 RankProfile(                     name=\"semantic\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     first_phase=\"closeness(field, embedding)\",                 ),                 RankProfile(                     name=\"fusion\",                     inherits=\"bm25\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     first_phase=\"closeness(field, embedding)\",                     global_phase=GlobalPhaseRanking(                         expression=\"reciprocal_rank_fusion(bm25sum, closeness(field, embedding))\",                         rerank_count=1000,                     ),                 ),             ],         )     ],     components=[         Component(             id=\"e5\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(                     \"transformer-model\",                     {                         \"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\"                     },                 ),                 Parameter(                     \"tokenizer-model\",                     {                         \"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\"                     },                 ),             ],         )     ], ) <p>Note that the name cannot have <code>-</code> or <code>_</code>.</p> In\u00a0[4]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=application,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=application,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=package, ) <pre>Setting application...\nRunning: vespa config set application vespa-team.hybridsearch\nSetting target cloud...\nRunning: vespa config set target cloud\n\nApi-key found for control plane access. Using api-key.\n</pre> <p>For more details on different authentication options and methods, see authenticating-to-vespa-cloud.</p> <p>The following will upload the application package to Vespa Cloud Dev Zone (<code>aws-us-east-1c</code>), read more about Vespa Zones. The Vespa Cloud Dev Zone is considered as a sandbox environment where resources are down-scaled and idle deployments are expired automatically. For information about production deployments, see the following method.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p> <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up. (Applications that for example refer to large onnx-models may take a bit longer.)</p> In\u00a0[5]: Copied! <pre>app = vespa_cloud.deploy()\n</pre> app = vespa_cloud.deploy() <pre>Deployment started in run 7 of dev-aws-us-east-1c for vespa-team.hybridsearch. This may take a few minutes the first time.\nINFO    [07:04:51]  Deploying platform version 8.367.14 and application dev build 6 for dev-aws-us-east-1c of default ...\nINFO    [07:04:51]  Using CA signed certificate version 3\nINFO    [07:04:52]  Using 1 nodes in container cluster 'hybridsearch_container'\nINFO    [07:04:53]  Validating Onnx models memory usage for container cluster 'hybridsearch_container', percentage of available memory too low (10 &lt; 15) to avoid restart, consider a flavor with more memory to avoid this\nWARNING [07:04:53]  Auto-overriding validation which would be disallowed in production: certificate-removal: Data plane certificate(s) from cluster 'hybridsearch_container' is removed (removed certificates: [CN=cloud.vespa.example]) This can cause client connection issues.. To allow this add &lt;allow until='yyyy-mm-dd'&gt;certificate-removal&lt;/allow&gt; to validation-overrides.xml, see https://docs.vespa.ai/en/reference/validation-overrides.html\nINFO    [07:04:55]  Session 298587 for tenant 'vespa-team' prepared and activated.\nINFO    [07:04:55]  ######## Details for all nodes ########\nINFO    [07:04:55]  h94416a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:04:55]  --- platform vespa/cloud-tenant-rhel8:8.367.14\nINFO    [07:04:55]  --- container on port 4080 has config generation 298580, wanted is 298587\nINFO    [07:04:55]  --- metricsproxy-container on port 19092 has config generation 298587, wanted is 298587\nINFO    [07:04:55]  h94249f.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:04:55]  --- platform vespa/cloud-tenant-rhel8:8.367.14\nINFO    [07:04:55]  --- container-clustercontroller on port 19050 has config generation 298580, wanted is 298587\nINFO    [07:04:55]  --- metricsproxy-container on port 19092 has config generation 298580, wanted is 298587\nINFO    [07:04:55]  h93394a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:04:55]  --- platform vespa/cloud-tenant-rhel8:8.367.14\nINFO    [07:04:55]  --- logserver-container on port 4080 has config generation 298587, wanted is 298587\nINFO    [07:04:55]  --- metricsproxy-container on port 19092 has config generation 298580, wanted is 298587\nINFO    [07:04:55]  h94419a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:04:55]  --- platform vespa/cloud-tenant-rhel8:8.367.14\nINFO    [07:04:55]  --- storagenode on port 19102 has config generation 298587, wanted is 298587\nINFO    [07:04:55]  --- searchnode on port 19107 has config generation 298587, wanted is 298587\nINFO    [07:04:55]  --- distributor on port 19111 has config generation 298587, wanted is 298587\nINFO    [07:04:55]  --- metricsproxy-container on port 19092 has config generation 298587, wanted is 298587\nINFO    [07:05:02]  Found endpoints:\nINFO    [07:05:02]  - dev.aws-us-east-1c\nINFO    [07:05:02]   |-- https://f7f73182.eb1181f2.z.vespa-app.cloud/ (cluster 'hybridsearch_container')\nINFO    [07:05:02]  Deployment of new application complete!\nFound mtls endpoint for hybridsearch_container\nURL: https://f7f73182.eb1181f2.z.vespa-app.cloud/\nConnecting to https://f7f73182.eb1181f2.z.vespa-app.cloud/\nUsing mtls_key_cert Authentication against endpoint https://f7f73182.eb1181f2.z.vespa-app.cloud//ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p>If the deployment failed, it is possible you forgot to add the key in the Vespa Cloud Console in the <code>vespa auth api-key</code> step above.</p> <p>If you can authenticate, you should see lines like the following</p> <pre><code> Deployment started in run 1 of dev-aws-us-east-1c for mytenant.hybridsearch.\n</code></pre> <p>The deployment takes a few minutes the first time while Vespa Cloud sets up the resources for your Vespa application</p> <p><code>app</code> now holds a reference to a Vespa instance. We can access the mTLS protected endpoint name using the control-plane (vespa_cloud) instance. This endpoint we can query and feed to (data plane access) using the mTLS certificate generated in previous steps.</p> In\u00a0[6]: Copied! <pre>endpoint = vespa_cloud.get_mtls_endpoint()\nendpoint\n</pre> endpoint = vespa_cloud.get_mtls_endpoint() endpoint <pre>Found mtls endpoint for hybridsearch_container\nURL: https://f7f73182.eb1181f2.z.vespa-app.cloud/\n</pre> Out[6]: <pre>'https://f7f73182.eb1181f2.z.vespa-app.cloud/'</pre> In\u00a0[7]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\"BeIR/nfcorpus\", \"corpus\", split=\"corpus\", streaming=True)\nvespa_feed = dataset.map(\n    lambda x: {\n        \"id\": x[\"_id\"],\n        \"fields\": {\"title\": x[\"title\"], \"body\": x[\"text\"], \"id\": x[\"_id\"]},\n    }\n)\n</pre> from datasets import load_dataset  dataset = load_dataset(\"BeIR/nfcorpus\", \"corpus\", split=\"corpus\", streaming=True) vespa_feed = dataset.map(     lambda x: {         \"id\": x[\"_id\"],         \"fields\": {\"title\": x[\"title\"], \"body\": x[\"text\"], \"id\": x[\"_id\"]},     } ) <p>Now we can feed to Vespa using <code>feed_iterable</code> which accepts any <code>Iterable</code> and an optional callback function where we can check the outcome of each operation. The application is configured to use embedding functionality, that produce a vector embedding using a concatenation of the title and the body input fields. This step is resource intensive.</p> <p>Read more about embedding inference in Vespa in the Accelerating Transformer-based Embedding Retrieval with Vespa blog post.</p> <p>Default node resources in Vespa Cloud have 2 v-cpu for the Dev Zone.</p> In\u00a0[8]: Copied! <pre>from vespa.io import VespaResponse, VespaQueryResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Error when feeding document {id}: {response.get_json()}\")\n\n\napp.feed_iterable(vespa_feed, schema=\"doc\", namespace=\"tutorial\", callback=callback)\n</pre> from vespa.io import VespaResponse, VespaQueryResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Error when feeding document {id}: {response.get_json()}\")   app.feed_iterable(vespa_feed, schema=\"doc\", namespace=\"tutorial\", callback=callback) <pre>Using mtls_key_cert Authentication against endpoint https://f7f73182.eb1181f2.z.vespa-app.cloud//ApplicationStatus\n</pre> In\u00a0[9]: Copied! <pre>import pandas as pd\n\n\ndef display_hits_as_df(response: VespaQueryResponse, fields) -&gt; pd.DataFrame:\n    records = []\n    for hit in response.hits:\n        record = {}\n        for field in fields:\n            record[field] = hit[\"fields\"][field]\n        records.append(record)\n    return pd.DataFrame(records)\n</pre> import pandas as pd   def display_hits_as_df(response: VespaQueryResponse, fields) -&gt; pd.DataFrame:     records = []     for hit in response.hits:         record = {}         for field in fields:             record[field] = hit[\"fields\"][field]         records.append(record)     return pd.DataFrame(records) In\u00a0[10]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where userQuery() limit 5\",\n        query=query,\n        ranking=\"bm25\",\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where userQuery() limit 5\",         query=query,         ranking=\"bm25\",     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) <pre>         id                                              title\n0  MED-2450  Protective effect of fruits, vegetables and th...\n1  MED-2464  Low vegetable intake is associated with allerg...\n2  MED-1162  Pesticide residues in imported, organic, and \"...\n3  MED-2461  The association of diet with respiratory sympt...\n4  MED-2085  Antiplatelet, anticoagulant, and fibrinolytic ...\n</pre> In\u00a0[11]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where ({targetHits:5}nearestNeighbor(embedding,q)) limit 5\",\n        query=query,\n        ranking=\"semantic\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where ({targetHits:5}nearestNeighbor(embedding,q)) limit 5\",         query=query,         ranking=\"semantic\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) <pre>         id                                              title\n0  MED-5072  Lycopene-rich treatments modify noneosinophili...\n1  MED-2472  Vegan regimen with reduced medication in the t...\n2  MED-2464  Low vegetable intake is associated with allerg...\n3  MED-2458  Manipulating antioxidant intake in asthma: a r...\n4  MED-2450  Protective effect of fruits, vegetables and th...\n</pre> In\u00a0[12]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where userQuery() or ({targetHits:1000}nearestNeighbor(embedding,q)) limit 5\",\n        query=query,\n        ranking=\"fusion\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where userQuery() or ({targetHits:1000}nearestNeighbor(embedding,q)) limit 5\",         query=query,         ranking=\"fusion\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) <pre>         id                                              title\n0  MED-2464  Low vegetable intake is associated with allerg...\n1  MED-2450  Protective effect of fruits, vegetables and th...\n2  MED-2458  Manipulating antioxidant intake in asthma: a r...\n3  MED-2461  The association of diet with respiratory sympt...\n4  MED-5072  Lycopene-rich treatments modify noneosinophili...\n</pre> In\u00a0[13]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5\",\n        query=query,\n        ranking=\"fusion\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5\",         query=query,         ranking=\"fusion\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) <pre>         id                                              title\n0  MED-2464  Low vegetable intake is associated with allerg...\n1  MED-2450  Protective effect of fruits, vegetables and th...\n2  MED-2458  Manipulating antioxidant intake in asthma: a r...\n3  MED-2461  The association of diet with respiratory sympt...\n4  MED-5072  Lycopene-rich treatments modify noneosinophili...\n</pre> In\u00a0[14]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql='select * from sources * where title contains \"vegetable\" and rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5',\n        query=query,\n        ranking=\"fusion\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql='select * from sources * where title contains \"vegetable\" and rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5',         query=query,         ranking=\"fusion\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) <pre>         id                                              title\n0  MED-2464  Low vegetable intake is associated with allerg...\n1  MED-2450  Protective effect of fruits, vegetables and th...\n2  MED-3199  Potential risks resulting from fruit/vegetable...\n3  MED-2085  Antiplatelet, anticoagulant, and fibrinolytic ...\n4  MED-4496  The effect of fruit and vegetable intake on ri...\n</pre> <p>Set up a dataplane connection using the cert/key pair:</p> In\u00a0[15]: Copied! <pre>import requests\n\ncert_path = app.cert\nkey_path = app.key\nsession = requests.Session()\nsession.cert = (cert_path, key_path)\n</pre> import requests  cert_path = app.cert key_path = app.key session = requests.Session() session.cert = (cert_path, key_path) <p>Get a document from the endpoint returned when we deployed to Vespa Cloud above. PyVespa wraps the Vespa document api internally and in these examples we use the document api directly, but with the mTLS key/cert pair we used when deploying the app.</p> In\u00a0[16]: Copied! <pre>url = \"{0}/document/v1/{1}/{2}/docid/{3}\".format(endpoint, \"tutorial\", \"doc\", \"MED-10\")\ndoc = session.get(url).json()\ndoc\n</pre> url = \"{0}/document/v1/{1}/{2}/docid/{3}\".format(endpoint, \"tutorial\", \"doc\", \"MED-10\") doc = session.get(url).json() doc Out[16]: <pre>{'pathId': '/document/v1/tutorial/doc/docid/MED-10',\n 'id': 'id:tutorial:doc::MED-10',\n 'fields': {'body': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995\u20132003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08\u20139.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38\u20130.55 and HR 0.54, 95% CI 0.44\u20130.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins\u2019 effect on survival in breast cancer patients.',\n  'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland',\n  'id': 'MED-10'}}</pre> <p>Update the title and post the new version:</p> In\u00a0[17]: Copied! <pre>doc[\"fields\"][\"title\"] = \"Can you eat lobster?\"\nresponse = session.post(url, json=doc).json()\nresponse\n</pre> doc[\"fields\"][\"title\"] = \"Can you eat lobster?\" response = session.post(url, json=doc).json() response Out[17]: <pre>{'pathId': '/document/v1/tutorial/doc/docid/MED-10',\n 'id': 'id:tutorial:doc::MED-10'}</pre> <p>Get the doc again to see the new title:</p> In\u00a0[18]: Copied! <pre>doc = session.get(url).json()\ndoc\n</pre> doc = session.get(url).json() doc Out[18]: <pre>{'pathId': '/document/v1/tutorial/doc/docid/MED-10',\n 'id': 'id:tutorial:doc::MED-10',\n 'fields': {'body': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995\u20132003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08\u20139.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38\u20130.55 and HR 0.54, 95% CI 0.44\u20130.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins\u2019 effect on survival in breast cancer patients.',\n  'title': 'Can you eat lobster?',\n  'id': 'MED-10'}}</pre> In\u00a0[19]: Copied! <pre># cert_path = \"/Users/me/.vespa/mytenant.hybridsearch.default/data-plane-public-cert.pem\"\n# key_path  = \"/Users/me/.vespa/mytenant.hybridsearch.default/data-plane-private-key.pem\"\n\nfrom vespa.application import Vespa\n\nthe_app = Vespa(endpoint, cert=cert_path, key=key_path)\n\nres = the_app.query(\n    yql=\"select documentid, id, title from sources * where userQuery()\",\n    query=\"Can you eat lobster?\",\n    ranking=\"bm25\",\n)\nres.hits[0]\n</pre> # cert_path = \"/Users/me/.vespa/mytenant.hybridsearch.default/data-plane-public-cert.pem\" # key_path  = \"/Users/me/.vespa/mytenant.hybridsearch.default/data-plane-private-key.pem\"  from vespa.application import Vespa  the_app = Vespa(endpoint, cert=cert_path, key=key_path)  res = the_app.query(     yql=\"select documentid, id, title from sources * where userQuery()\",     query=\"Can you eat lobster?\",     ranking=\"bm25\", ) res.hits[0] <pre>Using mtls_key_cert Authentication against endpoint https://f7f73182.eb1181f2.z.vespa-app.cloud//ApplicationStatus\n</pre> Out[19]: <pre>{'id': 'id:tutorial:doc::MED-10',\n 'relevance': 25.27992205160453,\n 'source': 'hybridsearch_content',\n 'fields': {'documentid': 'id:tutorial:doc::MED-10',\n  'id': 'MED-10',\n  'title': 'Can you eat lobster?'}}</pre> <p>A common problem is a cert mismatch - the cert/key pair used when deployed is different than the pair used when making requests against Vespa. This will cause 40x errors.</p> <p>Make sure it is the same pair / re-create with <code>vespa auth cert -f</code> AND redeploy.</p> <p>If you re-generate a mTLS certificate pair, and use that when connecting to Vespa cloud endpoint, it will fail until you have updaded the deployment with the new public certificate.</p> In\u00a0[20]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete() <pre>Deactivated vespa-team.hybridsearch in dev.aws-us-east-1c\nDeleted instance vespa-team.hybridsearch.default\n</pre>"},{"location":"getting-started-pyvespa-cloud.html#hybrid-search-quickstart-on-vespa-cloud","title":"Hybrid Search - Quickstart on Vespa Cloud\u00b6","text":"<p>This is the same guide as getting-started-pyvespa, deploying to Vespa Cloud.</p>"},{"location":"getting-started-pyvespa-cloud.html#install","title":"Install\u00b6","text":"<p>Install pyvespa &gt;= 0.45 and the Vespa CLI. The Vespa CLI is used for data and control plane key management (Vespa Cloud Security Guide).</p>"},{"location":"getting-started-pyvespa-cloud.html#configure-application","title":"Configure application\u00b6","text":""},{"location":"getting-started-pyvespa-cloud.html#create-an-application-package","title":"Create an application package\u00b6","text":"<p>The application package has all the Vespa configuration files - create one from scratch:</p>"},{"location":"getting-started-pyvespa-cloud.html#deploy-to-vespa-cloud","title":"Deploy to Vespa Cloud\u00b6","text":"<p>The app is now defined and ready to deploy to Vespa Cloud.</p> <p>Deploy <code>package</code> to Vespa Cloud, by creating an instance of VespaCloud:</p>"},{"location":"getting-started-pyvespa-cloud.html#feeding-documents-to-vespa","title":"Feeding documents to Vespa\u00b6","text":"<p>In this example we use the HF Datasets library to stream the BeIR/nfcorpus dataset and index in our newly deployed Vespa instance. Read more about the NFCorpus:</p> <p>NFCorpus is a full-text English retrieval data set for Medical Information Retrieval.</p> <p>The following uses the stream option of datasets to stream the data without downloading all the contents locally. The <code>map</code> functionality allows us to convert the dataset fields into the expected feed format for <code>pyvespa</code> which expects a dict with the keys <code>id</code> and <code>fields</code>:</p> <p><code>{\u00a0\"id\": \"vespa-document-id\", \"fields\": {\"vespa_field\": \"vespa-field-value\"}}</code></p>"},{"location":"getting-started-pyvespa-cloud.html#querying-vespa","title":"Querying Vespa\u00b6","text":"<p>Using the Vespa Query language we can query the indexed data.</p> <ul> <li>Using a context manager <code>with app.syncio() as session</code> to handle connection pooling (best practices)</li> <li>The query method accepts any valid Vespa query api parameter in <code>**kwargs</code></li> <li>Vespa api parameter names that contains <code>.</code> must be sent as <code>dict</code> parameters in the <code>body</code> method argument</li> </ul> <p>The following searches for <code>How Fruits and Vegetables Can Treat Asthma?</code> using different retrieval and ranking strategies.</p> <p>Query the text search app using the Vespa Query language by sending the parameters to the body argument of Vespa.query.</p> <p>First we define a simple routine that will return a dataframe of the results for prettier display in the notebook.</p>"},{"location":"getting-started-pyvespa-cloud.html#plain-keyword-search","title":"Plain Keyword search\u00b6","text":"<p>The following uses plain keyword search functionality with bm25 ranking, the <code>bm25</code> rank-profile was configured in the application package to use a linear combination of the bm25 score of the query terms against the title and the body field.</p>"},{"location":"getting-started-pyvespa-cloud.html#plain-semantic-search","title":"Plain Semantic Search\u00b6","text":"<p>The following uses dense vector representations of the query and the document and matching is performed and accelerated by Vespa's support for approximate nearest neighbor search. The vector embedding representation of the text is obtained using Vespa's embedder functionality.</p>"},{"location":"getting-started-pyvespa-cloud.html#hybrid-search","title":"Hybrid Search\u00b6","text":"<p>This is one approach to combine the two retrieval strategies and where we use Vespa's support for cross-hits feature normalization and reciprocal rank fusion. This functionality is exposed in the context of <code>global</code> re-ranking, after the distributed query retrieval execution which might span 1000s of nodes.</p>"},{"location":"getting-started-pyvespa-cloud.html#hybrid-search-with-the-or-query-operator","title":"Hybrid search with the OR query operator\u00b6","text":"<p>This combines the two methods using logical disjunction (OR). Note that the first-phase expression in our <code>fusion</code> expression is only using the semantic score, this because usually semantic search provides better recall than sparse keyword search alone.</p>"},{"location":"getting-started-pyvespa-cloud.html#hybrid-search-with-the-rank-query-operator","title":"Hybrid search with the RANK query operator\u00b6","text":"<p>This combines the two methods using the rank query operator. In this case we express that we want to retrieve the top-1000 documents using vector search, and then have sparse features like BM25 calculated as well (second operand of the rank operator). Finally the hits are re-ranked using the reciprocal rank fusion</p>"},{"location":"getting-started-pyvespa-cloud.html#hybrid-search-with-filters","title":"Hybrid search with filters\u00b6","text":"<p>In this example we add another query term to the yql, restricting the nearest neighbor search to only consider documents that have vegetable in the title.</p>"},{"location":"getting-started-pyvespa-cloud.html#next-steps","title":"Next steps\u00b6","text":"<p>This is just an intro into the capabilities of Vespa and pyvespa. Browse the site to learn more about schemas, feeding and queries - find more complex applications in examples.</p>"},{"location":"getting-started-pyvespa-cloud.html#example-document-operations-using-certkey-pair","title":"Example: Document operations using cert/key pair\u00b6","text":"<p>Above, we deployed to Vespa Cloud, and as part of that, generated a data-plane mTLS cert/key pair.</p> <p>This pair can be used to access the dataplane for reads/writes to documents and running queries from many different clients. The following demonstrates that using the <code>requests</code> library.</p>"},{"location":"getting-started-pyvespa-cloud.html#example-reconnect-pyvespa-using-certkey-pair","title":"Example: Reconnect pyvespa using cert/key pair\u00b6","text":"<p>Above, we stored the dataplane credentials for later use. Deployment of an application usually happens when the schema changes, whereas accessing the dataplane is for document updates and user queries.</p> <p>One only needs to know the endpoint and the cert/key pair to enable a connection to a Vespa Cloud application:</p>"},{"location":"getting-started-pyvespa-cloud.html#delete-application","title":"Delete application\u00b6","text":"<p>The following will delete the application and data from the dev environment.</p>"},{"location":"getting-started-pyvespa.html","title":"Hybrid Search - Quickstart","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Install pyvespa and start Docker Daemon, validate minimum 6G available:</p> In\u00a0[1]: Copied! <pre>!pip3 install pyvespa\n!docker info | grep \"Total Memory\"\n</pre> !pip3 install pyvespa !docker info | grep \"Total Memory\" In\u00a0[1]: Copied! <pre>from vespa.package import (\n    ApplicationPackage,\n    Field,\n    Schema,\n    Document,\n    HNSW,\n    RankProfile,\n    Component,\n    Parameter,\n    FieldSet,\n    GlobalPhaseRanking,\n    Function,\n)\n\npackage = ApplicationPackage(\n    name=\"hybridsearch\",\n    schema=[\n        Schema(\n            name=\"doc\",\n            document=Document(\n                fields=[\n                    Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n                    Field(\n                        name=\"title\",\n                        type=\"string\",\n                        indexing=[\"index\", \"summary\"],\n                        index=\"enable-bm25\",\n                    ),\n                    Field(\n                        name=\"body\",\n                        type=\"string\",\n                        indexing=[\"index\", \"summary\"],\n                        index=\"enable-bm25\",\n                        bolding=True,\n                    ),\n                    Field(\n                        name=\"embedding\",\n                        type=\"tensor&lt;float&gt;(x[384])\",\n                        indexing=[\n                            'input title . \" \" . input body',\n                            \"embed\",\n                            \"index\",\n                            \"attribute\",\n                        ],\n                        ann=HNSW(distance_metric=\"angular\"),\n                        is_document_field=False,\n                    ),\n                ]\n            ),\n            fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"body\"])],\n            rank_profiles=[\n                RankProfile(\n                    name=\"bm25\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    functions=[\n                        Function(name=\"bm25sum\", expression=\"bm25(title) + bm25(body)\")\n                    ],\n                    first_phase=\"bm25sum\",\n                ),\n                RankProfile(\n                    name=\"semantic\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    first_phase=\"closeness(field, embedding)\",\n                ),\n                RankProfile(\n                    name=\"fusion\",\n                    inherits=\"bm25\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    first_phase=\"closeness(field, embedding)\",\n                    global_phase=GlobalPhaseRanking(\n                        expression=\"reciprocal_rank_fusion(bm25sum, closeness(field, embedding))\",\n                        rerank_count=1000,\n                    ),\n                ),\n            ],\n        )\n    ],\n    components=[\n        Component(\n            id=\"e5\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    \"transformer-model\",\n                    {\n                        \"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\"\n                    },\n                ),\n                Parameter(\n                    \"tokenizer-model\",\n                    {\n                        \"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\"\n                    },\n                ),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import (     ApplicationPackage,     Field,     Schema,     Document,     HNSW,     RankProfile,     Component,     Parameter,     FieldSet,     GlobalPhaseRanking,     Function, )  package = ApplicationPackage(     name=\"hybridsearch\",     schema=[         Schema(             name=\"doc\",             document=Document(                 fields=[                     Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),                     Field(                         name=\"title\",                         type=\"string\",                         indexing=[\"index\", \"summary\"],                         index=\"enable-bm25\",                     ),                     Field(                         name=\"body\",                         type=\"string\",                         indexing=[\"index\", \"summary\"],                         index=\"enable-bm25\",                         bolding=True,                     ),                     Field(                         name=\"embedding\",                         type=\"tensor(x[384])\",                         indexing=[                             'input title . \" \" . input body',                             \"embed\",                             \"index\",                             \"attribute\",                         ],                         ann=HNSW(distance_metric=\"angular\"),                         is_document_field=False,                     ),                 ]             ),             fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"body\"])],             rank_profiles=[                 RankProfile(                     name=\"bm25\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     functions=[                         Function(name=\"bm25sum\", expression=\"bm25(title) + bm25(body)\")                     ],                     first_phase=\"bm25sum\",                 ),                 RankProfile(                     name=\"semantic\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     first_phase=\"closeness(field, embedding)\",                 ),                 RankProfile(                     name=\"fusion\",                     inherits=\"bm25\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     first_phase=\"closeness(field, embedding)\",                     global_phase=GlobalPhaseRanking(                         expression=\"reciprocal_rank_fusion(bm25sum, closeness(field, embedding))\",                         rerank_count=1000,                     ),                 ),             ],         )     ],     components=[         Component(             id=\"e5\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(                     \"transformer-model\",                     {                         \"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\"                     },                 ),                 Parameter(                     \"tokenizer-model\",                     {                         \"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\"                     },                 ),             ],         )     ], ) <p>Note that the name cannot have <code>-</code> or <code>_</code>.</p> In\u00a0[1]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=package)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy(application_package=package) <p><code>app</code> now holds a reference to a Vespa instance.</p> In\u00a0[1]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\"BeIR/nfcorpus\", \"corpus\", split=\"corpus\", streaming=True)\nvespa_feed = dataset.map(\n    lambda x: {\n        \"id\": x[\"_id\"],\n        \"fields\": {\"title\": x[\"title\"], \"body\": x[\"text\"], \"id\": x[\"_id\"]},\n    }\n)\n</pre> from datasets import load_dataset  dataset = load_dataset(\"BeIR/nfcorpus\", \"corpus\", split=\"corpus\", streaming=True) vespa_feed = dataset.map(     lambda x: {         \"id\": x[\"_id\"],         \"fields\": {\"title\": x[\"title\"], \"body\": x[\"text\"], \"id\": x[\"_id\"]},     } ) <p>Now we can feed to Vespa using <code>feed_iterable</code> which accepts any <code>Iterable</code> and an optional callback function where we can check the outcome of each operation. The application is configured to use embedding functionality, that produce a vector embedding using a concatenation of the title and the body input fields. This step is computionally expensive. Read more about embedding inference in Vespa in the Accelerating Transformer-based Embedding Retrieval with Vespa.</p> In\u00a0[1]: Copied! <pre>from vespa.io import VespaResponse, VespaQueryResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Error when feeding document {id}: {response.get_json()}\")\n\n\napp.feed_iterable(vespa_feed, schema=\"doc\", namespace=\"tutorial\", callback=callback)\n</pre> from vespa.io import VespaResponse, VespaQueryResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Error when feeding document {id}: {response.get_json()}\")   app.feed_iterable(vespa_feed, schema=\"doc\", namespace=\"tutorial\", callback=callback) In\u00a0[1]: Copied! <pre>import pandas as pd\n\n\ndef display_hits_as_df(response: VespaQueryResponse, fields) -&gt; pd.DataFrame:\n    records = []\n    for hit in response.hits:\n        record = {}\n        for field in fields:\n            record[field] = hit[\"fields\"][field]\n        records.append(record)\n    return pd.DataFrame(records)\n</pre> import pandas as pd   def display_hits_as_df(response: VespaQueryResponse, fields) -&gt; pd.DataFrame:     records = []     for hit in response.hits:         record = {}         for field in fields:             record[field] = hit[\"fields\"][field]         records.append(record)     return pd.DataFrame(records) In\u00a0[1]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where userQuery() limit 5\",\n        query=query,\n        ranking=\"bm25\",\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where userQuery() limit 5\",         query=query,         ranking=\"bm25\",     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) In\u00a0[1]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where ({targetHits:1000}nearestNeighbor(embedding,q)) limit 5\",\n        query=query,\n        ranking=\"semantic\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where ({targetHits:1000}nearestNeighbor(embedding,q)) limit 5\",         query=query,         ranking=\"semantic\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) In\u00a0[1]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where userQuery() or ({targetHits:1000}nearestNeighbor(embedding,q)) limit 5\",\n        query=query,\n        ranking=\"fusion\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where userQuery() or ({targetHits:1000}nearestNeighbor(embedding,q)) limit 5\",         query=query,         ranking=\"fusion\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) <p>This combines the two methods using the rank query operator. In this case we express that we want to retrieve the top-1000 documents using vector search, and then have sparse features like BM25 calculated as well (second operand of the rank operator). Finally the hits are re-ranked using the reciprocal rank fusion</p> In\u00a0[1]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5\",\n        query=query,\n        ranking=\"fusion\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5\",         query=query,         ranking=\"fusion\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) In\u00a0[1]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql='select * from sources * where title contains \"vegetable\" and rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5',\n        query=query,\n        ranking=\"fusion\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql='select * from sources * where title contains \"vegetable\" and rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5',         query=query,         ranking=\"fusion\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) In\u00a0[1]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"getting-started-pyvespa.html#hybrid-search-quickstart","title":"Hybrid Search - Quickstart\u00b6","text":"<p>This tutorial creates a hybrid text search application combining traditional keyword matching with semantic vector search (dense retrieval). It also demonstrates using Vespa native embedder functionality.</p>"},{"location":"getting-started-pyvespa.html#create-an-application-package","title":"Create an application package\u00b6","text":"<p>The application package has all the Vespa configuration files - create one from scratch:</p>"},{"location":"getting-started-pyvespa.html#deploy-the-vespa-application","title":"Deploy the Vespa application\u00b6","text":"<p>Deploy <code>package</code> on the local machine using Docker, without leaving the notebook, by creating an instance of VespaDocker. <code>VespaDocker</code> connects to the local Docker daemon socket and starts the Vespa docker image.</p> <p>If this step fails, please check that the Docker daemon is running, and that the Docker daemon socket can be used by clients (Configurable under advanced settings in Docker Desktop).</p>"},{"location":"getting-started-pyvespa.html#feeding-documents-to-vespa","title":"Feeding documents to Vespa\u00b6","text":"<p>In this example we use the HF Datasets library to stream the BeIR/nfcorpus dataset and index in our newly deployed Vespa instance. Read more about the NFCorpus:</p> <p>NFCorpus is a full-text English retrieval data set for Medical Information Retrieval.</p> <p>The following uses the stream option of datasets to stream the data without downloading all the contents locally. The <code>map</code> functionality allows us to convert the dataset fields into the expected feed format for <code>pyvespa</code> which expects a dict with the keys <code>id</code> and <code>fields</code>:</p> <p><code>{\u00a0\"id\": \"vespa-document-id\", \"fields\": {\"vespa_field\": \"vespa-field-value\"}}</code></p>"},{"location":"getting-started-pyvespa.html#querying-vespa","title":"Querying Vespa\u00b6","text":"<p>Using the Vespa Query language we can query the indexed data.</p> <ul> <li>Using a context manager <code>with app.syncio() as session</code> to handle connection pooling (best practices)</li> <li>The query method accepts any valid Vespa query api parameter in <code>**kwargs</code></li> <li>Vespa api parameter names that contains <code>.</code> must be sent as <code>dict</code> parameters in the <code>body</code> method argument</li> </ul> <p>The following searches for <code>How Fruits and Vegetables Can Treat Asthma?</code> using different retrieval and ranking strategies.</p>"},{"location":"getting-started-pyvespa.html#plain-keyword-search","title":"Plain Keyword search\u00b6","text":"<p>The following uses plain keyword search functionality with bm25 ranking, the <code>bm25</code> rank-profile was configured in the application package to use a linear combination of the bm25 score of the query terms against the title and the body field.</p>"},{"location":"getting-started-pyvespa.html#plain-semantic-search","title":"Plain Semantic Search\u00b6","text":"<p>The following uses dense vector representations of the query and the document and matching is performed and accelerated by Vespa's support for approximate nearest neighbor search. The vector embedding representation of the text is obtained using Vespa's embedder functionality.</p>"},{"location":"getting-started-pyvespa.html#hybrid-search","title":"Hybrid Search\u00b6","text":"<p>This is one approach to combine the two retrieval strategies and where we use Vespa's support for cross-hits feature normalization and reciprocal rank fusion. This functionality is exposed in the context of <code>global</code> re-ranking, after the distributed query retrieval execution which might span 1000s of nodes.</p>"},{"location":"getting-started-pyvespa.html#hybrid-search-with-the-or-query-operator","title":"Hybrid search with the OR query operator\u00b6","text":"<p>This combines the two methods using logical disjunction (OR). Note that the first-phase expression in our <code>fusion</code> expression is only using the semantic score, this because usually semantic search provides better recall than sparse keyword search alone.</p>"},{"location":"getting-started-pyvespa.html#hybrid-search-with-the-rank-query-operator","title":"Hybrid search with the RANK query operator\u00b6","text":""},{"location":"getting-started-pyvespa.html#hybrid-search-with-filters","title":"Hybrid search with filters\u00b6","text":"<p>In this example we add another query term to the yql, restricting the nearest neighbor search to only consider documents that have vegetable in the title.</p>"},{"location":"getting-started-pyvespa.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"getting-started-pyvespa.html#next-steps","title":"Next steps\u00b6","text":"<p>This is just an intro into the capabilities of Vespa and pyvespa. Browse the site to learn more about schemas, feeding and queries - find more complex applications in examples.</p>"},{"location":"query.html","title":"Querying Vespa","text":"Refer to troubleshooting     for any problem when running this guide.  <p>You can run this tutorial in Google Colab:</p> <p></p> In\u00a0[1]: Copied! <pre>!pip3 install pyvespa\n</pre> !pip3 install pyvespa <p>Connect to a running Vespa instance.</p> In\u00a0[2]: Copied! <pre>from vespa.application import Vespa\nfrom vespa.io import VespaQueryResponse\nfrom vespa.exceptions import VespaError\n\napp = Vespa(url=\"https://api.cord19.vespa.ai\")\n</pre> from vespa.application import Vespa from vespa.io import VespaQueryResponse from vespa.exceptions import VespaError  app = Vespa(url=\"https://api.cord19.vespa.ai\") <p>See the Vespa query language for Vespa query api request parameters.</p> <p>The YQL userQuery() operator uses the query read from <code>query</code>. The query also specifies to use the app-specific bm25 rank profile. The code uses context manager <code>with session</code> statement to make sure that connection pools are released. If you attempt to make multiple queries, this is important as each query will not have to set up new connections.</p> In\u00a0[3]: Copied! <pre>with app.syncio() as session:\n    response: VespaQueryResponse = session.query(\n        yql=\"select documentid, cord_uid, title, abstract from sources * where userQuery()\",\n        hits=1,\n        query=\"Is remdesivir an effective treatment for COVID-19?\",\n        ranking=\"bm25\",\n    )\n    print(response.is_successful())\n    print(response.url)\n</pre> with app.syncio() as session:     response: VespaQueryResponse = session.query(         yql=\"select documentid, cord_uid, title, abstract from sources * where userQuery()\",         hits=1,         query=\"Is remdesivir an effective treatment for COVID-19?\",         ranking=\"bm25\",     )     print(response.is_successful())     print(response.url) <pre>True\nhttps://api.cord19.vespa.ai/search/?yql=select+documentid%2C+cord_uid%2C+title%2C+abstract+from+sources+%2A+where+userQuery%28%29&amp;hits=1&amp;query=Is+remdesivir+an+effective+treatment+for+COVID-19%3F&amp;ranking=bm25\n</pre> <p>Alternatively, if the native Vespa query parameter contains \".\", which cannot be used as a <code>kwarg</code>, the parameters can be sent as HTTP POST with the <code>body</code> argument. In this case, <code>ranking</code> is an alias of <code>ranking.profile</code>, but using <code>ranking.profile</code> as a <code>**kwargs</code>\u00a0argument is not allowed in python. This will combine HTTP parameters with an HTTP POST body.</p> In\u00a0[4]: Copied! <pre>with app.syncio() as session:\n    response: VespaQueryResponse = session.query(\n        hits=1,\n        body={\n            \"yql\": \"select documentid, cord_uid, title, abstract from sources * where userQuery()\",\n            \"query\": \"Is remdesivir an effective treatment for COVID-19?\",\n            \"ranking.profile\": \"bm25\",\n            \"presentation.timing\": True,\n        },\n    )\n    print(response.is_successful())\n</pre> with app.syncio() as session:     response: VespaQueryResponse = session.query(         hits=1,         body={             \"yql\": \"select documentid, cord_uid, title, abstract from sources * where userQuery()\",             \"query\": \"Is remdesivir an effective treatment for COVID-19?\",             \"ranking.profile\": \"bm25\",             \"presentation.timing\": True,         },     )     print(response.is_successful()) <pre>True\n</pre> <p>The query specified that we wanted one hit:</p> In\u00a0[5]: Copied! <pre>response.hits\n</pre> response.hits Out[5]: <pre>[{'id': 'id:covid-19:doc::534720',\n  'relevance': 26.6769101612402,\n  'source': 'content',\n  'fields': {'title': 'A Review on &lt;hi&gt;Remdesivir&lt;/hi&gt;: A Possible Promising Agent for the &lt;hi&gt;Treatment&lt;/hi&gt; of &lt;hi&gt;COVID&lt;/hi&gt;-&lt;hi&gt;19&lt;/hi&gt;',\n   'abstract': '&lt;sep /&gt;manufacturing of specific therapeutics and vaccines to treat &lt;hi&gt;COVID&lt;/hi&gt;-&lt;hi&gt;19&lt;/hi&gt; are time-consuming processes. At this time, using available conventional therapeutics along with other &lt;hi&gt;treatment&lt;/hi&gt; options may be useful to fight &lt;hi&gt;COVID&lt;/hi&gt;-&lt;hi&gt;19&lt;/hi&gt;. In different clinical trials, efficacy of &lt;hi&gt;remdesivir&lt;/hi&gt; (GS-5734) against Ebola virus has been demonstrated. Moreover, &lt;hi&gt;remdesivir&lt;/hi&gt; may be an &lt;hi&gt;effective&lt;/hi&gt; therapy in vitro and in animal models infected by SARS and MERS coronaviruses. Hence, the drug may be theoretically &lt;hi&gt;effective&lt;/hi&gt; against SARS-CoV-2. &lt;hi&gt;Remdesivir&lt;/hi&gt;&lt;sep /&gt;',\n   'documentid': 'id:covid-19:doc::534720',\n   'cord_uid': 'xej338lo'}}]</pre> <p>Example of iterating over the returned hits obtained from <code>response.hits</code>, extracting the <code>cord_uid</code> field:</p> In\u00a0[6]: Copied! <pre>[hit[\"fields\"][\"cord_uid\"] for hit in response.hits]\n</pre> [hit[\"fields\"][\"cord_uid\"] for hit in response.hits] Out[6]: <pre>['xej338lo']</pre> <p>Access the full JSON response in the Vespa default JSON result format:</p> In\u00a0[7]: Copied! <pre>response.json\n</pre> response.json Out[7]: <pre>{'timing': {'querytime': 0.007, 'summaryfetchtime': 0.0, 'searchtime': 0.008},\n 'root': {'id': 'toplevel',\n  'relevance': 1.0,\n  'fields': {'totalCount': 2373},\n  'coverage': {'coverage': 100,\n   'documents': 976355,\n   'full': True,\n   'nodes': 2,\n   'results': 1,\n   'resultsFull': 1},\n  'children': [{'id': 'id:covid-19:doc::534720',\n    'relevance': 26.6769101612402,\n    'source': 'content',\n    'fields': {'title': 'A Review on &lt;hi&gt;Remdesivir&lt;/hi&gt;: A Possible Promising Agent for the &lt;hi&gt;Treatment&lt;/hi&gt; of &lt;hi&gt;COVID&lt;/hi&gt;-&lt;hi&gt;19&lt;/hi&gt;',\n     'abstract': '&lt;sep /&gt;manufacturing of specific therapeutics and vaccines to treat &lt;hi&gt;COVID&lt;/hi&gt;-&lt;hi&gt;19&lt;/hi&gt; are time-consuming processes. At this time, using available conventional therapeutics along with other &lt;hi&gt;treatment&lt;/hi&gt; options may be useful to fight &lt;hi&gt;COVID&lt;/hi&gt;-&lt;hi&gt;19&lt;/hi&gt;. In different clinical trials, efficacy of &lt;hi&gt;remdesivir&lt;/hi&gt; (GS-5734) against Ebola virus has been demonstrated. Moreover, &lt;hi&gt;remdesivir&lt;/hi&gt; may be an &lt;hi&gt;effective&lt;/hi&gt; therapy in vitro and in animal models infected by SARS and MERS coronaviruses. Hence, the drug may be theoretically &lt;hi&gt;effective&lt;/hi&gt; against SARS-CoV-2. &lt;hi&gt;Remdesivir&lt;/hi&gt;&lt;sep /&gt;',\n     'documentid': 'id:covid-19:doc::534720',\n     'cord_uid': 'xej338lo'}}]}}</pre> In\u00a0[8]: Copied! <pre>with app.syncio(connections=12) as session:\n    response: VespaQueryResponse = session.query(\n        hits=1,\n        body={\n            \"yql\": \"select documentid, cord_uid, title, abstract from sources * where userQuery()\",\n            \"query\": \"Is remdesivir an effective treatment for COVID-19?\",\n            \"ranking.profile\": \"bm25\",\n            \"presentation.timing\": True,\n        },\n    )\n    print(response.is_successful())\n</pre> with app.syncio(connections=12) as session:     response: VespaQueryResponse = session.query(         hits=1,         body={             \"yql\": \"select documentid, cord_uid, title, abstract from sources * where userQuery()\",             \"query\": \"Is remdesivir an effective treatment for COVID-19?\",             \"ranking.profile\": \"bm25\",             \"presentation.timing\": True,         },     )     print(response.is_successful()) <pre>True\n</pre> In\u00a0[9]: Copied! <pre>import time\n\n# Will not compress the request, as body is less than 1024 bytes\nwith app.syncio(connections=1, compress=\"auto\") as session:\n    response: VespaQueryResponse = session.query(\n        hits=1,\n        body={\n            \"yql\": \"select documentid, cord_uid, title, abstract from sources * where userQuery()\",\n            \"query\": \"Is remdesivir an effective treatment for COVID-19?\",\n            \"ranking.profile\": \"bm25\",\n            \"presentation.timing\": True,\n        },\n    )\n    print(response.is_successful())\n\n# Will compress, as the size of the body exceeds 1024 bytes.\nlarge_body = {\n    \"yql\": \"select documentid, cord_uid, title, abstract from sources * where userQuery()\",\n    \"query\": \"Is remdesivir an effective treatment for COVID-19?\",\n    \"input.query(q)\": \"asdf\" * 10000,\n    \"ranking.profile\": \"bm25\",\n    \"presentation.timing\": True,\n}\ncompress_time = {}\n\nwith app.syncio(connections=1, compress=True) as session:\n    start_time = time.time()\n    response: VespaQueryResponse = session.query(\n        hits=1,\n        body=large_body,\n    )\n    end_time = time.time()\n    compress_time[\"force_compression\"] = end_time - start_time\n    print(response.is_successful())\n\nwith app.syncio(connections=1, compress=\"auto\") as session:\n    start_time = time.time()\n    response: VespaQueryResponse = session.query(\n        hits=1,\n        body=large_body,\n    )\n    end_time = time.time()\n    compress_time[\"auto\"] = end_time - start_time\n    print(response.is_successful())\n\n# Force no compression\nwith app.syncio(compress=False) as session:\n    start_time = time.time()\n    response: VespaQueryResponse = session.query(\n        hits=1,\n        body=large_body,\n        timeout=\"5s\",\n    )\n    end_time = time.time()\n    compress_time[\"no_compression\"] = end_time - start_time\n    print(response.is_successful())\n</pre> import time  # Will not compress the request, as body is less than 1024 bytes with app.syncio(connections=1, compress=\"auto\") as session:     response: VespaQueryResponse = session.query(         hits=1,         body={             \"yql\": \"select documentid, cord_uid, title, abstract from sources * where userQuery()\",             \"query\": \"Is remdesivir an effective treatment for COVID-19?\",             \"ranking.profile\": \"bm25\",             \"presentation.timing\": True,         },     )     print(response.is_successful())  # Will compress, as the size of the body exceeds 1024 bytes. large_body = {     \"yql\": \"select documentid, cord_uid, title, abstract from sources * where userQuery()\",     \"query\": \"Is remdesivir an effective treatment for COVID-19?\",     \"input.query(q)\": \"asdf\" * 10000,     \"ranking.profile\": \"bm25\",     \"presentation.timing\": True, } compress_time = {}  with app.syncio(connections=1, compress=True) as session:     start_time = time.time()     response: VespaQueryResponse = session.query(         hits=1,         body=large_body,     )     end_time = time.time()     compress_time[\"force_compression\"] = end_time - start_time     print(response.is_successful())  with app.syncio(connections=1, compress=\"auto\") as session:     start_time = time.time()     response: VespaQueryResponse = session.query(         hits=1,         body=large_body,     )     end_time = time.time()     compress_time[\"auto\"] = end_time - start_time     print(response.is_successful())  # Force no compression with app.syncio(compress=False) as session:     start_time = time.time()     response: VespaQueryResponse = session.query(         hits=1,         body=large_body,         timeout=\"5s\",     )     end_time = time.time()     compress_time[\"no_compression\"] = end_time - start_time     print(response.is_successful()) <pre>True\nTrue\nTrue\nTrue\n</pre> In\u00a0[10]: Copied! <pre>compress_time\n</pre> compress_time Out[10]: <pre>{'force_compression': 0.6639358997344971,\n 'auto': 0.6602010726928711,\n 'no_compression': 1.3003361225128174}</pre> <p>The differences will be more significant the larger the size of the body, and the slower the network. It might be beneficial to perform a proper benchmarking if performance is critical for your application.</p> In\u00a0[11]: Copied! <pre># This cell is necessary when running async code in Jupyter Notebooks, as it already runs an event loop\nimport nest_asyncio\n\nnest_asyncio.apply()\n</pre> # This cell is necessary when running async code in Jupyter Notebooks, as it already runs an event loop import nest_asyncio  nest_asyncio.apply() In\u00a0[12]: Copied! <pre>import asyncio\nimport time\n\n\n# Define a single query function that takes a session\nasync def run_query_async(session, body):\n    start_time = time.time()\n    response = await session.query(body=body)\n    end_time = time.time()\n    return response, end_time - start_time\n\n\nquery = {\n    \"yql\": \"select documentid, cord_uid, title, abstract from sources * where userQuery()\",\n    \"query\": \"Is remdesivir an effective treatment for COVID-19?\",\n    \"ranking.profile\": \"bm25\",\n    \"presentation.timing\": True,\n}\n\n# List of queries with hits from 1 to 100\nqueries = [{**query, \"hits\": hits} for hits in range(1, 101)]\n\n\n# Define a function to run multiple queries concurrently using the same session\nasync def run_multiple_queries(queries):\n    # Async client uses HTTP/2, so we only need one connection\n    async with app.asyncio(connections=1) as session:  # Reuse the same session\n        tasks = []\n        for q in queries:\n            tasks.append(run_query_async(session, q))\n        responses = await asyncio.gather(*tasks)\n    return responses\n\n\n# Run the queries concurrently\nstart_time = time.time()\nresponses = asyncio.run(run_multiple_queries(queries))\nend_time = time.time()\nprint(f\"Total time: {end_time - start_time:.2f} seconds\")\n# Print QPS\nprint(f\"QPS: {len(queries) / (end_time - start_time):.2f}\")\n</pre> import asyncio import time   # Define a single query function that takes a session async def run_query_async(session, body):     start_time = time.time()     response = await session.query(body=body)     end_time = time.time()     return response, end_time - start_time   query = {     \"yql\": \"select documentid, cord_uid, title, abstract from sources * where userQuery()\",     \"query\": \"Is remdesivir an effective treatment for COVID-19?\",     \"ranking.profile\": \"bm25\",     \"presentation.timing\": True, }  # List of queries with hits from 1 to 100 queries = [{**query, \"hits\": hits} for hits in range(1, 101)]   # Define a function to run multiple queries concurrently using the same session async def run_multiple_queries(queries):     # Async client uses HTTP/2, so we only need one connection     async with app.asyncio(connections=1) as session:  # Reuse the same session         tasks = []         for q in queries:             tasks.append(run_query_async(session, q))         responses = await asyncio.gather(*tasks)     return responses   # Run the queries concurrently start_time = time.time() responses = asyncio.run(run_multiple_queries(queries)) end_time = time.time() print(f\"Total time: {end_time - start_time:.2f} seconds\") # Print QPS print(f\"QPS: {len(queries) / (end_time - start_time):.2f}\") <pre>Total time: 10.17 seconds\nQPS: 9.84\n</pre> In\u00a0[13]: Copied! <pre>dict_responses = [response.json | {\"time\": timing} for response, timing in responses]\n</pre> dict_responses = [response.json | {\"time\": timing} for response, timing in responses] In\u00a0[14]: Copied! <pre>dict_responses[0]\n</pre> dict_responses[0] Out[14]: <pre>{'timing': {'querytime': 0.005, 'summaryfetchtime': 0.0, 'searchtime': 0.006},\n 'root': {'id': 'toplevel',\n  'relevance': 1.0,\n  'fields': {'totalCount': 2395},\n  'coverage': {'coverage': 100,\n   'documents': 976355,\n   'full': True,\n   'nodes': 2,\n   'results': 1,\n   'resultsFull': 1},\n  'children': [{'id': 'id:covid-19:doc::534720',\n    'relevance': 26.6769101612402,\n    'source': 'content',\n    'fields': {'title': 'A Review on &lt;hi&gt;Remdesivir&lt;/hi&gt;: A Possible Promising Agent for the &lt;hi&gt;Treatment&lt;/hi&gt; of &lt;hi&gt;COVID&lt;/hi&gt;-&lt;hi&gt;19&lt;/hi&gt;',\n     'abstract': '&lt;sep /&gt;manufacturing of specific therapeutics and vaccines to treat &lt;hi&gt;COVID&lt;/hi&gt;-&lt;hi&gt;19&lt;/hi&gt; are time-consuming processes. At this time, using available conventional therapeutics along with other &lt;hi&gt;treatment&lt;/hi&gt; options may be useful to fight &lt;hi&gt;COVID&lt;/hi&gt;-&lt;hi&gt;19&lt;/hi&gt;. In different clinical trials, efficacy of &lt;hi&gt;remdesivir&lt;/hi&gt; (GS-5734) against Ebola virus has been demonstrated. Moreover, &lt;hi&gt;remdesivir&lt;/hi&gt; may be an &lt;hi&gt;effective&lt;/hi&gt; therapy in vitro and in animal models infected by SARS and MERS coronaviruses. Hence, the drug may be theoretically &lt;hi&gt;effective&lt;/hi&gt; against SARS-CoV-2. &lt;hi&gt;Remdesivir&lt;/hi&gt;&lt;sep /&gt;',\n     'documentid': 'id:covid-19:doc::534720',\n     'cord_uid': 'xej338lo'}}]},\n 'time': 0.6231591701507568}</pre> In\u00a0[15]: Copied! <pre># Create a pandas DataFrame with the responses\nimport pandas as pd\n\ndf = pd.DataFrame(\n    [\n        {\n            \"hits\": len(response[\"root\"][\"children\"]),\n            \"search_time\": response[\"timing\"][\"searchtime\"],\n            \"query_time\": response[\"timing\"][\"querytime\"],\n            \"summary_time\": response[\"timing\"][\"summaryfetchtime\"],\n            \"total_time\": response[\"time\"],\n        }\n        for response in dict_responses\n    ]\n)\ndf\n</pre> # Create a pandas DataFrame with the responses import pandas as pd  df = pd.DataFrame(     [         {             \"hits\": len(response[\"root\"][\"children\"]),             \"search_time\": response[\"timing\"][\"searchtime\"],             \"query_time\": response[\"timing\"][\"querytime\"],             \"summary_time\": response[\"timing\"][\"summaryfetchtime\"],             \"total_time\": response[\"time\"],         }         for response in dict_responses     ] ) df Out[15]: hits search_time query_time summary_time total_time 0 1 0.006 0.005 0.000 0.623159 1 2 0.014 0.010 0.003 6.300802 2 3 0.012 0.010 0.001 6.300616 3 4 0.012 0.010 0.001 2.144979 4 5 0.009 0.007 0.001 4.597889 ... ... ... ... ... ... 95 96 0.066 0.031 0.034 10.134708 96 97 0.058 0.030 0.027 10.148543 97 98 0.029 0.011 0.017 10.148778 98 99 0.051 0.018 0.032 10.135024 99 100 0.043 0.015 0.027 10.149029 <p>100 rows \u00d7 5 columns</p> In\u00a0[16]: Copied! <pre>with app.syncio(connections=12) as session:\n    try:\n        response: VespaQueryResponse = session.query(\n            hits=1,\n            body={\n                \"yql\": \"select * from sources * where userQuery()\",\n                \"query\": \"Is remdesivir an effective treatment for COVID-19?\",\n                \"timeout\": \"1ms\",\n            },\n        )\n        print(response.is_successful())\n    except VespaError as e:\n        print(str(e))\n</pre> with app.syncio(connections=12) as session:     try:         response: VespaQueryResponse = session.query(             hits=1,             body={                 \"yql\": \"select * from sources * where userQuery()\",                 \"query\": \"Is remdesivir an effective treatment for COVID-19?\",                 \"timeout\": \"1ms\",             },         )         print(response.is_successful())     except VespaError as e:         print(str(e)) <pre>[{'code': 12, 'summary': 'Timed out', 'message': 'No time left after waiting for 1ms to execute query'}]\n</pre> <p>In the following example, we forgot to include the <code>query</code> parameter but still reference it in the yql. This causes a bad client request response (400):</p> In\u00a0[17]: Copied! <pre>with app.syncio(connections=12) as session:\n    try:\n        response: VespaQueryResponse = session.query(\n            hits=1, body={\"yql\": \"select * from sources * where userQuery()\"}\n        )\n        print(response.is_successful())\n    except VespaError as e:\n        print(str(e))\n</pre> with app.syncio(connections=12) as session:     try:         response: VespaQueryResponse = session.query(             hits=1, body={\"yql\": \"select * from sources * where userQuery()\"}         )         print(response.is_successful())     except VespaError as e:         print(str(e)) <pre>[{'code': 3, 'summary': 'Illegal query', 'source': 'content', 'message': 'No query'}]\n</pre> In\u00a0[18]: Copied! <pre>app = Vespa(url=\"https://api.search.vespa.ai\")\n</pre> app = Vespa(url=\"https://api.search.vespa.ai\") In\u00a0[19]: Copied! <pre>import vespa.querybuilder as qb\nfrom vespa.querybuilder import QueryField\n\nnamespace = QueryField(\"namespace\")\nq = (\n    qb.select([\"title\", \"path\", \"term_count\"])\n    .from_(\"doc\")\n    .where(\n        namespace.matches(\"pyvespa\")\n    )  # matches is regex-match, see https://docs.vespa.ai/en/reference/query-language-reference.html#matches\n    .order_by(\"term_count\", ascending=False)\n    .set_limit(10)\n)\nprint(f\"Query: {q}\")\nresp = app.query(yql=q)\nresults = [hit[\"fields\"] for hit in resp.hits]\ndf = pd.DataFrame(results)\ndf\n</pre> import vespa.querybuilder as qb from vespa.querybuilder import QueryField  namespace = QueryField(\"namespace\") q = (     qb.select([\"title\", \"path\", \"term_count\"])     .from_(\"doc\")     .where(         namespace.matches(\"pyvespa\")     )  # matches is regex-match, see https://docs.vespa.ai/en/reference/query-language-reference.html#matches     .order_by(\"term_count\", ascending=False)     .set_limit(10) ) print(f\"Query: {q}\") resp = app.query(yql=q) results = [hit[\"fields\"] for hit in resp.hits] df = pd.DataFrame(results) df <pre>Query: select title, path, term_count from doc where namespace matches \"pyvespa\" order by term_count desc limit 10\n</pre> Out[19]: path title term_count 0 /examples/feed_performance.html Feeding performance 74798 1 /reference-api.html Reference API 18282 2 /examples/simplified-retrieval-with-colpali-vl... Scaling ColPALI (VLM) Retrieval 13943 3 /examples/pdf-retrieval-with-ColQwen2-vlm_Vesp... PDF-Retrieval using ColQWen2 (ColPali) with Vespa 12939 4 /examples/colpali-document-retrieval-vision-la... Vespa \ud83e\udd1d ColPali: Efficient Document Retrieval ... 12666 5 /examples/colpali-benchmark-vqa-vlm_Vespa-clou... ColPali Ranking Experiments on DocVQA 11707 6 /examples/multi-vector-indexing.html Multi-vector indexing with HNSW 7907 7 /examples/billion-scale-vector-search-with-coh... Billion-scale vector search with Cohere binary... 6531 8 /examples/visual_pdf_rag_with_vespa_colpali_cl... Visual PDF RAG with Vespa - ColPali demo appli... 5666 9 /examples/chat_with_your_pdfs_using_colbert_la... Chat with your pdfs with ColBERT, langchain, a... 5628 In\u00a0[20]: Copied! <pre>import vespa.querybuilder as qb\nfrom vespa.querybuilder import QueryField\nfrom datetime import datetime\n\nqueryterm = \"embedding\"\n\n# We need to instantiate a QueryField for fields that we want to call methods on\nlast_updated = QueryField(\"last_updated\")\ntitle = QueryField(\"title\")\nheaders = QueryField(\"headers\")\npath = QueryField(\"path\")\nnamespace = QueryField(\"namespace\")\ncontent = QueryField(\"content\")\n\nfrom_ts = int(datetime(2024, 1, 1).timestamp())\nto_ts = int(datetime.now().timestamp())\nprint(f\"From: {from_ts}, To: {to_ts}\")\nq = (\n    qb.select(\n        [title, last_updated, content]\n    )  # Select takes either a list of QueryField or strings, (or '*' for all fields)\n    .from_(\"doc\")\n    .where(\n        namespace.matches(\"op.*\")\n        &amp; last_updated.in_range(from_ts, to_ts)  # could also use &gt; and &lt;\n        &amp; qb.weakAnd(\n            title.contains(queryterm),\n            content.contains(queryterm),\n            headers.contains(queryterm),\n            path.contains(queryterm),\n        )\n    )\n    .set_limit(3)\n)\nprint(f\"Query: {q}\")\nresp = app.query(yql=q, ranking=\"documentation\")\n</pre> import vespa.querybuilder as qb from vespa.querybuilder import QueryField from datetime import datetime  queryterm = \"embedding\"  # We need to instantiate a QueryField for fields that we want to call methods on last_updated = QueryField(\"last_updated\") title = QueryField(\"title\") headers = QueryField(\"headers\") path = QueryField(\"path\") namespace = QueryField(\"namespace\") content = QueryField(\"content\")  from_ts = int(datetime(2024, 1, 1).timestamp()) to_ts = int(datetime.now().timestamp()) print(f\"From: {from_ts}, To: {to_ts}\") q = (     qb.select(         [title, last_updated, content]     )  # Select takes either a list of QueryField or strings, (or '*' for all fields)     .from_(\"doc\")     .where(         namespace.matches(\"op.*\")         &amp; last_updated.in_range(from_ts, to_ts)  # could also use &gt; and &lt;         &amp; qb.weakAnd(             title.contains(queryterm),             content.contains(queryterm),             headers.contains(queryterm),             path.contains(queryterm),         )     )     .set_limit(3) ) print(f\"Query: {q}\") resp = app.query(yql=q, ranking=\"documentation\") <pre>From: 1704063600, To: 1736775672\nQuery: select title, last_updated, content from doc where namespace matches \"op.*\" and range(last_updated, 1704063600, 1736775672) and weakAnd(title contains \"embedding\", content contains \"embedding\", headers contains \"embedding\", path contains \"embedding\") limit 3\n</pre> In\u00a0[21]: Copied! <pre>df = pd.DataFrame([hit[\"fields\"] | hit for hit in resp.hits])\ndf = pd.concat(\n    [\n        df.drop([\"matchfeatures\", \"fields\"], axis=1),\n        pd.json_normalize(df[\"matchfeatures\"]),\n    ],\n    axis=1,\n)\ndf.T\n</pre> df = pd.DataFrame([hit[\"fields\"] | hit for hit in resp.hits]) df = pd.concat(     [         df.drop([\"matchfeatures\", \"fields\"], axis=1),         pd.json_normalize(df[\"matchfeatures\"]),     ],     axis=1, ) df.T Out[21]: 0 1 2 content &lt;sep /&gt;similar data by finding nearby points i... Reference configuration for &lt;hi&gt;embedders&lt;/hi&gt;... &lt;sep /&gt; basic news search application - applic... title Embedding Embedding Reference News search and recommendation tutorial - embe... last_updated 1736505422 1736505422 1736505423 id index:documentation/0/5d6e77ca20d4e8ee29716747 index:documentation/1/a03c4aef22fcde916804d3d9 index:documentation/1/ad44f35cbd7b8214f88963e3 relevance 23.547446 22.404666 16.870303 source documentation documentation documentation bm25(content) 2.633568 2.597595 2.63281 bm25(headers) 7.572596 8.207328 5.537104 bm25(keywords) 0.0 0.0 0.0 bm25(path) 3.934699 3.293068 3.044614 bm25(title) 4.703291 4.153337 2.827888 fieldLength(content) 3830.0 2031.0 3273.0 fieldLength(title) 1.0 2.0 6.0 fieldMatch(content) 0.915766 0.892113 0.915871 fieldMatch(content).matches 1.0 1.0 1.0 fieldMatch(title) 1.0 0.933869 0.842758 query(contentWeight) 1.0 1.0 1.0 query(headersWeight) 1.0 1.0 1.0 query(pathWeight) 1.0 1.0 1.0 query(titleWeight) 2.0 2.0 2.0 In\u00a0[22]: Copied! <pre>from vespa.querybuilder import Grouping as G\n\ngrouping = G.all(\n    G.group(\"customer\"),\n    G.each(G.output(G.sum(\"price\"))),\n)\nq = qb.select(\"*\").from_(\"purchase\").where(True).set_limit(0).groupby(grouping)\nprint(f\"Query: {q}\")\nresp = app.query(yql=q)\ngroup = resp.hits[0][\"children\"][0][\"children\"]\n# get value and sum(price) into a DataFrame\ndf = pd.DataFrame([hit[\"fields\"] | hit for hit in group])\ndf = df.loc[:, [\"value\", \"sum(price)\"]]\ndf\n</pre> from vespa.querybuilder import Grouping as G  grouping = G.all(     G.group(\"customer\"),     G.each(G.output(G.sum(\"price\"))), ) q = qb.select(\"*\").from_(\"purchase\").where(True).set_limit(0).groupby(grouping) print(f\"Query: {q}\") resp = app.query(yql=q) group = resp.hits[0][\"children\"][0][\"children\"] # get value and sum(price) into a DataFrame df = pd.DataFrame([hit[\"fields\"] | hit for hit in group]) df = df.loc[:, [\"value\", \"sum(price)\"]] df <pre>Query: select * from purchase where true limit 0 | all(group(customer) each(output(sum(price))))\n</pre> Out[22]: value sum(price) 0 Brown 20537 1 Jones 39816 2 Smith 19484 In\u00a0[23]: Copied! <pre>from vespa.querybuilder import Grouping as G\n\n# First, we construct the grouping expression:\ngrouping = G.all(\n    G.group(\"customer\"),\n    G.each(\n        G.group(G.time_date(\"date\")),\n        G.each(\n            G.output(G.sum(\"price\")),\n        ),\n    ),\n)\n# Then, we construct the query:\nq = qb.select(\"*\").from_(\"purchase\").where(True).groupby(grouping)\nprint(f\"Query: {q}\")\nresp = app.query(yql=q)\ngroup_data = resp.hits[0][\"children\"][0][\"children\"]\nrecords = [\n    {\n        \"GroupId\": group[\"value\"],\n        \"Date\": date_entry[\"value\"],\n        \"Sum(price)\": date_entry[\"fields\"].get(\"sum(price)\", 0),\n    }\n    for group in group_data\n    for date_group in group.get(\"children\", [])\n    for date_entry in date_group.get(\"children\", [])\n]\n\n# Create DataFrame\ndf = pd.DataFrame(records)\ndf\n</pre> from vespa.querybuilder import Grouping as G  # First, we construct the grouping expression: grouping = G.all(     G.group(\"customer\"),     G.each(         G.group(G.time_date(\"date\")),         G.each(             G.output(G.sum(\"price\")),         ),     ), ) # Then, we construct the query: q = qb.select(\"*\").from_(\"purchase\").where(True).groupby(grouping) print(f\"Query: {q}\") resp = app.query(yql=q) group_data = resp.hits[0][\"children\"][0][\"children\"] records = [     {         \"GroupId\": group[\"value\"],         \"Date\": date_entry[\"value\"],         \"Sum(price)\": date_entry[\"fields\"].get(\"sum(price)\", 0),     }     for group in group_data     for date_group in group.get(\"children\", [])     for date_entry in date_group.get(\"children\", []) ]  # Create DataFrame df = pd.DataFrame(records) df <pre>Query: select * from purchase where true | all(group(customer) each(group(time.date(date)) each(output(sum(price)))))\n</pre> Out[23]: GroupId Date Sum(price) 0 Brown 2006-9-10 7540 1 Brown 2006-9-11 1597 2 Brown 2006-9-8 8000 3 Brown 2006-9-9 3400 4 Jones 2006-9-10 8900 5 Jones 2006-9-11 20816 6 Jones 2006-9-8 8000 7 Jones 2006-9-9 2100 8 Smith 2006-9-10 6100 9 Smith 2006-9-11 2584 10 Smith 2006-9-6 1000 11 Smith 2006-9-7 3000 12 Smith 2006-9-9 6800 In\u00a0[24]: Copied! <pre>from vespa.querybuilder import Grouping as G\n\ngrouping = G.all(\n    G.group(G.mod(G.div(\"date\", G.mul(60, 60)), 24)),\n    G.order(-G.sum(\"price\")),\n    G.each(G.output(G.sum(\"price\"))),\n)\nq = qb.select(\"*\").from_(\"purchase\").where(True).groupby(grouping)\nprint(f\"Query: {q}\")\nresp = app.query(yql=q)\ngroup_data = resp.hits[0][\"children\"][0][\"children\"]\ndf = pd.DataFrame([hit[\"fields\"] | hit for hit in group_data])\ndf = df.loc[:, [\"value\", \"sum(price)\"]]\ndf\n</pre> from vespa.querybuilder import Grouping as G  grouping = G.all(     G.group(G.mod(G.div(\"date\", G.mul(60, 60)), 24)),     G.order(-G.sum(\"price\")),     G.each(G.output(G.sum(\"price\"))), ) q = qb.select(\"*\").from_(\"purchase\").where(True).groupby(grouping) print(f\"Query: {q}\") resp = app.query(yql=q) group_data = resp.hits[0][\"children\"][0][\"children\"] df = pd.DataFrame([hit[\"fields\"] | hit for hit in group_data]) df = df.loc[:, [\"value\", \"sum(price)\"]] df <pre>Query: select * from purchase where true | all(group(mod(div(date, mul(60, 60)),24)) order(-sum(price)) each(output(sum(price))))\n</pre> Out[24]: value sum(price) 0 10 26181 1 9 23524 2 8 22367 3 11 6765 4 7 1000"},{"location":"query.html#querying-vespa","title":"Querying Vespa\u00b6","text":"<p>This guide goes through how to query a Vespa instance using the Query API and https://cord19.vespa.ai/ and https://search.vespa.ai/ apps as examples.</p>"},{"location":"query.html#query-performance","title":"Query Performance\u00b6","text":"<p>There are several things that impact end-to-end query performance:</p> <ul> <li>HTTP layer performance, connecting handling, mututal TLS handshake and network round-trip latency<ul> <li>Make sure to re-use connections using context manager <code>with vespa.app.syncio():</code>\u00a0to avoid setting up new connections for every unique query. See http best practises</li> <li>The size of the fields and the number of hits requested also greatly impact network performance; a larger payload means higher latency.</li> <li>By adding <code>\"presentation.timing\": True</code> as a request parameter, the Vespa response includes the server-side processing (also including reading the query from the network, but not delivering the result over the network). This can be handy for debugging latency.</li> </ul> </li> <li>Vespa performance, the features used inside the Vespa instance.</li> </ul>"},{"location":"query.html#compressing-queries","title":"Compressing queries\u00b6","text":"<p>The <code>VespaSync</code> class has a <code>compress</code> argument that can be used to compress the query before sending it to Vespa. This can be useful when the query is large and/or the network is slow. The compression is done using <code>gzip</code>, and is supported by Vespa.</p> <p>By default, the <code>compress</code> argument is set to <code>\"auto\"</code>, which means that the query will be compressed if the size of the query is larger than 1024 bytes. The <code>compress</code> argument can also be set to <code>True</code> or <code>False</code> to force the query to be compressed or not, respectively.</p> <p>The compression will be applied to both queries and feed operations. (HTTP POST or PUT requests).</p>"},{"location":"query.html#running-queries-asynchronously","title":"Running Queries asynchronously\u00b6","text":"<p>If you want to benchmark the capacity of a Vespa application, we suggest using vespa-fbench, which is a load generator tool that lets you measure throughput and latency with a predefined number of clients. Vespa-fbench is not Vespa-specific, and can be used to benchmark any HTTP service.</p> <p>Another option is to use the Open Source k6 load testing tool.</p> <p>If you want to run multiple queries from pyvespa, we suggest using the async client. Below, we will demonstrate a simple example of running 100 queries in parallel using the async client, and capture both the server-reported times and the client-reported times (including network latency).</p>"},{"location":"query.html#error-handling","title":"Error handling\u00b6","text":"<p>Vespa's default query timeout is 500ms; Pyvespa will by default retry up to 3 times for queries that return response codes like 429, 500,503 and 504. A <code>VespaError</code> is raised if retries did not end up with success. In the following example, we set a very low timeout of <code>1ms</code> which will cause Vespa to time out the request, and it returns a 504 http error code. The underlying error is wrapped in a <code>VespaError</code> with the payload error message returned from Vespa:</p>"},{"location":"query.html#using-the-querybuilder-dsl-api","title":"Using the Querybuilder DSL API\u00b6","text":"<p>From <code>pyvespa&gt;=0.52.0</code>, we provide a Domain Specific Language (DSL) that allows you to build queries programmatically in the <code>vespa.querybuilder</code>-module. See reference for full details. There are also many examples in our tests:</p> <ul> <li>https://github.com/vespa-engine/pyvespa/blob/master/tests/unit/test_grouping.py</li> <li>https://github.com/vespa-engine/pyvespa/blob/master/tests/unit/test_qb.py</li> <li>https://github.com/vespa-engine/pyvespa/blob/master/tests/integration/test_integration_grouping.py</li> <li>https://github.com/vespa-engine/pyvespa/blob/master/tests/integration/test_integration_queries.py</li> </ul> <p>This section demonstrates common query patterns using the querybuilder DSL. All features of the Vespa Query Language are supported by the querybuilder DSL.</p>      Using the Querybuilder DSL is completely optional, and you can always use the Vespa Query Language directly by passing the query as a string, which might be more convenient for simple queries.  <p>We will use our own documentation search app for the following examples. For details of the app configuration, see the corresponding github repository.</p>"},{"location":"query.html#example-1-matches-order-by-and-limit","title":"Example 1 - matches, order by and limit\u00b6","text":"<p>We want to find the 10 documents with the most terms in the 'pyvespa'-namespace (the documentation search has a 'namespace'-field, which refers to the source of the documentation). Note that the documentation search operates on the 'paragraph'-schema, but for demo purposes, we will use the 'document'-schema.</p>"},{"location":"query.html#example-2-timestamp-range-contains","title":"Example 2 - timestamp range, contains\u00b6","text":"<p>We want to find the documents where one of the indexed fields contains the query term <code>embedding</code>,is updated after Jan 1st 2024 and the current timestamp, and have the documents ranked the 'documentation' rank profile. See https://github.com/vespa-cloud/vespa-documentation-search/blob/main/src/main/application/schemas/doc.sd.</p>"},{"location":"query.html#example-3-basic-grouping","title":"Example 3 - Basic grouping\u00b6","text":"<p>Vespa supports grouping and aggregation of matches through the Vespa grouping language. For an introduction to grouping, see https://docs.vespa.ai/en/grouping.html.</p> <p>We will use purchase schema that is also deployed in the documentation search app.</p>"},{"location":"query.html#example-4-nested-grouping","title":"Example 4 - Nested grouping\u00b6","text":"<p>Let's find out how much each customer has spent per day by grouping on customer, then date:</p>"},{"location":"query.html#example-5-grouping-with-expressions","title":"Example 5 - Grouping with expressions\u00b6","text":"<p>Instead of just grouping on some attribute value, the group clause may contain arbitrarily complex expressions - see Grouping reference for exhaustive list.</p> <p>Examples:</p> <ul> <li>Select the minimum or maximum of sub-expressions</li> <li>Addition, subtraction, multiplication, division, and even modulo of - sub-expressions</li> <li>Bitwise operations on sub-expressions</li> <li>Concatenation of the results of sub-expressions</li> </ul> <p>Let's use some of these expressions to get the sum the prices of purchases on a per-hour-of-day basis.</p>"},{"location":"reads-writes.html","title":"Read and write operations","text":"Refer to troubleshooting     for any problem when running this guide.  In\u00a0[1]: Copied! <pre>!docker info | grep \"Total Memory\"\n</pre> !docker info | grep \"Total Memory\" <p>Define a simple application package with five fields</p> In\u00a0[1]: Copied! <pre>from vespa.application import ApplicationPackage\nfrom vespa.package import Schema, Document, Field, FieldSet, HNSW, RankProfile\n\napp_package = ApplicationPackage(\n    name=\"vector\",\n    schema=[\n        Schema(\n            name=\"doc\",\n            document=Document(\n                fields=[\n                    Field(name=\"id\", type=\"string\", indexing=[\"attribute\", \"summary\"]),\n                    Field(\n                        name=\"title\",\n                        type=\"string\",\n                        indexing=[\"index\", \"summary\"],\n                        index=\"enable-bm25\",\n                    ),\n                    Field(\n                        name=\"body\",\n                        type=\"string\",\n                        indexing=[\"index\", \"summary\"],\n                        index=\"enable-bm25\",\n                    ),\n                    Field(\n                        name=\"popularity\",\n                        type=\"float\",\n                        indexing=[\"attribute\", \"summary\"],\n                    ),\n                    Field(\n                        name=\"embedding\",\n                        type=\"tensor&lt;bfloat16&gt;(x[1536])\",\n                        indexing=[\"attribute\", \"summary\", \"index\"],\n                        ann=HNSW(\n                            distance_metric=\"innerproduct\",\n                            max_links_per_node=16,\n                            neighbors_to_explore_at_insert=128,\n                        ),\n                    ),\n                ]\n            ),\n            fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"body\"])],\n            rank_profiles=[\n                RankProfile(\n                    name=\"default\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[1536])\")],\n                    first_phase=\"closeness(field, embedding)\",\n                )\n            ],\n        )\n    ],\n)\n</pre> from vespa.application import ApplicationPackage from vespa.package import Schema, Document, Field, FieldSet, HNSW, RankProfile  app_package = ApplicationPackage(     name=\"vector\",     schema=[         Schema(             name=\"doc\",             document=Document(                 fields=[                     Field(name=\"id\", type=\"string\", indexing=[\"attribute\", \"summary\"]),                     Field(                         name=\"title\",                         type=\"string\",                         indexing=[\"index\", \"summary\"],                         index=\"enable-bm25\",                     ),                     Field(                         name=\"body\",                         type=\"string\",                         indexing=[\"index\", \"summary\"],                         index=\"enable-bm25\",                     ),                     Field(                         name=\"popularity\",                         type=\"float\",                         indexing=[\"attribute\", \"summary\"],                     ),                     Field(                         name=\"embedding\",                         type=\"tensor(x[1536])\",                         indexing=[\"attribute\", \"summary\", \"index\"],                         ann=HNSW(                             distance_metric=\"innerproduct\",                             max_links_per_node=16,                             neighbors_to_explore_at_insert=128,                         ),                     ),                 ]             ),             fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"body\"])],             rank_profiles=[                 RankProfile(                     name=\"default\",                     inputs=[(\"query(q)\", \"tensor(x[1536])\")],                     first_phase=\"closeness(field, embedding)\",                 )             ],         )     ], ) In\u00a0[2]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=app_package)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy(application_package=app_package) <pre>Waiting for configuration server, 0/60 seconds...\nWaiting for configuration server, 5/60 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 15/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 20/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[3]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\n    \"KShivendu/dbpedia-entities-openai-1M\", split=\"train\", streaming=True\n).take(1000)\n</pre> from datasets import load_dataset  dataset = load_dataset(     \"KShivendu/dbpedia-entities-openai-1M\", split=\"train\", streaming=True ).take(1000) <pre>/Users/thomas/.pyenv/versions/3.11.8/envs/pyvespa-build/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[4]: Copied! <pre>pyvespa_feed_format = dataset.map(\n    lambda x: {\"id\": x[\"_id\"], \"fields\": {\"id\": x[\"_id\"], \"embedding\": x[\"openai\"]}}\n)\n</pre> pyvespa_feed_format = dataset.map(     lambda x: {\"id\": x[\"_id\"], \"fields\": {\"id\": x[\"_id\"], \"embedding\": x[\"openai\"]}} ) <p>Feed using feed_iterable which accepts an <code>Iterable</code>. <code>feed_iterable</code> accepts a callback callable routine that is called for every single data operation so we can check the result. If the result <code>is_successful()</code> the operation is persisted and applied in Vespa.</p> In\u00a0[5]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         ) In\u00a0[6]: Copied! <pre>app.feed_iterable(\n    iter=pyvespa_feed_format,\n    schema=\"doc\",\n    namespace=\"benchmark\",\n    callback=callback,\n    max_queue_size=4000,\n    max_workers=16,\n    max_connections=16,\n)\n</pre> app.feed_iterable(     iter=pyvespa_feed_format,     schema=\"doc\",     namespace=\"benchmark\",     callback=callback,     max_queue_size=4000,     max_workers=16,     max_connections=16, ) In\u00a0[7]: Copied! <pre>def my_generator() -&gt; dict:\n    for i in range(1000):\n        yield {\n            \"id\": str(i),\n            \"fields\": {\n                \"id\": str(i),\n                \"title\": \"title\",\n                \"body\": \"this is body\",\n                \"popularity\": 1.0,\n            },\n        }\n</pre> def my_generator() -&gt; dict:     for i in range(1000):         yield {             \"id\": str(i),             \"fields\": {                 \"id\": str(i),                 \"title\": \"title\",                 \"body\": \"this is body\",                 \"popularity\": 1.0,             },         } In\u00a0[8]: Copied! <pre>app.feed_iterable(\n    iter=my_generator(),\n    schema=\"doc\",\n    namespace=\"benchmark\",\n    callback=callback,\n    max_queue_size=4000,\n    max_workers=12,\n    max_connections=12,\n)\n</pre> app.feed_iterable(     iter=my_generator(),     schema=\"doc\",     namespace=\"benchmark\",     callback=callback,     max_queue_size=4000,     max_workers=12,     max_connections=12, ) In\u00a0[9]: Copied! <pre>all_docs = []\nfor slice in app.visit(\n    content_cluster_name=\"vector_content\",\n    schema=\"doc\",\n    namespace=\"benchmark\",\n    selection=\"true\",  # Document selection - see https://docs.vespa.ai/en/reference/document-select-language.html\n    slices=4,\n    wanted_document_count=300,\n):\n    for response in slice:\n        print(response.number_documents_retrieved)\n        all_docs.extend(response.documents)\n</pre> all_docs = [] for slice in app.visit(     content_cluster_name=\"vector_content\",     schema=\"doc\",     namespace=\"benchmark\",     selection=\"true\",  # Document selection - see https://docs.vespa.ai/en/reference/document-select-language.html     slices=4,     wanted_document_count=300, ):     for response in slice:         print(response.number_documents_retrieved)         all_docs.extend(response.documents) <pre>300\n196\n303\n185\n309\n191\n303\n213\n</pre> In\u00a0[10]: Copied! <pre>len(all_docs)\n</pre> len(all_docs) Out[10]: <pre>2000</pre> In\u00a0[11]: Copied! <pre>for slice in app.visit(\n    content_cluster_name=\"vector_content\", wanted_document_count=1000\n):\n    for response in slice:\n        print(response.number_documents_retrieved)\n</pre> for slice in app.visit(     content_cluster_name=\"vector_content\", wanted_document_count=1000 ):     for response in slice:         print(response.number_documents_retrieved) <pre>190\n189\n226\n205\n184\n214\n202\n181\n217\n192\n</pre> In\u00a0[12]: Copied! <pre>def my_update_generator() -&gt; dict:\n    for i in range(1000):\n        yield {\"id\": str(i), \"fields\": {\"popularity\": 2.0}}\n</pre> def my_update_generator() -&gt; dict:     for i in range(1000):         yield {\"id\": str(i), \"fields\": {\"popularity\": 2.0}} In\u00a0[13]: Copied! <pre>app.feed_iterable(\n    iter=my_update_generator(),\n    schema=\"doc\",\n    namespace=\"benchmark\",\n    callback=callback,\n    operation_type=\"update\",\n    max_queue_size=4000,\n    max_workers=12,\n    max_connections=12,\n)\n</pre> app.feed_iterable(     iter=my_update_generator(),     schema=\"doc\",     namespace=\"benchmark\",     callback=callback,     operation_type=\"update\",     max_queue_size=4000,     max_workers=12,     max_connections=12, ) In\u00a0[14]: Copied! <pre>def my_increment_generator() -&gt; dict:\n    for i in range(1000):\n        yield {\"id\": str(i), \"fields\": {\"popularity\": {\"increment\": 1.0}}}\n</pre> def my_increment_generator() -&gt; dict:     for i in range(1000):         yield {\"id\": str(i), \"fields\": {\"popularity\": {\"increment\": 1.0}}} In\u00a0[15]: Copied! <pre>app.feed_iterable(\n    iter=my_increment_generator(),\n    schema=\"doc\",\n    namespace=\"benchmark\",\n    callback=callback,\n    operation_type=\"update\",\n    max_queue_size=4000,\n    max_workers=12,\n    max_connections=12,\n    auto_assign=False,\n)\n</pre> app.feed_iterable(     iter=my_increment_generator(),     schema=\"doc\",     namespace=\"benchmark\",     callback=callback,     operation_type=\"update\",     max_queue_size=4000,     max_workers=12,     max_connections=12,     auto_assign=False, ) <p>We can now query the data, notice how we use a context manager <code>with</code> to close connection after query This avoids resource leakage and allows for reuse of connections. In this case, we only do a single query and there is no need for having more than one connection. Setting more connections will just increase connection level overhead.</p> In\u00a0[16]: Copied! <pre>from vespa.io import VespaQueryResponse\n\nwith app.syncio(connections=1):\n    response: VespaQueryResponse = app.query(\n        yql=\"select id from doc where popularity &gt; 2.5\", hits=0\n    )\n    print(response.number_documents_retrieved)\n</pre> from vespa.io import VespaQueryResponse  with app.syncio(connections=1):     response: VespaQueryResponse = app.query(         yql=\"select id from doc where popularity &gt; 2.5\", hits=0     )     print(response.number_documents_retrieved) <pre>1000\n</pre> In\u00a0[16]: Copied! <pre>def my_delete_generator() -&gt; dict:\n    for i in range(1000):\n        yield {\"id\": str(i)}\n\n\napp.feed_iterable(\n    iter=my_delete_generator(),\n    schema=\"doc\",\n    namespace=\"benchmark\",\n    callback=callback,\n    operation_type=\"delete\",\n    max_queue_size=5000,\n    max_workers=48,\n    max_connections=48,\n)\n</pre> def my_delete_generator() -&gt; dict:     for i in range(1000):         yield {\"id\": str(i)}   app.feed_iterable(     iter=my_delete_generator(),     schema=\"doc\",     namespace=\"benchmark\",     callback=callback,     operation_type=\"delete\",     max_queue_size=5000,     max_workers=48,     max_connections=48, ) In\u00a0[17]: Copied! <pre># Dump some operation to a jsonl file, we store it in the format expected by pyvespa\n# This to demonstrate feeding from a file in the next section.\nimport json\n\nwith open(\"documents.jsonl\", \"w\") as f:\n    for doc in dataset:\n        d = {\"id\": doc[\"_id\"], \"fields\": {\"id\": doc[\"_id\"], \"embedding\": doc[\"openai\"]}}\n        f.write(json.dumps(d) + \"\\n\")\n</pre> # Dump some operation to a jsonl file, we store it in the format expected by pyvespa # This to demonstrate feeding from a file in the next section. import json  with open(\"documents.jsonl\", \"w\") as f:     for doc in dataset:         d = {\"id\": doc[\"_id\"], \"fields\": {\"id\": doc[\"_id\"], \"embedding\": doc[\"openai\"]}}         f.write(json.dumps(d) + \"\\n\") <p>Define the file generator that will yield one line at a time</p> In\u00a0[18]: Copied! <pre>import json\n\n\ndef from_file_generator() -&gt; dict:\n    with open(\"documents.jsonl\") as f:\n        for line in f:\n            yield json.loads(line)\n</pre> import json   def from_file_generator() -&gt; dict:     with open(\"documents.jsonl\") as f:         for line in f:             yield json.loads(line) In\u00a0[19]: Copied! <pre>app.feed_iterable(\n    iter=from_file_generator(),\n    schema=\"doc\",\n    namespace=\"benchmark\",\n    callback=callback,\n    operation_type=\"feed\",\n    max_queue_size=4000,\n    max_workers=32,\n    max_connections=32,\n)\n</pre> app.feed_iterable(     iter=from_file_generator(),     schema=\"doc\",     namespace=\"benchmark\",     callback=callback,     operation_type=\"feed\",     max_queue_size=4000,     max_workers=32,     max_connections=32, ) In\u00a0[20]: Copied! <pre>with app.syncio(connections=1):\n    response: VespaResponse = app.feed_data_point(\n        schema=\"doc\",\n        namespace=\"benchmark\",\n        data_id=\"1\",\n        fields={\n            \"id\": \"1\",\n            \"title\": \"title\",\n            \"body\": \"this is body\",\n            \"popularity\": 1.0,\n        },\n    )\n    print(response.is_successful())\n    print(response.get_json())\n</pre> with app.syncio(connections=1):     response: VespaResponse = app.feed_data_point(         schema=\"doc\",         namespace=\"benchmark\",         data_id=\"1\",         fields={             \"id\": \"1\",             \"title\": \"title\",             \"body\": \"this is body\",             \"popularity\": 1.0,         },     )     print(response.is_successful())     print(response.get_json()) <pre>True\n{'pathId': '/document/v1/benchmark/doc/docid/1', 'id': 'id:benchmark:doc::1'}\n</pre> <p>Get the same document, try also to change data_id to a document that does not exist which will raise a 404 http error.</p> In\u00a0[21]: Copied! <pre>with app.syncio(connections=1):\n    response: VespaResponse = app.get_data(\n        schema=\"doc\",\n        namespace=\"benchmark\",\n        data_id=\"1\",\n    )\n    print(response.is_successful())\n    print(response.get_json())\n</pre> with app.syncio(connections=1):     response: VespaResponse = app.get_data(         schema=\"doc\",         namespace=\"benchmark\",         data_id=\"1\",     )     print(response.is_successful())     print(response.get_json()) <pre>True\n{'pathId': '/document/v1/benchmark/doc/docid/1', 'id': 'id:benchmark:doc::1', 'fields': {'body': 'this is body', 'title': 'title', 'popularity': 1.0, 'id': '1'}}\n</pre> In\u00a0[22]: Copied! <pre>with app.syncio(connections=1):\n    response: VespaResponse = app.update_data(\n        schema=\"doc\",\n        namespace=\"benchmark\",\n        data_id=\"does-not-exist\",\n        fields={\"popularity\": 3.0},\n        create=True,\n    )\n    print(response.is_successful())\n    print(response.get_json())\n</pre> with app.syncio(connections=1):     response: VespaResponse = app.update_data(         schema=\"doc\",         namespace=\"benchmark\",         data_id=\"does-not-exist\",         fields={\"popularity\": 3.0},         create=True,     )     print(response.is_successful())     print(response.get_json()) <pre>True\n{'pathId': '/document/v1/benchmark/doc/docid/does-not-exist', 'id': 'id:benchmark:doc::does-not-exist'}\n</pre> In\u00a0[23]: Copied! <pre>with app.syncio(connections=1):\n    response: VespaResponse = app.get_data(\n        schema=\"doc\",\n        namespace=\"benchmark\",\n        data_id=\"does-not-exist\",\n    )\n    print(response.is_successful())\n    print(response.get_json())\n</pre> with app.syncio(connections=1):     response: VespaResponse = app.get_data(         schema=\"doc\",         namespace=\"benchmark\",         data_id=\"does-not-exist\",     )     print(response.is_successful())     print(response.get_json()) <pre>True\n{'pathId': '/document/v1/benchmark/doc/docid/does-not-exist', 'id': 'id:benchmark:doc::does-not-exist', 'fields': {'popularity': 3.0}}\n</pre> In\u00a0[24]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"reads-writes.html#read-and-write-operations","title":"Read and write operations\u00b6","text":"<p>This notebook documents ways to feed, get, update and delete data:</p> <ul> <li>Using context manager with <code>with</code> for efficiently managing resources</li> <li>Feeding streams of data using <code>feed_iter</code> which can feed from streams, Iterables, Lists and files by the use of generators</li> </ul>"},{"location":"reads-writes.html#deploy-a-sample-application","title":"Deploy a sample application\u00b6","text":"<p>Install pyvespa and start Docker, validate minimum 4G available:</p>"},{"location":"reads-writes.html#feed-data-by-streaming-over-iterable-type","title":"Feed data by streaming over Iterable type\u00b6","text":"<p>This example notebook uses the dbpedia-entities-openai-1M dataset (1M OpenAI Embeddings (1536 dimensions) from June 2023).</p> <p>The <code>streaming=True</code> option allow paging the data on-demand from HF S3. This is extremely useful for large datasets, where the data does not fit in memory and downloading the entire dataset is not needed. Read more about datasets stream.</p>"},{"location":"reads-writes.html#converting-to-dataset-field-names-to-vespa-schema-field-names","title":"Converting to dataset field names to Vespa schema field names\u00b6","text":"<p>We need to convert the dataset field names to the configured Vespa schema field names, we do this with a simple lambda function.</p> <p>The map function does not page the data, the map step is performed lazily if we start iterating over the dataset. This allows chaining of map operations where the lambda is yielding the next document.</p>"},{"location":"reads-writes.html#feeding-with-generators","title":"Feeding with generators\u00b6","text":"<p>The above handled streaming data from a remote repo, we can also use generators or just List. In this example, we generate synthetic data using a generator function.</p>"},{"location":"reads-writes.html#visiting","title":"Visiting\u00b6","text":"<p>Visiting is a feature to efficiently get or process a set of documents, identified by a document selection expression. Visit yields multiple slices (run concurrently) each yielding responses (depending on number of documents in each slice). This allows for custom handling of each response.</p> <p>Visiting can be useful for exporting data, for example for ML training or for migrating a vespa application.</p>"},{"location":"reads-writes.html#updates","title":"Updates\u00b6","text":"<p>Using a similar generator we can update the fake data we added. This performs partial updates, assigning the <code>popularity</code> field to have the value <code>2.0</code>.</p>"},{"location":"reads-writes.html#other-update-operations","title":"Other update operations\u00b6","text":"<p>We can also perform other update operations, see Vespa docs on reads and writes. To achieve this we need to set the <code>auto_assign</code> parameter to <code>False</code> in the <code>feed_iterable</code> method (which will pass this to <code>update_data_point</code>-method).</p>"},{"location":"reads-writes.html#deleting","title":"Deleting\u00b6","text":"<p>Delete all the synthetic data with a custom generator. Now we don't need the <code>fields</code> key.</p>"},{"location":"reads-writes.html#feeding-operations-from-a-file","title":"Feeding operations from a file\u00b6","text":"<p>This demonstrates how we can use <code>feed_iter</code> to feed from a large file without reading the entire file, this also uses a generator.</p> <p>First we dump some operations to the file and peak at the first line:</p>"},{"location":"reads-writes.html#get-and-feed-individual-data-points","title":"Get and Feed individual data points\u00b6","text":"<p>Feed a single data point to Vespa</p>"},{"location":"reads-writes.html#upsert","title":"Upsert\u00b6","text":"<p>The following sends an update operation, if the document exist, the popularity field will be updated to take the value 3.0, and if the document does not exist, it's created and where the popularity value is 3.0.</p>"},{"location":"reads-writes.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"reads-writes.html#next-steps","title":"Next steps\u00b6","text":"<p>Read more on writing to Vespa in reads-and-writes.</p>"},{"location":"troubleshooting.html","title":"Troubleshooting","text":"<p>Also see the Vespa FAQ and Vespa support for more help resources.</p>"},{"location":"troubleshooting.html#vespaai-and-pyvespa","title":"Vespa.ai and pyvespa","text":"<p>Both Vespa and pyvespa APIs change regularly - make sure to use the latest version of vespaengine/vespa by running <code>docker pull vespaengine/vespa</code> and install pyvespa.</p> <p>To check the current version, run:</p> <pre><code>python3 -m pip show pyvespa\n</code></pre>"},{"location":"troubleshooting.html#docker-memory","title":"Docker Memory","text":"<p>pyvespa will start a Docker container with 4G memory by default - make sure Docker settings have at least this. Use the Docker Desktop settings or <code>docker info | grep \"Total Memory\"</code> or <code>podman info | grep \"memTotal\"</code> to validate.</p>"},{"location":"troubleshooting.html#port-conflicts-docker","title":"Port conflicts / Docker","text":"<p>Some of the notebooks run a Docker container. Make sure to stop running Docker containers before (re)running pyvespa notebooks - run <code>docker ps</code> and <code>docker ps -a -q -f status=exited</code> to list containers.</p>"},{"location":"troubleshooting.html#deployment","title":"Deployment","text":"<p>Vespa has safeguards for incompatible deployments, and will warn with validation-override or INVALID_APPLICATION_PACKAGE in the deploy output. See validation-overrides. This is most often due to pyvespa reusing a Docker container instance. The fix is to list (<code>docker ps</code>) and remove (<code>docker rm -f &lt;container id&gt;</code>) the existing Docker containers. Alternatively, use the Docker Dashboard application. Then deploy again.</p> <p>After deployment, validate status:</p> <ul> <li>Config server state: http://localhost:19071/state/v1/health</li> <li>Container state: http://localhost:8080/state/v1/health</li> </ul> <p>Look for <code>\"status\" : { \"code\" : \"up\"}</code> - both URLs must work before feeding or querying.</p>"},{"location":"troubleshooting.html#full-disk","title":"Full disk","text":"<p>Make sure to allocate enough disk space for Docker in Docker settings. If writes/queries fail or return no results, look in the <code>vespa.log</code> (output in the Docker dashboard):</p> <pre><code>WARNING searchnode\nproton.proton.server.disk_mem_usage_filter   Write operations are now blocked:\n'diskLimitReached: { action: \"add more content nodes\",\nreason: \"disk used (0.939172) &gt; disk limit (0.9)\",\nstats: { capacity: 50406772736, used: 47340617728, diskUsed: 0.939172, diskLimit: 0.9}}'\n</code></pre> <p>Future pyvespa versions might throw an exception in these cases. See Feed block - Vespa stops writes before the disk goes full. Add more disk space, clean up, or follow the example to reconfigure for higher usage.</p>"},{"location":"troubleshooting.html#check-number-of-indexed-documents","title":"Check number of indexed documents","text":"<p>For query errors, check the number of documents indexed before debugging further:</p> <pre><code>app.query(yql='select * from sources * where true').number_documents_indexed\n</code></pre> <p>If this is zero, check that the deployment of the application worked, and that the subsequent feeding step completed successfully.</p>"},{"location":"troubleshooting.html#too-many-open-files-during-batch-feeding","title":"Too many open files during batch feeding","text":"<p>This is an OS-related issue. There are two options to solve the problem:</p> <ol> <li> <p>Reduce the number of connections via the <code>connections</code> parameter:    <pre><code>with app.syncio(connections=12):\n</code></pre></p> </li> <li> <p>Increase the open file limit: <code>ulimit -n 10000</code>. Check if the limit was increased with <code>ulimit -Sn</code>.</p> </li> </ol>"},{"location":"troubleshooting.html#data-export","title":"Data export","text":"<p><code>vespa visit</code> exports data from Vespa - see Vespa CLI. Use this to validate data feeding and troubleshoot query issues.</p>"},{"location":"api/summary.html","title":"API Reference","text":"<ul> <li>vespa<ul> <li>application</li> <li>deployment</li> <li>evaluation</li> <li>exceptions</li> <li>io</li> <li>package</li> <li>querybuilder<ul> <li>builder<ul> <li>builder</li> </ul> </li> <li>grouping<ul> <li>grouping</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"api/vespa/index.html","title":"Index","text":""},{"location":"api/vespa/index.html#vespa","title":"<code>vespa</code>","text":""},{"location":"api/vespa/application.html","title":"Application","text":""},{"location":"api/vespa/application.html#vespa.application","title":"<code>vespa.application</code>","text":""},{"location":"api/vespa/application.html#vespa.application.Vespa","title":"<code>Vespa(url, port=None, deployment_message=None, cert=None, key=None, vespa_cloud_secret_token=None, output_file=sys.stdout, application_package=None, additional_headers=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Establish a connection with an existing Vespa application.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Vespa endpoint URL.</p> required <code>port</code> <code>int</code> <p>Vespa endpoint port.</p> <code>None</code> <code>deployment_message</code> <code>str</code> <p>Message returned by Vespa engine after deployment. Used internally by deploy methods.</p> <code>None</code> <code>cert</code> <code>str</code> <p>Path to data plane certificate and key file in case the 'key' parameter is None. If 'key' is not None, this should be the path of the certificate file. Typically generated by Vespa-cli with 'vespa auth cert'.</p> <code>None</code> <code>key</code> <code>str</code> <p>Path to the data plane key file. Typically generated by Vespa-cli with 'vespa auth cert'.</p> <code>None</code> <code>vespa_cloud_secret_token</code> <code>str</code> <p>Vespa Cloud data plane secret token.</p> <code>None</code> <code>output_file</code> <code>str</code> <p>Output file to write output messages.</p> <code>stdout</code> <code>application_package</code> <code>str</code> <p>Application package definition used to deploy the application.</p> <code>None</code> <code>additional_headers</code> <code>dict</code> <p>Additional headers to be sent to the Vespa application.</p> <code>None</code> Example usage <pre><code>Vespa(url=\"https://cord19.vespa.ai\")   # doctest: +SKIP\n\nVespa(url=\"http://localhost\", port=8080)\nVespa(http://localhost, 8080)\n\nVespa(url=\"https://token-endpoint..z.vespa-app.cloud\", vespa_cloud_secret_token=\"your_token\")  # doctest: +SKIP\n\nVespa(url=\"https://mtls-endpoint..z.vespa-app.cloud\", cert=\"/path/to/cert.pem\", key=\"/path/to/key.pem\")  # doctest: +SKIP\n\nVespa(url=\"https://mtls-endpoint..z.vespa-app.cloud\", cert=\"/path/to/cert.pem\", key=\"/path/to/key.pem\", additional_headers={\"X-Custom-Header\": \"test\"})  # doctest: +SKIP\n</code></pre>"},{"location":"api/vespa/application.html#vespa.application.Vespa.application_package","title":"<code>application_package</code>  <code>property</code>","text":"<p>Get application package definition, if available.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.asyncio","title":"<code>asyncio(connections=1, total_timeout=None, timeout=httpx.Timeout(5), **kwargs)</code>","text":"<p>Access Vespa asynchronous connection layer. Should be used as a context manager.</p> Example usage <pre><code>async with app.asyncio() as async_app:\n    response = await async_app.query(body=body)\n\n# passing kwargs\nlimits = httpx.Limits(max_keepalive_connections=5, max_connections=5, keepalive_expiry=15)\ntimeout = httpx.Timeout(connect=3, read=4, write=2, pool=5)\nasync with app.asyncio(connections=5, timeout=timeout, limits=limits) as async_app:\n    response = await async_app.query(body=body)\n</code></pre> <p>See <code>VespaAsync</code> for more details on the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>int</code> <p>Number of maximum_keepalive_connections.</p> <code>1</code> <code>total_timeout</code> <code>int</code> <p>Deprecated. Will be ignored. Use timeout instead.</p> <code>None</code> <code>timeout</code> <code>Timeout</code> <p>httpx.Timeout object. See Timeouts. Defaults to 5 seconds.</p> <code>Timeout(5)</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the httpx.AsyncClient.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>VespaAsync</code> <code>VespaAsync</code> <p>Instance of Vespa asynchronous layer.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.syncio","title":"<code>syncio(connections=8, compress='auto')</code>","text":"<p>Access Vespa synchronous connection layer. Should be used as a context manager.</p> <p>Example usage:</p> <pre><code>```python\nwith app.syncio() as sync_app:\n    response = sync_app.query(body=body)\n```\n</code></pre> <p>See  for more details. <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>int</code> <p>Number of allowed concurrent connections.</p> <code>8</code> <code>total_timeout</code> <code>float</code> <p>Total timeout in seconds.</p> required <code>compress</code> <code>Union[str, bool]</code> <p>Whether to compress the request body. Defaults to \"auto\", which will compress if the body is larger than 1024 bytes.</p> <code>'auto'</code> <p>Returns:</p> Name Type Description <code>VespaAsyncLayer</code> <code>VespaSync</code> <p>Instance of Vespa asynchronous layer.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.wait_for_application_up","title":"<code>wait_for_application_up(max_wait=300)</code>","text":"<p>Wait for application endpoint ready (/ApplicationStatus).</p> <p>Parameters:</p> Name Type Description Default <code>max_wait</code> <code>int</code> <p>Seconds to wait for the application endpoint.</p> <code>300</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If not able to reach endpoint within <code>max_wait</code> or the client fails to authenticate.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.get_application_status","title":"<code>get_application_status()</code>","text":"<p>Get application status (/ApplicationStatus).</p> <p>Returns:</p> Type Description <code>Optional[Response]</code> <p>None</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.get_model_endpoint","title":"<code>get_model_endpoint(model_id=None)</code>","text":"<p>Get stateless model evaluation endpoints.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.query","title":"<code>query(body=None, groupname=None, **kwargs)</code>","text":"<p>Send a query request to the Vespa application.</p> <p>Send 'body' containing all the request parameters.</p> <p>Parameters:</p> Name Type Description Default <code>body</code> <code>dict</code> <p>Dictionary containing request parameters.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname used with streaming search.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Extra Vespa Query API parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VespaQueryResponse</code> <p>The response from the Vespa application.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.feed_data_point","title":"<code>feed_data_point(schema, data_id, fields, namespace=None, groupname=None, compress='auto', **kwargs)</code>","text":"<p>Feed a data point to a Vespa app. Will create a new VespaSync with connection overhead.</p> Example usage <pre><code>app = Vespa(url=\"localhost\", port=8080)\ndata_id = \"1\",\nfields = {\n        \"field1\": \"value1\",\n    }\nwith VespaSync(app) as sync_app:\n    response = sync_app.feed_data_point(\n        schema=\"schema_name\",\n        data_id=data_id,\n        fields=fields\n    )\nprint(response)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The schema that we are sending data to.</p> required <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>fields</code> <code>dict</code> <p>Dictionary containing all the fields required by the <code>schema</code>.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are sending data to.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname that we are sending data to.</p> <code>None</code> <code>compress</code> <code>Union[str, bool]</code> <p>Whether to compress the request body. Defaults to \"auto\", which will compress if the body is larger than 1024 bytes.</p> <code>'auto'</code> <p>Returns:</p> Name Type Description <code>VespaResponse</code> <code>VespaResponse</code> <p>The response of the HTTP POST request.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.feed_iterable","title":"<code>feed_iterable(iter, schema=None, namespace=None, callback=None, operation_type='feed', max_queue_size=1000, max_workers=8, max_connections=16, compress='auto', **kwargs)</code>","text":"<p>Feed data from an Iterable of Dict with the keys 'id' and 'fields' to be used in the <code>feed_data_point</code> function.</p> <p>Uses a queue to feed data in parallel with a thread pool. The result of each operation is forwarded to the user-provided callback function that can process the returned <code>VespaResponse</code>.</p> Example usage <pre><code>app = Vespa(url=\"localhost\", port=8080)\ndata = [\n    {\"id\": \"1\", \"fields\": {\"field1\": \"value1\"}},\n    {\"id\": \"2\", \"fields\": {\"field1\": \"value2\"}},\n]\ndef callback(response, id):\n    print(f\"Response for id {id}: {response.status_code}\")\napp.feed_iterable(data, schema=\"schema_name\", callback=callback)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>iter</code> <code>Iterable[dict]</code> <p>An iterable of Dict containing the keys 'id' and 'fields' to be used in the <code>feed_data_point</code>. Note that this 'id' is only the last part of the full document id, which will be generated automatically by pyvespa.</p> required <code>schema</code> <code>str</code> <p>The Vespa schema name that we are sending data to.</p> <code>None</code> <code>namespace</code> <code>str</code> <p>The Vespa document id namespace. If no namespace is provided, the schema is used.</p> <code>None</code> <code>callback</code> <code>function</code> <p>A callback function to be called on each result. Signature <code>callback(response: VespaResponse, id: str)</code>.</p> <code>None</code> <code>operation_type</code> <code>str</code> <p>The operation to perform. Defaults to <code>feed</code>. Valid values are <code>feed</code>, <code>update</code>, or <code>delete</code>.</p> <code>'feed'</code> <code>max_queue_size</code> <code>int</code> <p>The maximum size of the blocking queue and max in-flight operations.</p> <code>1000</code> <code>max_workers</code> <code>int</code> <p>The maximum number of workers in the threadpool executor.</p> <code>8</code> <code>max_connections</code> <code>int</code> <p>The maximum number of persisted connections to the Vespa endpoint.</p> <code>16</code> <code>compress</code> <code>Union[str, bool]</code> <p>Whether to compress the request body. Defaults to \"auto\", which will compress if the body is larger than 1024 bytes.</p> <code>'auto'</code> <code>**kwargs</code> <code>dict</code> <p>Additional parameters passed to the respective operation type specific function (<code>_data_point</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.feed_async_iterable","title":"<code>feed_async_iterable(iter, schema=None, namespace=None, callback=None, operation_type='feed', max_queue_size=1000, max_workers=64, max_connections=1, **kwargs)</code>","text":"<p>Feed data asynchronously using httpx.AsyncClient with HTTP/2. Feed from an Iterable of Dict with the keys 'id' and 'fields' to be used in the <code>feed_data_point</code> function. The result of each operation is forwarded to the user-provided callback function that can process the returned <code>VespaResponse</code>. Prefer using this method over <code>feed_iterable</code> when the operation is I/O bound from the client side.</p> Example usage <pre><code>app = Vespa(url=\"localhost\", port=8080)\ndata = [\n    {\"id\": \"1\", \"fields\": {\"field1\": \"value1\"}},\n    {\"id\": \"2\", \"fields\": {\"field1\": \"value2\"}},\n]\ndef callback(response, id):\n    print(f\"Response for id {id}: {response.status_code}\")\napp.feed_async_iterable(data, schema=\"schema_name\", callback=callback)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>iter</code> <code>Iterable[dict]</code> <p>An iterable of Dict containing the keys 'id' and 'fields' to be used in the <code>feed_data_point</code>. Note that this 'id' is only the last part of the full document id, which will be generated automatically by pyvespa.</p> required <code>schema</code> <code>str</code> <p>The Vespa schema name that we are sending data to.</p> <code>None</code> <code>namespace</code> <code>str</code> <p>The Vespa document id namespace. If no namespace is provided, the schema is used.</p> <code>None</code> <code>callback</code> <code>function</code> <p>A callback function to be called on each result. Signature <code>callback(response: VespaResponse, id: str)</code>.</p> <code>None</code> <code>operation_type</code> <code>str</code> <p>The operation to perform. Defaults to <code>feed</code>. Valid values are <code>feed</code>, <code>update</code>, or <code>delete</code>.</p> <code>'feed'</code> <code>max_queue_size</code> <code>int</code> <p>The maximum number of tasks waiting to be processed. Useful to limit memory usage. Default is 1000.</p> <code>1000</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent requests to have in-flight, bound by an asyncio.Semaphore, that needs to be acquired by a submit task. Increase if the server is scaled to handle more requests.</p> <code>64</code> <code>max_connections</code> <code>int</code> <p>The maximum number of connections passed to httpx.AsyncClient to the Vespa endpoint. As HTTP/2 is used, only one connection is needed.</p> <code>1</code> <code>**kwargs</code> <code>dict</code> <p>Additional parameters passed to the respective operation type-specific function (<code>_data_point</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.query_many_async","title":"<code>query_many_async(queries, num_connections=1, max_concurrent=100, client_kwargs={}, **query_kwargs)</code>  <code>async</code>","text":"<p>Execute many queries asynchronously using httpx.AsyncClient. Number of concurrent requests is controlled by the <code>max_concurrent</code> parameter. Each query will be retried up to 3 times using an exponential backoff strategy.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Iterable[dict]</code> <p>Iterable of query bodies (dictionaries) to be sent.</p> required <code>num_connections</code> <code>int</code> <p>Number of connections to be used in the asynchronous client (uses HTTP/2). Defaults to 1.</p> <code>1</code> <code>max_concurrent</code> <code>int</code> <p>Maximum concurrent requests to be sent. Defaults to 100. Be careful with increasing too much.</p> <code>100</code> <code>client_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the httpx.AsyncClient.</p> <code>{}</code> <code>**query_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the query method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[VespaQueryResponse]</code> <p>List[VespaQueryResponse]: List of <code>VespaQueryResponse</code> objects.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.query_many","title":"<code>query_many(queries, num_connections=1, max_concurrent=100, client_kwargs={}, **query_kwargs)</code>","text":"<p>Execute many queries asynchronously using httpx.AsyncClient. This method is a wrapper around the <code>query_many_async</code> method that uses the asyncio event loop to run the coroutine. Number of concurrent requests is controlled by the <code>max_concurrent</code> parameter. Each query will be retried up to 3 times using an exponential backoff strategy.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Iterable[dict]</code> <p>Iterable of query bodies (dictionaries) to be sent.</p> required <code>num_connections</code> <code>int</code> <p>Number of connections to be used in the asynchronous client (uses HTTP/2). Defaults to 1.</p> <code>1</code> <code>max_concurrent</code> <code>int</code> <p>Maximum concurrent requests to be sent. Defaults to 100. Be careful with increasing too much.</p> <code>100</code> <code>client_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the httpx.AsyncClient.</p> <code>{}</code> <code>**query_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the query method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[VespaQueryResponse]</code> <p>List[VespaQueryResponse]: List of <code>VespaQueryResponse</code> objects.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.delete_data","title":"<code>delete_data(schema, data_id, namespace=None, groupname=None, **kwargs)</code>","text":"<p>Delete a data point from a Vespa app.</p> Example usage <pre><code>app = Vespa(url=\"localhost\", port=8080)\nresponse = app.delete_data(schema=\"schema_name\", data_id=\"1\")\nprint(response)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The schema that we are deleting data from.</p> required <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are deleting data from. If no namespace is provided, the schema is used.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname that we are deleting data from.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the HTTP DELETE request. See Vespa API documentation for more details.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>VespaResponse</code> <p>The response of the HTTP DELETE request.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.delete_all_docs","title":"<code>delete_all_docs(content_cluster_name, schema, namespace=None, slices=1, **kwargs)</code>","text":"<p>Delete all documents associated with the schema. This might block for a long time as it requires sending multiple delete requests to complete.</p> <p>Parameters:</p> Name Type Description Default <code>content_cluster_name</code> <code>str</code> <p>Name of content cluster to GET from, or visit.</p> required <code>schema</code> <code>str</code> <p>The schema that we are deleting data from.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are deleting data from. If no namespace is provided, the schema is used.</p> <code>None</code> <code>slices</code> <code>int</code> <p>Number of slices to use for parallel delete requests. Defaults to 1.</p> <code>1</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the HTTP DELETE request. See Vespa API documentation for more details.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>The response of the HTTP DELETE request.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.visit","title":"<code>visit(content_cluster_name, schema=None, namespace=None, slices=1, selection='true', wanted_document_count=500, slice_id=None, **kwargs)</code>","text":"<p>Visit all documents associated with the schema and matching the selection.</p> <p>Will run each slice on a separate thread, for each slice yields the response for each page.</p> Example usage <pre><code>for slice in app.visit(schema=\"schema_name\", slices=2):\n    for response in slice:\n        print(response.json)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>content_cluster_name</code> <code>str</code> <p>Name of content cluster to GET from.</p> required <code>schema</code> <code>str</code> <p>The schema that we are visiting data from.</p> <code>None</code> <code>namespace</code> <code>str</code> <p>The namespace that we are visiting data from.</p> <code>None</code> <code>slices</code> <code>int</code> <p>Number of slices to use for parallel GET.</p> <code>1</code> <code>selection</code> <code>str</code> <p>Selection expression to filter documents.</p> <code>'true'</code> <code>wanted_document_count</code> <code>int</code> <p>Best effort number of documents to retrieve for each request. May contain less if there are not enough documents left.</p> <code>500</code> <code>slice_id</code> <code>int</code> <p>Slice id to use for the visit. If None, all slices will be used.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional HTTP request parameters. See Vespa API documentation.</p> <code>{}</code> <p>Yields:</p> Type Description <code>Generator[VespaVisitResponse, None, None]</code> <p>Generator[Generator[Response]]: A generator of slices, each containing a generator of responses.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If an HTTP error occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.get_data","title":"<code>get_data(schema, data_id, namespace=None, groupname=None, raise_on_not_found=False, **kwargs)</code>","text":"<p>Get a data point from a Vespa app.</p> <p>Parameters:</p> Name Type Description Default <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>schema</code> <code>str</code> <p>The schema that we are getting data from. Will attempt to infer schema name if not provided.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are getting data from. If no namespace is provided, the schema is used.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname that we are getting data from.</p> <code>None</code> <code>raise_on_not_found</code> <code>bool</code> <p>Raise an exception if the data_id is not found. Default is False.</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the HTTP GET request. See Vespa API documentation.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>VespaResponse</code> <p>The response of the HTTP GET request.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.update_data","title":"<code>update_data(schema, data_id, fields, create=False, namespace=None, groupname=None, compress='auto', **kwargs)</code>","text":"<p>Update a data point in a Vespa app.</p> Example usage <pre><code>vespa = Vespa(url=\"localhost\", port=8080)\n\nfields = {\"mystringfield\": \"value1\", \"myintfield\": 42}\nresponse = vespa.update_data(schema=\"schema_name\", data_id=\"id1\", fields=fields)\n# or, with partial update, setting auto_assign=False\nfields = {\"myintfield\": {\"increment\": 1}}\nresponse = vespa.update_data(schema=\"schema_name\", data_id=\"id1\", fields=fields, auto_assign=False)\nprint(response.json)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The schema that we are updating data.</p> required <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>fields</code> <code>dict</code> <p>Dict containing all the fields you want to update.</p> required <code>create</code> <code>bool</code> <p>If true, updates to non-existent documents will create an empty document to update.</p> <code>False</code> <code>auto_assign</code> <code>bool</code> <p>Assumes <code>fields</code>-parameter is an assignment operation. If set to false, the fields parameter should be a dictionary including the update operation.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are updating data. If no namespace is provided, the schema is used.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname that we are updating data.</p> <code>None</code> <code>compress</code> <code>Union[str, bool]</code> <p>Whether to compress the request body. Defaults to \"auto\", which will compress if the body is larger than 1024 bytes.</p> <code>'auto'</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the HTTP PUT request. See Vespa API documentation.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>VespaResponse</code> <p>The response of the HTTP PUT request.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.get_model_from_application_package","title":"<code>get_model_from_application_package(model_name)</code>","text":"<p>Get model definition from application package, if available.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.predict","title":"<code>predict(x, model_id, function_name='output_0')</code>","text":"<p>Obtain a stateless model evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>various</code> <p>Input where the format depends on the task that the model is serving.</p> required <code>model_id</code> <code>str</code> <p>The id of the model used to serve the prediction.</p> required <code>function_name</code> <code>str</code> <p>The name of the output function to be evaluated.</p> <code>'output_0'</code> <p>Returns:</p> Name Type Description <code>var</code> <p>Model prediction.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.get_document_v1_path","title":"<code>get_document_v1_path(id, schema=None, namespace=None, group=None, number=None)</code>","text":"<p>Convert to document v1 path.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The id of the document.</p> required <code>namespace</code> <code>str</code> <p>The namespace of the document.</p> <code>None</code> <code>schema</code> <code>str</code> <p>The schema of the document.</p> <code>None</code> <code>group</code> <code>str</code> <p>The group of the document.</p> <code>None</code> <code>number</code> <code>int</code> <p>The number of the document.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the document v1 endpoint.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync","title":"<code>VespaSync(app, pool_maxsize=10, pool_connections=10, compress='auto')</code>","text":"<p>               Bases: <code>object</code></p> <p>Class to handle synchronous requests to Vespa. This class is intended to be used as a context manager.</p> Example usage <pre><code>with VespaSync(app) as sync_app:\n    response = sync_app.query(body=body)\nprint(response)\n</code></pre> <p>Can also be accessed directly through <code>Vespa.syncio</code>:     <pre><code>app = Vespa(url=\"localhost\", port=8080)\nwith app.syncio() as sync_app:\n    response = sync_app.query(body=body)\n</code></pre> See also <code>Vespa.feed_iterable</code> for a convenient way to feed data synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>Vespa</code> <p>Vespa app object.</p> required <code>pool_maxsize</code> <code>int</code> <p>The maximum number of connections to save in the pool. Defaults to 10.</p> <code>10</code> <code>pool_connections</code> <code>int</code> <p>The number of urllib3 connection pools to cache. Defaults to 10.</p> <code>10</code> <code>compress</code> <code>Union[str, bool]</code> <p>Whether to compress the request body. Defaults to \"auto\", which will compress if the body is larger than 1024 bytes.</p> <code>'auto'</code>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.get_model_endpoint","title":"<code>get_model_endpoint(model_id=None)</code>","text":"<p>Get model evaluation endpoints.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.predict","title":"<code>predict(model_id, function_name, encoded_tokens)</code>","text":"<p>Obtain a stateless model evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The id of the model used to serve the prediction.</p> required <code>function_name</code> <code>str</code> <p>The name of the output function to be evaluated.</p> required <code>encoded_tokens</code> <code>str</code> <p>URL-encoded input to the model.</p> required <p>Returns:</p> Type Description <p>The model prediction.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.feed_data_point","title":"<code>feed_data_point(schema, data_id, fields, namespace=None, groupname=None, **kwargs)</code>","text":"<p>Feed a data point to a Vespa app.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The schema that we are sending data to.</p> required <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>fields</code> <code>dict</code> <p>Dict containing all the fields required by the <code>schema</code>.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are sending data to. If no namespace is provided, the schema is used.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The group that we are sending data to.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional HTTP request parameters. See: https://docs.vespa.ai/en/reference/document-v1-api-reference.html#request-parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VespaResponse</code> <p>The response of the HTTP POST request.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If one occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.query","title":"<code>query(body=None, groupname=None, **kwargs)</code>","text":"<p>Send a query request to the Vespa application.</p> <p>Parameters:</p> Name Type Description Default <code>body</code> <code>dict</code> <p>Dict containing all the request parameters.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname used in streaming search.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional valid Vespa HTTP Query API parameters. See: https://docs.vespa.ai/en/reference/query-api-reference.html.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VespaQueryResponse</code> <p>The request body if <code>debug_request</code> is True, or the result from the Vespa application.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If one occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.delete_data","title":"<code>delete_data(schema, data_id, namespace=None, groupname=None, **kwargs)</code>","text":"<p>Delete a data point from a Vespa app.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The schema that we are deleting data from.</p> required <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are deleting data from.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional HTTP request parameters. See: https://docs.vespa.ai/en/reference/document-v1-api-reference.html#request-parameters.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>VespaResponse</code> <p>The response of the HTTP DELETE request.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If one occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.delete_all_docs","title":"<code>delete_all_docs(content_cluster_name, schema, namespace=None, slices=1, **kwargs)</code>","text":"<p>Delete all documents associated with the schema.</p> <p>Parameters:</p> Name Type Description Default <code>content_cluster_name</code> <code>str</code> <p>Name of content cluster to GET from or visit.</p> required <code>schema</code> <code>str</code> <p>The schema that we are deleting data from.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are deleting data from.</p> <code>None</code> <code>slices</code> <code>int</code> <p>Number of slices to use for parallel delete.</p> <code>1</code> <code>**kwargs</code> <code>dict</code> <p>Additional HTTP request parameters. See: https://docs.vespa.ai/en/reference/document-v1-api-reference.html#request-parameters.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>None</code> <p>The response of the HTTP DELETE request.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If one occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.visit","title":"<code>visit(content_cluster_name, schema=None, namespace=None, slices=1, selection='true', wanted_document_count=500, slice_id=None, **kwargs)</code>","text":"<p>Visit all documents associated with the schema and matching the selection.</p> <p>This method will run each slice on a separate thread, yielding the response for each page for each slice.</p> <p>Parameters:</p> Name Type Description Default <code>content_cluster_name</code> <code>str</code> <p>Name of content cluster to GET from.</p> required <code>schema</code> <code>str</code> <p>The schema that we are visiting data from.</p> <code>None</code> <code>namespace</code> <code>str</code> <p>The namespace that we are visiting data from.</p> <code>None</code> <code>slices</code> <code>int</code> <p>Number of slices to use for parallel GET.</p> <code>1</code> <code>wanted_document_count</code> <code>int</code> <p>Best effort number of documents to retrieve for each request. May contain fewer if there are not enough documents left.</p> <code>500</code> <code>selection</code> <code>str</code> <p>Selection expression to use. Defaults to \"true\". See: https://docs.vespa.ai/en/reference/document-select-language.html.</p> <code>'true'</code> <code>slice_id</code> <code>int</code> <p>Slice id to use. Defaults to -1, which means all slices.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional HTTP request parameters. See: https://docs.vespa.ai/en/reference/document-v1-api-reference.html#request-parameters.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>generator</code> <code>None</code> <p>A generator of slices, each containing a generator of responses.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If one occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.get_data","title":"<code>get_data(schema, data_id, namespace=None, groupname=None, raise_on_not_found=False, **kwargs)</code>","text":"<p>Get a data point from a Vespa app.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The schema that we are getting data from.</p> required <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are getting data from.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname used to get data.</p> <code>None</code> <code>raise_on_not_found</code> <code>bool</code> <p>Raise an exception if the document is not found. Default is False.</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Additional HTTP request parameters. See: https://docs.vespa.ai/en/reference/document-v1-api-reference.html#request-parameters.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>VespaResponse</code> <p>The response of the HTTP GET request.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If one occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.update_data","title":"<code>update_data(schema, data_id, fields, create=False, auto_assign=True, namespace=None, groupname=None, **kwargs)</code>","text":"<p>Update a data point in a Vespa app.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The schema that we are updating data in.</p> required <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>fields</code> <code>dict</code> <p>Dict containing all the fields you want to update.</p> required <code>create</code> <code>bool</code> <p>If true, updates to non-existent documents will create an empty document to update. Default is False.</p> <code>False</code> <code>auto_assign</code> <code>bool</code> <p>Assumes <code>fields</code>-parameter is an assignment operation. If set to False, the fields parameter should include the update operation. Default is True.</p> <code>True</code> <code>namespace</code> <code>str</code> <p>The namespace that we are updating data in.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname used to update data.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional HTTP request parameters. See: &lt;https://docs.vespa.ai/en/reference/document-v1-api-reference.html#request-parameters.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>VespaResponse</code> <p>The response of the HTTP PUT request.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If one occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaAsync","title":"<code>VespaAsync(app, connections=1, total_timeout=None, timeout=httpx.Timeout(5), **kwargs)</code>","text":"<p>               Bases: <code>object</code></p> <p>Class to handle asynchronous HTTP connections to Vespa.</p> <p>Uses <code>httpx</code> as the async HTTP client, and HTTP/2 by default. This class is intended to be used as a context manager.</p> <p>Basic usage:     <pre><code>async with VespaAsync(app) as async_app:\n    response = await async_app.query(\n        body={\"yql\": \"select * from sources * where title contains 'music';\"}\n    )\n</code></pre></p> <p>Passing custom timeout and limits:     <pre><code>import httpx\n\ntimeout = httpx.Timeout(10.0, connect=5.0)\nlimits = httpx.Limits(max_connections=10, max_keepalive_connections=5)\n\nasync with VespaAsync(app, timeout=timeout, limits=limits) as async_app:\n    response = await async_app.query(\n        body={\"yql\": \"select * from sources * where title contains 'music';\"}\n    )\n</code></pre></p> <p>Using additional kwargs (e.g., proxies):     <pre><code>proxies = \"http://localhost:8080\"\n\nasync with VespaAsync(app, proxies=proxies) as async_app:\n    response = await async_app.query(\n        body={\"yql\": \"select * from sources * where title contains 'music';\"}\n    )\n</code></pre></p> <p>Accessing via <code>Vespa.asyncio</code>:     <pre><code>app = Vespa(url=\"localhost\", port=8080)\nasync with app.asyncio(timeout=timeout, limits=limits) as async_app:\n    response = await async_app.query(\n        body={\"yql\": \"select * from sources * where title contains 'music';\"}\n    )\n</code></pre></p> <p>See also <code>Vespa.feed_async_iterable</code> for a convenient interface to async data feeding.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>Vespa</code> <p>Vespa application object.</p> required <code>connections</code> <code>Optional[int]</code> <p>Number of connections. Defaults to 1 as HTTP/2 is multiplexed.</p> <code>1</code> <code>total_timeout</code> <code>int</code> <p>Deprecated. Will be ignored and removed in future versions. Use <code>timeout</code> to pass an <code>httpx.Timeout</code> object instead.</p> <code>None</code> <code>timeout</code> <code>Timeout</code> <p>Timeout settings for the <code>httpx.AsyncClient</code>. Defaults to <code>httpx.Timeout(5)</code>.</p> <code>Timeout(5)</code> <code>**kwargs</code> <p>Additional arguments to be passed to the <code>httpx.AsyncClient</code>. See HTTPX AsyncClient documentation for more details.</p> <code>{}</code> Note <ul> <li>Passing <code>timeout</code> allows you to configure timeouts for connect, read, write, and overall request time.</li> <li>The <code>limits</code> parameter can be used to control connection pooling behavior, such as the maximum number of concurrent connections.</li> <li>See HTTPX documentation for more information on <code>httpx</code> and its features.</li> </ul>"},{"location":"api/vespa/application.html#vespa.application.raise_for_status","title":"<code>raise_for_status(response, raise_on_not_found=False)</code>","text":"<p>Raises an appropriate error if necessary.</p> <p>If the response contains an error message, <code>VespaError</code> is raised along with <code>HTTPError</code> to provide more details.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response</code> <p>Response object from the Vespa API.</p> required <code>raise_on_not_found</code> <code>bool</code> <p>If True, raises <code>HTTPError</code> if status_code is 404.</p> <code>False</code> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If status_code is between 400 and 599.</p> <code>VespaError</code> <p>If the response JSON contains an error message.</p>"},{"location":"api/vespa/deployment.html","title":"Deployment","text":""},{"location":"api/vespa/deployment.html#vespa.deployment","title":"<code>vespa.deployment</code>","text":""},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDeployment","title":"<code>VespaDeployment</code>","text":""},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDeployment.read_app_package_from_disk","title":"<code>read_app_package_from_disk(application_root)</code>","text":"<p>Reads the contents of an application package on disk into a zip file.</p> <p>Parameters:</p> Name Type Description Default <code>application_root</code> <code>str</code> <p>The directory root of the application package.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The zipped application package.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker","title":"<code>VespaDocker(port=8080, container_memory=4 * 1024 ** 3, output_file=sys.stdout, container=None, container_image='vespaengine/vespa', volumes=None, cfgsrv_port=19071, debug_port=5005)</code>","text":"<p>               Bases: <code>VespaDeployment</code></p> <p>Manage Docker deployments.</p> <p>Make sure to start the Docker daemon before instantiating this class.</p> Example usage <pre><code>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker(port=8080)\n# or initialize from a running container:\nvespa_docker = VespaDocker('http://localhost', 8080, None, None, 4294967296, 'vespaengine/vespa')\n</code></pre> <p>Note:</p> <p>It is NOT possible to refer to Volume Mounts in your Application Package. This means that for example .onnx-model files that are part of the Application Package must be on your host machine, so that it can be uploaded as part of the Application Package to the Vespa container.</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int</code> <p>The port for the container. Default is 8080.</p> <code>8080</code> <code>cfgsrv_port</code> <code>int</code> <p>The Vespa Config Server port. Default is 19071.</p> <code>19071</code> <code>debug_port</code> <code>int</code> <p>The port to connect to for debugging the Vespa container. Default is 5005.</p> <code>5005</code> <code>output_file</code> <code>str</code> <p>The file to write output messages to.</p> <code>stdout</code> <code>container_memory</code> <code>int</code> <p>Memory available to the container in bytes. Default is 4GB.</p> <code>4 * 1024 ** 3</code> <code>container</code> <code>str</code> <p>Used when instantiating <code>VespaDocker</code> from a running container.</p> <code>None</code> <code>volumes</code> <code>list of str</code> <p>A list of volume mount strings, such as <code>['/home/user1/:/mnt/vol2', '/var/www:/mnt/vol1']</code>. The Application Package cannot reference volume mounts.</p> <code>None</code> <code>container_image</code> <code>str</code> <p>The Docker container image to use.</p> <code>'vespaengine/vespa'</code>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker.from_container_name_or_id","title":"<code>from_container_name_or_id(name_or_id, output_file=sys.stdout)</code>  <code>staticmethod</code>","text":"<p>Instantiate VespaDocker from a running container.</p> <p>Parameters:</p> Name Type Description Default <code>name_or_id</code> <code>str</code> <p>The name or id of the running container.</p> required <code>output_file</code> <code>str</code> <p>The file to write output messages to.</p> <code>stdout</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified container is not found.</p> <p>Returns:</p> Name Type Description <code>VespaDocker</code> <code>VespaDocker</code> <p>An instance of VespaDocker associated with the running container.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker.deploy","title":"<code>deploy(application_package, max_wait_configserver=60, max_wait_deployment=300, max_wait_docker=300, debug=False)</code>","text":"<p>Deploy the application package into a Vespa container.</p> <p>Parameters:</p> Name Type Description Default <code>application_package</code> <code>ApplicationPackage</code> <p>The application package to be deployed.</p> required <code>max_wait_configserver</code> <code>int</code> <p>Maximum seconds to wait for the config server to start.</p> <code>60</code> <code>max_wait_deployment</code> <code>int</code> <p>Maximum seconds to wait for the deployment to complete.</p> <code>300</code> <code>max_wait_docker</code> <code>int</code> <p>Maximum seconds to wait for the Docker container to start.</p> <code>300</code> <code>debug</code> <code>bool</code> <p>If True, adds the configured debug_port to the Docker port mapping.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>VespaConnection</code> <code>Vespa</code> <p>A Vespa connection instance once the deployment is complete.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker.deploy_from_disk","title":"<code>deploy_from_disk(application_name, application_root, max_wait_configserver=60, max_wait_application=300, docker_timeout=300, debug=False)</code>","text":"<p>Deploy from a directory tree.</p> <p>This method is used when making changes to application package files that are not supported by pyvespa. This is why this method is not found in the ApplicationPackage class.</p> <p>Parameters:</p> Name Type Description Default <code>application_name</code> <code>str</code> <p>The name of the application package.</p> required <code>application_root</code> <code>str</code> <p>The root directory of the application package.</p> required <code>debug</code> <code>bool</code> <p>If True, adds the configured debug_port to the Docker port mapping.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>VespaConnection</code> <code>Vespa</code> <p>A Vespa connection instance once the deployment is complete.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker.wait_for_config_server_start","title":"<code>wait_for_config_server_start(max_wait=300)</code>","text":"<p>Waits for the Config Server to start inside the Docker image.</p> <p>Parameters:</p> Name Type Description Default <code>max_wait</code> <code>int</code> <p>The maximum number of seconds to wait for the application endpoint to become available.</p> <code>300</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the config server does not start within the specified max_wait time.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker.start_services","title":"<code>start_services(max_wait=120)</code>","text":"<p>Start Vespa services inside the Docker image, first waiting for the Config Server, then for other services.</p> <p>Parameters:</p> Name Type Description Default <code>max_wait</code> <code>int</code> <p>The maximum number of seconds to wait for the application endpoint to become available.</p> <code>120</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a container has not been set or the services fail to start within the specified max_wait time.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker.stop_services","title":"<code>stop_services()</code>","text":"<p>Stop Vespa services inside the Docker image, first stopping the services, then stopping the Config Server.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a container has not been set or an error occurs while stopping the services.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker.restart_services","title":"<code>restart_services()</code>","text":"<p>Restart Vespa services inside the Docker image. This is equivalent to calling  <code>self.stop_services()</code> followed by <code>self.start_services()</code>.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a container has not been set or an error occurs during the restart process.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud","title":"<code>VespaCloud(tenant, application, application_package=None, key_location=None, key_content=None, auth_client_token_id=None, output_file=sys.stdout, application_root=None, cluster=None, instance='default')</code>","text":"<p>               Bases: <code>VespaDeployment</code></p> <p>Deploy an application to the Vespa Cloud (cloud.vespa.ai).</p> <p>There are several ways to initialize VespaCloud: - Application source: From a Python-defined application package or from the application_root folder. - Control plane access: Using an API key (must be added to Vespa Cloud Console) or an access token, obtained by interactive login. - Data plane access: mTLS is used by default, but Vespa applications can also be configured to use token-based authentication (token must be added to Vespa Cloud Console, and the corresponding auth_token_id must be provided).</p> Example usage <pre><code># 1. Initialize VespaCloud with an application package and existing API key for control plane access.\nvespa_cloud = VespaCloud(\n    tenant=\"my-tenant\",\n    application=\"my-application\",\n    application_package=app_package,\n    key_location=\"/path/to/private-key.pem\",\n)\n\n# 2. Initialize VespaCloud from disk folder by interactive control plane auth.\nvespa_cloud = VespaCloud(\n    tenant=\"my-tenant\",\n    application=\"my-application\",\n    application_root=\"/path/to/application\",\n)\n\n# 3. Initialize VespaCloud with an application package and token-based data plane access.\nvespa_cloud = VespaCloud(\n    tenant=\"my-tenant\",\n    application=\"my-application\",\n    application_package=app_package,\n    auth_client_token_id=\"my-token-id\", # Must be added in Vespa Cloud Console\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tenant</code> <code>str</code> <p>Tenant name registered in the Vespa Cloud.</p> required <code>application</code> <code>str</code> <p>Application name in the Vespa Cloud.</p> required <code>application_package</code> <code>ApplicationPackage</code> <p>Application package to be deployed. Either this or application_root must be set.</p> <code>None</code> <code>key_location</code> <code>str</code> <p>Location of the control plane key used for signing HTTP requests to the Vespa Cloud.</p> <code>None</code> <code>key_content</code> <code>str</code> <p>Content of the control plane key used for signing HTTP requests to the Vespa Cloud. Use only when the key file is not available.</p> <code>None</code> <code>auth_client_token_id</code> <code>str</code> <p>Token-based data plane authentication. This token name must be configured in the Vespa Cloud Console. It configures Vespa's services.xml, and the token must have read and write permissions.</p> <code>None</code> <code>output_file</code> <code>str</code> <p>Output file to write output messages. Default is sys.stdout.</p> <code>stdout</code> <code>application_root</code> <code>str</code> <p>Directory for the application root (location of services.xml, models/, schemas/, etc.). If the application is packaged with Maven, use the generated <code>&lt;myapp&gt;/target/application</code> directory.</p> <code>None</code> <code>cluster</code> <code>str</code> <p>Name of the cluster to target when retrieving endpoints. This affects which endpoints are used for initializing the :class:<code>Vespa</code> instance in <code>VespaCloud.get_application</code> and <code>VespaCloud.deploy</code>.</p> <code>None</code> <code>instance</code> <code>str</code> <p>Name of the application instance. Default is \"default\".</p> <code>'default'</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If deployment fails.</p> <p>Returns:</p> Name Type Description <code>Vespa</code> <code>None</code> <p>A Vespa connection instance for interacting with the deployed application.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.deploy","title":"<code>deploy(instance='default', disk_folder=None, version=None, max_wait=1800)</code>","text":"<p>Deploy the given application package as the given instance in the Vespa Cloud dev environment.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>Name of this instance of the application in the Vespa Cloud.</p> <code>'default'</code> <code>disk_folder</code> <code>str</code> <p>Disk folder to save the required Vespa config files. Defaults to the application name folder within the user's current working directory.</p> <code>None</code> <code>version</code> <code>str</code> <p>Vespa version to use for deployment. Defaults to None, meaning the latest version. Should only be set based on instructions from the Vespa team. Must be a valid Vespa version, e.g., \"8.435.13\".</p> <code>None</code> <code>max_wait</code> <code>int</code> <p>Seconds to wait for the deployment to complete.</p> <code>1800</code> <p>Returns:</p> Name Type Description <code>Vespa</code> <code>Vespa</code> <p>A Vespa connection instance. This instance connects to the mTLS endpoint. To connect to the token endpoint, use <code>VespaCloud.get_application(endpoint_type=\"token\")</code>.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If deployment fails or if there are issues with the deployment process.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.deploy_to_prod","title":"<code>deploy_to_prod(instance='default', application_root=None, source_url='')</code>","text":"<p>Deploy the given application package as the given instance in the Vespa Cloud prod environment. NB! This feature is experimental and may fail in unexpected ways. Expect better support in future releases.</p> <p>If submitting an application that is not yet packaged, tests should be located in /tests. If submitting an application packaged with maven, application_root should refer to the generated /target/application directory. <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>Name of this instance of the application in the Vespa Cloud.</p> <code>'default'</code> <code>application_root</code> <code>str</code> <p>Path to either save the required Vespa config files (if initialized with application_package) or read them from (if initialized with application_root).</p> <code>None</code> <code>source_url</code> <code>str</code> <p>Optional source URL (including commit hash) for the deployment. This is a URL to the source code repository, e.g., GitHub, that is used to build the application package. Example: https://github.com/vespa-cloud/vector-search/commit/474d7771bd938d35dc5dcfd407c21c019d15df3c. The source URL will show up in the Vespa Cloud Console next to the build number.</p> <code>''</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If deployment fails or if there are issues with the deployment process.</p> Note <p>This feature is still experimental and may not have full stability in production. Future releases will provide better support for this functionality.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.get_application","title":"<code>get_application(instance='default', environment='dev', endpoint_type='mtls', vespa_cloud_secret_token=None, region=None, max_wait=60)</code>","text":"<p>Get a connection to the Vespa application instance. Will only work if the application is already deployed.</p> Example usage <pre><code>vespa_cloud = VespaCloud(...)\napp: Vespa = vespa_cloud.get_application()\n# Feed, query, visit, etc.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>Name of this instance of the application in the Vespa Cloud. Default is \"default\".</p> <code>'default'</code> <code>environment</code> <code>str</code> <p>Environment of the application. Default is \"dev\". Options are \"dev\" or \"prod\".</p> <code>'dev'</code> <code>endpoint_type</code> <code>str</code> <p>Type of endpoint to connect to. Default is \"mtls\". Options are \"mtls\" or \"token\".</p> <code>'mtls'</code> <code>vespa_cloud_secret_token</code> <code>str</code> <p>Vespa Cloud Secret Token. Only required if endpoint_type is \"token\".</p> <code>None</code> <code>region</code> <code>str</code> <p>Region of the application in Vespa Cloud, e.g., \"aws-us-east-1c\". If not provided, the first region from the environment will be used.</p> <code>None</code> <code>max_wait</code> <code>int</code> <p>Seconds to wait for the application to be up. Default is 60 seconds.</p> <code>60</code> <p>Returns:</p> Name Type Description <code>Vespa</code> <code>Vespa</code> <p>Vespa application instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the application is not yet deployed or there are issues retrieving the connection.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.check_production_build_status","title":"<code>check_production_build_status(build_no)</code>","text":"<p>Check the status of a production build. Useful for example in CI/CD pipelines to check when a build has converged.</p> Example usage <pre><code>vespa_cloud = VespaCloud(...)\nbuild_no = vespa_cloud.deploy_to_prod()\nstatus = vespa_cloud.check_production_build_status(build_no)\n# This can yield one of three responses:\n# 1. If the revision (build_no), or higher, has successfully converged everywhere, and nothing older has then been deployed on top of that again. Nothing more will happen in this case.\n# {\n#     \"deployed\": True,\n#     \"status\": \"done\"\n# }\n\n# 2. If the revision (build_no), or newer, has not yet converged, but the system is (most likely) still trying to deploy it. There is a point in polling again later when this is the response.\n# {\n#     \"deployed\": False,\n#     \"status\": \"deploying\"\n# }\n# 3. If the revision, or newer, has not yet converged everywhere, and it's never going to, because it was similar to the previous build, or marked obsolete by a user. There is no point in asking again for this revision.\n# {\n#     \"deployed\": False,\n#     \"status\": \"done\"\n# }\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>build_no</code> <code>int</code> <p>The build number to check.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with the aggregated status of all deployment jobs for the given build number. The dictionary contains: - \"deployed\" (bool): Whether the build has successfully converged. - \"status\" (str): The current status of the build (\"done\", \"deploying\").</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there are issues with retrieving the status of the build.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.wait_for_prod_deployment","title":"<code>wait_for_prod_deployment(build_no=None, max_wait=3600, poll_interval=5)</code>","text":"<p>Wait for a production deployment to finish. Useful for example in CI/CD pipelines to wait for a deployment to finish.</p> Example usage <pre><code>vespa_cloud = VespaCloud(...)\nbuild_no = vespa_cloud.deploy_to_prod()\nsuccess = vespa_cloud.wait_for_prod_deployment(build_no, max_wait=3600, poll_interval=5)\nprint(success)\n# Output: True\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>build_no</code> <code>int</code> <p>The build number to check.</p> <code>None</code> <code>max_wait</code> <code>int</code> <p>Maximum time to wait for the deployment in seconds. Default is 3600 (1 hour).</p> <code>3600</code> <code>poll_interval</code> <code>int</code> <p>Polling interval in seconds. Default is 5 seconds.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the deployment is done and converged, False if the deployment has failed.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the deployment did not finish within <code>max_wait</code> seconds.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.deploy_from_disk","title":"<code>deploy_from_disk(instance, application_root, max_wait=300, version=None)</code>","text":"<p>Deploy to the development environment from a directory tree. This method is used when making changes to application package files that are not supported by pyvespa. Note: Requires a certificate and key to be generated using 'vespa auth cert'.</p> Example usage <pre><code>vespa_cloud = VespaCloud(...)\nvespa_cloud.deploy_to_dev_from_directory(\n    instance=\"my-instance\",\n    application_root=\"/path/to/application\",\n    max_wait=3600,\n    version=\"8.435.13\"\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>The name of the instance where the application will be run.</p> required <code>application_root</code> <code>str</code> <p>The root directory of the application package.</p> required <code>max_wait</code> <code>int</code> <p>The maximum number of seconds to wait for the deployment. Default is 3600 (1 hour).</p> <code>300</code> <code>version</code> <code>str</code> <p>The Vespa version to use for the deployment. Default is None, which means the latest version. It must be a valid Vespa version (e.g., \"8.435.13\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Vespa</code> <code>Vespa</code> <p>A Vespa connection instance. This connects to the mtls endpoint. To connect to the token endpoint, use <code>VespaCloud.get_application(endpoint_type=\"token\")</code>.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.delete","title":"<code>delete(instance='default')</code>","text":"<p>Delete the specified instance from the development environment in the Vespa Cloud. To delete a production instance, you must submit a new deployment with <code>deployment-removal</code> added to the 'validation-overrides.xml'. See https://cloud.vespa.ai/en/deleting-applications for more details.</p> Example usage <pre><code>vespa_cloud = VespaCloud(...)\nvespa_cloud.delete_instance(instance=\"my-instance\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>The name of the instance to delete.</p> <code>'default'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.get_all_endpoints","title":"<code>get_all_endpoints(instance='default', region=None, environment='dev')</code>","text":"<p>Get all endpoints for the application instance.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>Application instance name.</p> <code>'default'</code> <code>region</code> <code>str</code> <p>Region name, e.g. 'aws-us-east-1c'.</p> <code>None</code> <code>environment</code> <code>str</code> <p>Environment (dev/prod).</p> <code>'dev'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List[Dict[str, str]]</code> <p>List of endpoints.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.get_endpoint_auth_method","title":"<code>get_endpoint_auth_method(url, instance='default', region=None, environment='dev')</code>","text":"<p>Get the authentication method for the given endpoint URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The endpoint URL.</p> required <code>instance</code> <code>str</code> <p>Application instance name.</p> <code>'default'</code> <code>region</code> <code>str</code> <p>Region name, e.g. 'aws-us-east-1c'.</p> <code>None</code> <code>environment</code> <code>str</code> <p>Environment (dev/prod).</p> <code>'dev'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The authentication method ('mtls' or 'token').</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.get_endpoint","title":"<code>get_endpoint(auth_method, instance='default', region=None, environment='dev', cluster=None)</code>","text":"<p>Get the endpoint URL for the application.</p> <p>Tip: See the 'endpoint'-tab in Vespa Cloud Console for available endpoints.</p> <p>Parameters:</p> Name Type Description Default <code>auth_method</code> <code>str</code> <p>Authentication method. Options are 'mtls' or 'token'.</p> required <code>instance</code> <code>str</code> <p>Application instance name.</p> <code>'default'</code> <code>region</code> <code>str</code> <p>Region name, e.g. 'aws-us-east-1c'.</p> <code>None</code> <code>environment</code> <code>str</code> <p>Environment (dev/prod).</p> <code>'dev'</code> <code>cluster</code> <code>str</code> <p>Specific cluster to get the endpoint for. If None, uses the instance's default cluster.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The endpoint URL.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.get_mtls_endpoint","title":"<code>get_mtls_endpoint(instance='default', region=None, environment='dev', cluster=None)</code>","text":"<p>Get the endpoint URL of a mTLS endpoint for the application. Will return the first mTLS endpoint found if multiple exist. Use <code>VespaCloud.get_all_endpoints</code> to get all endpoints.</p> <p>Tip: See the 'endpoint'-tab in Vespa Cloud Console for available endpoints.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>Application instance name.</p> <code>'default'</code> <code>region</code> <code>str</code> <p>Region name.</p> <code>None</code> <code>environment</code> <code>str</code> <p>Environment (dev/prod).</p> <code>'dev'</code> <code>cluster</code> <code>str</code> <p>Specific cluster to get the endpoint for. If None, uses the instance's default cluster.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The endpoint URL.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.get_token_endpoint","title":"<code>get_token_endpoint(instance='default', region=None, environment='dev', cluster=None)</code>","text":"<p>Get the endpoint URL of a token endpoint for the application. Will return the first token endpoint found if multiple exist. Use <code>VespaCloud.get_all_endpoints</code> to get all endpoints.</p> <p>Tip: See the 'endpoint'-tab in Vespa Cloud Console for available endpoints.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>Application instance name.</p> <code>'default'</code> <code>region</code> <code>str</code> <p>Region name.</p> <code>None</code> <code>environment</code> <code>str</code> <p>Environment (dev/prod).</p> <code>'dev'</code> <code>cluster</code> <code>str</code> <p>Specific cluster to get the endpoint for. If None, uses the instance's default cluster.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The endpoint URL.</p>"},{"location":"api/vespa/evaluation.html","title":"Evaluation","text":""},{"location":"api/vespa/evaluation.html#vespa.evaluation","title":"<code>vespa.evaluation</code>","text":""},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaEvaluator","title":"<code>VespaEvaluator(queries, relevant_docs, vespa_query_fn, app, name='', id_field='', accuracy_at_k=[1, 3, 5, 10], precision_recall_at_k=[1, 3, 5, 10], mrr_at_k=[10], ndcg_at_k=[10], map_at_k=[100], write_csv=False, csv_dir=None)</code>","text":"<p>Evaluate retrieval performance on a Vespa application.</p> <p>This class:</p> <ul> <li>Iterates over queries and issues them against your Vespa application.</li> <li>Retrieves top-k documents per query (with k = max of your IR metrics).</li> <li>Compares the retrieved documents with a set of relevant document ids.</li> <li>Computes IR metrics: Accuracy@k, Precision@k, Recall@k, MRR@k, NDCG@k, MAP@k.</li> <li>Logs vespa search times for each query.</li> <li>Logs/returns these metrics.</li> <li>Optionally writes out to CSV.</li> </ul> Example usage <pre><code>from vespa.application import Vespa\nfrom vespa.evaluation import VespaEvaluator\n\nqueries = {\n    \"q1\": \"What is the best GPU for gaming?\",\n    \"q2\": \"How to bake sourdough bread?\",\n    # ...\n}\nrelevant_docs = {\n    \"q1\": {\"d12\", \"d99\"},\n    \"q2\": {\"d101\"},\n    # ...\n}\n# relevant_docs can also be a dict of query_id =&gt; single relevant doc_id\n# relevant_docs = {\n#     \"q1\": \"d12\",\n#     \"q2\": \"d101\",\n#     # ...\n# }\n# Or, relevant_docs can be a dict of query_id =&gt; map of doc_id =&gt; relevance\n# relevant_docs = {\n#     \"q1\": {\"d12\": 1, \"d99\": 0.1},\n#     \"q2\": {\"d101\": 0.01},\n#     # ...\n# Note that for non-binary relevance, the relevance values should be in [0, 1], and that\n# only the nDCG metric will be computed.\n\ndef my_vespa_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": 'select * from sources * where userInput(\"' + query_text + '\");',\n        \"hits\": top_k,\n        \"ranking\": \"your_ranking_profile\",\n    }\n\napp = Vespa(url=\"http://localhost\", port=8080)\n\nevaluator = VespaEvaluator(\n    queries=queries,\n    relevant_docs=relevant_docs,\n    vespa_query_fn=my_vespa_query_fn,\n    app=app,\n    name=\"test-run\",\n    accuracy_at_k=[1, 3, 5],\n    precision_recall_at_k=[1, 3, 5],\n    mrr_at_k=[10],\n    ndcg_at_k=[10],\n    map_at_k=[100],\n    write_csv=True\n)\n\nresults = evaluator()\nprint(\"Primary metric:\", evaluator.primary_metric)\nprint(\"All results:\", results)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>dict</code> <p>A dictionary of query_id =&gt; query text.</p> required <code>relevant_docs</code> <code>dict</code> <p>A dictionary of query_id =&gt; set of relevant doc_ids or query_id =&gt; dict of doc_id =&gt; relevance. See example usage.</p> required <code>vespa_query_fn</code> <code>callable</code> <p>A callable with the signature <code>my_func(query: str, top_k: int) -&gt; dict</code>. Given a query string and top_k, returns a Vespa query body (dict).</p> required <code>app</code> <code>Vespa</code> <p>A <code>vespa.application.Vespa</code> instance.</p> required <code>name</code> <code>str</code> <p>A name or tag for this evaluation run.</p> <code>''</code> <code>id_field</code> <code>str</code> <p>The field name in Vespa that contains the document ID. If unset, will try to use the Vespa internal document ID, but this may fail in some cases (see https://docs.vespa.ai/en/documents.html#docid-in-results).</p> <code>''</code> <code>accuracy_at_k</code> <code>list of int</code> <p>List of k-values for Accuracy@k.</p> <code>[1, 3, 5, 10]</code> <code>precision_recall_at_k</code> <code>list of int</code> <p>List of k-values for Precision@k and Recall@k.</p> <code>[1, 3, 5, 10]</code> <code>mrr_at_k</code> <code>list of int</code> <p>List of k-values for MRR@k.</p> <code>[10]</code> <code>ndcg_at_k</code> <code>list of int</code> <p>List of k-values for NDCG@k.</p> <code>[10]</code> <code>map_at_k</code> <code>list of int</code> <p>List of k-values for MAP@k.</p> <code>[100]</code> <code>write_csv</code> <code>bool</code> <p>If True, writes results to CSV.</p> <code>False</code> <code>csv_dir</code> <code>str</code> <p>Path to write the CSV file (default is the current working directory).</p> <code>None</code>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaEvaluator.filter_queries","title":"<code>filter_queries(queries, relevant_docs)</code>","text":"<p>Filter out queries that have no relevant docs</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaEvaluator.run","title":"<code>run()</code>","text":"<p>Executes the evaluation by running queries and computing IR metrics.</p> <p>This method: 1. Executes all configured queries against the Vespa application. 2. Collects search results and timing information. 3. Computes the configured IR metrics (Accuracy@k, Precision@k, Recall@k, MRR@k, NDCG@k, MAP@k). 4. Records search timing statistics. 5. Logs results and optionally writes them to CSV.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, float]</code> <p>A dictionary containing: - IR metrics with names like \"accuracy@k\", \"precision@k\", etc. - Search time statistics (\"searchtime_avg\", \"searchtime_q50\", etc.). The values are floats between 0 and 1 for metrics and in seconds for timing.</p> Example <pre><code>{\n    \"accuracy@1\": 0.75,\n    \"ndcg@10\": 0.68,\n    \"searchtime_avg\": 0.0123,\n    ...\n}\n</code></pre>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.mean","title":"<code>mean(values)</code>","text":"<p>Compute the mean of a list of numbers without using numpy.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.percentile","title":"<code>percentile(values, p)</code>","text":"<p>Compute the p-th percentile of a list of values (0 &lt;= p &lt;= 100). This approximates numpy.percentile's behavior.</p>"},{"location":"api/vespa/exceptions.html","title":"Exceptions","text":""},{"location":"api/vespa/exceptions.html#vespa.exceptions","title":"<code>vespa.exceptions</code>","text":""},{"location":"api/vespa/exceptions.html#vespa.exceptions.VespaError","title":"<code>VespaError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Vespa returned an error response</p>"},{"location":"api/vespa/io.html","title":"IO","text":""},{"location":"api/vespa/io.html#vespa.io","title":"<code>vespa.io</code>","text":""},{"location":"api/vespa/io.html#vespa.io.VespaResponse","title":"<code>VespaResponse(json, status_code, url, operation_type)</code>","text":"<p>               Bases: <code>object</code></p> <p>Class to represent a Vespa HTTP API response.</p>"},{"location":"api/vespa/io.html#vespa.io.VespaResponse.get_status_code","title":"<code>get_status_code()</code>","text":"<p>Return status code of the response.</p>"},{"location":"api/vespa/io.html#vespa.io.VespaResponse.is_successfull","title":"<code>is_successfull()</code>","text":"<p>[Deprecated] Use is_successful() instead</p>"},{"location":"api/vespa/io.html#vespa.io.VespaResponse.is_successful","title":"<code>is_successful()</code>","text":"<p>True if status code is 200.</p>"},{"location":"api/vespa/io.html#vespa.io.VespaResponse.get_json","title":"<code>get_json()</code>","text":"<p>Return json of the response.</p>"},{"location":"api/vespa/io.html#vespa.io.VespaQueryResponse","title":"<code>VespaQueryResponse(json, status_code, url, request_body=None)</code>","text":"<p>               Bases: <code>VespaResponse</code></p>"},{"location":"api/vespa/io.html#vespa.io.VespaQueryResponse.get_json","title":"<code>get_json()</code>","text":"<p>For debugging when the response does not have hits.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>JSON object with full response</p>"},{"location":"api/vespa/package.html","title":"Package","text":""},{"location":"api/vespa/package.html#vespa.package","title":"<code>vespa.package</code>","text":""},{"location":"api/vespa/package.html#vespa.package.VT","title":"<code>VT(tag, cs, attrs=None, void_=False, replace_underscores=True, **kwargs)</code>","text":"<p>A 'Vespa Tag' structure, containing <code>tag</code>, <code>children</code>, and <code>attrs</code></p>"},{"location":"api/vespa/package.html#vespa.package.VT.sanitize_tag_name","title":"<code>sanitize_tag_name(tag)</code>  <code>staticmethod</code>","text":"<p>Convert invalid tag names (with '-') to valid Python identifiers (with '_')</p>"},{"location":"api/vespa/package.html#vespa.package.VT.restore_tag_name","title":"<code>restore_tag_name()</code>","text":"<p>Restore sanitized tag names back to the original names for XML generation</p>"},{"location":"api/vespa/package.html#vespa.package.Summary","title":"<code>Summary(name=None, type=None, fields=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Configures a summary field.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the summary field. Can be <code>None</code> if used inside a <code>Field</code>, which then uses the name of the <code>Field</code>.</p> <code>None</code> <code>type</code> <code>str</code> <p>The type of the summary field. Can be <code>None</code> if used inside a <code>Field</code>, which then uses the type of the <code>Field</code>.</p> <code>None</code> <code>fields</code> <code>list</code> <p>A list of properties used to configure the summary. These can be single properties (like \"summary: dynamic\", common in <code>Field</code>), or composite values (like \"source: another_field\").</p> <code>None</code> Example <pre><code>    Summary(None, None, [\"dynamic\"])\n    Summary(None, None, ['dynamic'])\n\n    Summary(\n        \"title\",\n        \"string\",\n        [(\"source\", \"title\")]\n    )\n    Summary('title', 'string', [('source', 'title')])\n\n    Summary(\n        \"title\",\n        \"string\",\n        [(\"source\", [\"title\", \"abstract\"])]\n    )\n    Summary('title', 'string', [('source', ['title', 'abstract'])])\n\n    Summary(\n        name=\"artist\",\n        type=\"string\",\n    )\n    Summary('artist', 'string', None)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Summary.as_lines","title":"<code>as_lines</code>  <code>property</code>","text":"<p>Returns the object as a list of strings, where each string represents a line of configuration that can be used during schema generation as shown below:</p> Example usage <pre><code>    {% for line in field.summary.as_lines %}\n        {{ line }}\n    {% endfor %}\n</code></pre> Example <pre><code>Summary(None, None, [\"dynamic\"]).as_lines\n['summary: dynamic']\n</code></pre> <pre><code>Summary(\n    \"artist\",\n    \"string\",\n).as_lines\n['summary artist type string {}']\n</code></pre> <pre><code>Summary(\n    \"artist\",\n    \"string\",\n    [(\"bolding\", \"on\"), (\"sources\", \"artist\")],\n).as_lines\n['summary artist type string {', '    bolding: on', '    sources: artist', '}']\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.HNSW","title":"<code>HNSW(distance_metric='euclidean', max_links_per_node=16, neighbors_to_explore_at_insert=200)</code>","text":"<p>               Bases: <code>object</code></p> <p>Configures Vespa HNSW indexes.</p> <p>For more information, check the Vespa documentation.</p> <p>Parameters:</p> Name Type Description Default <code>distance_metric</code> <code>str</code> <p>The distance metric to use when computing distance between vectors. Default is 'euclidean'.</p> <code>'euclidean'</code> <code>max_links_per_node</code> <code>int</code> <p>Specifies how many links per HNSW node to select when building the graph. Default is 16.</p> <code>16</code> <code>neighbors_to_explore_at_insert</code> <code>int</code> <p>Specifies how many neighbors to explore when inserting a document in the HNSW graph. Default is 200.</p> <code>200</code>"},{"location":"api/vespa/package.html#vespa.package.StructField","title":"<code>StructField(name, **kwargs)</code>","text":"<p>Create a Vespa struct-field.</p> <p>For more detailed information about struct-fields, check the Vespa documentation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the struct-field.</p> required <code>indexing</code> <code>list</code> <p>Configures how to process data of a struct-field during indexing.</p> required <code>attribute</code> <code>list</code> <p>Specifies a property of an index structure attribute.</p> required <code>match</code> <code>list</code> <p>Set properties that decide how the matching method for this field operates.</p> required <code>query_command</code> <code>list</code> <p>Add configuration for the query-command of the field.</p> required <code>summary</code> <code>Summary</code> <p>Add configuration for the summary of the field.</p> required <code>rank</code> <code>str</code> <p>Specifies the property that defines ranking calculations done for a field.</p> required Example <pre><code>StructField(\n    name = \"first_name\",\n)\nStructField('first_name', None, None, None, None, None, None)\n</code></pre> <pre><code>StructField(\n    name = \"first_name\",\n    indexing = [\"attribute\"],\n    attribute = [\"fast-search\"],\n)\nStructField('first_name', ['attribute'], ['fast-search'], None, None, None, None)\n</code></pre> <pre><code>StructField(\n    name = \"last_name\",\n    match = [\"exact\", (\"exact-terminator\", '\"@%\"')],\n    query_command = ['\"exact %%\"'],\n    summary = Summary(None, None, fields=[\"dynamic\", (\"bolding\", \"on\")])\n)\nStructField('last_name', None, None, ['exact', ('exact-terminator', '\"@%\"')], ['\"exact %%\"'], Summary(None, None, ['dynamic', ('bolding', 'on')]), None)\n</code></pre> <pre><code>StructField(\n    name = \"first_name\",\n    indexing = [\"attribute\"],\n    attribute = [\"fast-search\"],\n    rank = \"filter\",\n)\nStructField('first_name', ['attribute'], ['fast-search'], None, None, None, 'filter')\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Field","title":"<code>Field(name, type, indexing=None, index=None, attribute=None, ann=None, match=None, weight=None, bolding=None, summary=None, is_document_field=True, **kwargs)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa field.</p> <p>For more detailed information about fields, check the Vespa documentation.</p> <p>Once we have an <code>ApplicationPackage</code> instance containing a <code>Schema</code> and a <code>Document</code>,  we usually want to add fields so that we can store our data in a structured manner. We can accomplish that by creating <code>Field</code> instances and adding those to the <code>ApplicationPackage</code> instance via <code>Schema</code> and <code>Document</code> methods.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the field.</p> required <code>type</code> <code>str</code> <p>The data type of the field.</p> required <code>indexing</code> <code>list</code> <p>Configures how to process data of a field during indexing.</p> <code>None</code> <code>index</code> <code>str</code> <p>Sets index parameters. Fields with index are normalized and tokenized by default.</p> <code>None</code> <code>attribute</code> <code>list</code> <p>Specifies a property of an index structure attribute.</p> <code>None</code> <code>ann</code> <code>HNSW</code> <p>Add configuration for approximate nearest neighbor.</p> <code>None</code> <code>match</code> <code>list</code> <p>Set properties that decide how the matching method for this field operates.</p> <code>None</code> <code>weight</code> <code>int</code> <p>Sets the weight of the field, used when calculating rank scores.</p> <code>None</code> <code>bolding</code> <code>bool</code> <p>Whether to highlight matching query terms in the summary.</p> <code>None</code> <code>summary</code> <code>Summary</code> <p>Add configuration for the summary of the field.</p> <code>None</code> <code>is_document_field</code> <code>bool</code> <p>Whether the field is a document field or part of the schema. Default is True.</p> <code>True</code> <code>stemming</code> <code>str</code> <p>Add configuration for stemming of the field.</p> required <code>rank</code> <code>str</code> <p>Add configuration for ranking calculations of the field.</p> required <code>query_command</code> <code>list</code> <p>Add configuration for query-command of the field.</p> required <code>struct_fields</code> <code>list</code> <p>Add struct-fields to the field.</p> required <code>alias</code> <code>list</code> <p>Add alias to the field.</p> required Example <pre><code>Field(name = \"title\", type = \"string\", indexing = [\"index\", \"summary\"], index = \"enable-bm25\")\nField('title', 'string', ['index', 'summary'], 'enable-bm25', None, None, None, None, None, None, True, None, None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    indexing = [\"attribute\"],\n    attribute=[\"fast-search\", \"fast-access\"]\n)\nField('abstract', 'string', ['attribute'], None, ['fast-search', 'fast-access'], None, None, None, None, None, True, None, None, None, [], None)\n</code></pre> <pre><code>Field(name=\"tensor_field\",\n    type=\"tensor&lt;float&gt;(x[128])\",\n    indexing=[\"attribute\"],\n    ann=HNSW(\n        distance_metric=\"euclidean\",\n        max_links_per_node=16,\n        neighbors_to_explore_at_insert=200,\n    ),\n)\nField('tensor_field', 'tensor&lt;float&gt;(x[128])', ['attribute'], None, None, HNSW('euclidean', 16, 200), None, None, None, None, True, None, None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    match = [\"exact\", (\"exact-terminator\", '\"@%\"',)],\n)\nField('abstract', 'string', None, None, None, None, ['exact', ('exact-terminator', '\"@%\"')], None, None, None, True, None, None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    weight = 200,\n)\nField('abstract', 'string', None, None, None, None, None, 200, None, None, True, None, None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    bolding = True,\n)\nField('abstract', 'string', None, None, None, None, None, None, True, None, True, None, None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    summary = Summary(None, None, [\"dynamic\", [\"bolding\", \"on\"]]),\n)\nField('abstract', 'string', None, None, None, None, None, None, None, Summary(None, None, ['dynamic', ['bolding', 'on']]), True, None, None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    stemming = \"shortest\",\n)\nField('abstract', 'string', None, None, None, None, None, None, None, None, True, 'shortest', None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    rank = \"filter\",\n)\nField('abstract', 'string', None, None, None, None, None, None, None, None, True, None, 'filter', None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    query_command = ['\"exact %%\"'],\n)\nField('abstract', 'string', None, None, None, None, None, None, None, None, True, None, None, ['\"exact %%\"'], [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    struct_fields = [\n        StructField(\n            name = \"first_name\",\n            indexing = [\"attribute\"],\n            attribute = [\"fast-search\"],\n        ),\n    ],\n)\nField('abstract', 'string', None, None, None, None, None, None, None, None, True, None, None, None, [StructField('first_name', ['attribute'], ['fast-search'], None, None, None, None)], None)\n</code></pre> <pre><code>Field(\n    name = \"artist\",\n    type = \"string\",\n    alias = [\"artist_name\"],\n)\nField('artist', 'string', None, None, None, None, None, None, None, None, True, None, None, None, [], ['artist_name'])\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Field.add_struct_fields","title":"<code>add_struct_fields(*struct_fields)</code>","text":"<p>Add <code>StructField</code>'s to the <code>Field</code>.</p> <p>Parameters:</p> Name Type Description Default <code>struct_fields</code> <code>list</code> <p>A list of <code>StructField</code> objects to be added.</p> <code>()</code>"},{"location":"api/vespa/package.html#vespa.package.ImportedField","title":"<code>ImportedField(name, reference_field, field_to_import)</code>","text":"<p>               Bases: <code>object</code></p> <p>Imported field from a reference document.</p> <p>Useful to implement parent/child relationships.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Field name.</p> required <code>reference_field</code> <code>str</code> <p>A field of type reference that points to the document that contains the field to be imported.</p> required <code>field_to_import</code> <code>str</code> <p>Field name to be imported, as defined in the reference document.</p> required Example <pre><code>ImportedField(\n    name=\"global_category_ctrs\",\n    reference_field=\"category_ctr_ref\",\n    field_to_import=\"ctrs\",\n)\nImportedField('global_category_ctrs', 'category_ctr_ref', 'ctrs')\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Struct","title":"<code>Struct(name, fields=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa struct. A struct defines a composite type. Check the Vespa documentation for more detailed information about structs.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the struct.</p> required <code>fields</code> <code>list</code> <p>List of <code>Field</code> objects to be included in the fieldset.</p> <code>None</code> Example <pre><code>Struct(\"person\")\nStruct('person', None)\n\nStruct(\n    \"person\",\n    [\n        Field(\"first_name\", \"string\"),\n        Field(\"last_name\", \"string\"),\n    ],\n)\nStruct('person', [Field('first_name', 'string', None, None, None, None, None, None, None, None, True, None, None, None, [], None), Field('last_name', 'string', None, None, None, None, None, None, None, None, True, None, None, None, [], None)])\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.DocumentSummary","title":"<code>DocumentSummary(name, inherits=None, summary_fields=None, from_disk=None, omit_summary_features=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Document Summary. Check the Vespa documentation for more detailed information about document-summary.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the document-summary.</p> required <code>inherits</code> <code>str</code> <p>Name of another document-summary from which this inherits.</p> <code>None</code> <code>summary_fields</code> <code>list</code> <p>List of <code>Summary</code> objects used in this document-summary.</p> <code>None</code> <code>from_disk</code> <code>bool</code> <p>Marks this document-summary as accessing fields on disk.</p> <code>None</code> <code>omit_summary_features</code> <code>bool</code> <p>Specifies that summary-features should be omitted from this document summary.</p> <code>None</code> Example <pre><code>DocumentSummary(\n    name=\"document-summary\",\n)\nDocumentSummary('document-summary', None, None, None, None)\n\nDocumentSummary(\n    name=\"which-inherits\",\n    inherits=\"base-document-summary\",\n)\nDocumentSummary('which-inherits', 'base-document-summary', None, None, None)\n\nDocumentSummary(\n    name=\"with-field\",\n    summary_fields=[Summary(\"title\", \"string\", [(\"source\", \"title\")])]\n)\nDocumentSummary('with-field', None, [Summary('title', 'string', [('source', 'title')])], None, None)\n\nDocumentSummary(\n    name=\"with-bools\",\n    from_disk=True,\n    omit_summary_features=True,\n)\nDocumentSummary('with-bools', None, None, True, True)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Document","title":"<code>Document(fields=None, inherits=None, structs=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa Document.</p> <p>Check the Vespa documentation for more detailed information about documents.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list</code> <p>A list of <code>Field</code> objects to include in the document's schema.</p> <code>None</code> Example <pre><code>Document()\nDocument(None, None, None)\n\nDocument(fields=[Field(name=\"title\", type=\"string\")])\nDocument([Field('title', 'string', None, None, None, None, None, None, None, None, True, None, None, None, [], None)], None, None)\n\nDocument(fields=[Field(name=\"title\", type=\"string\")], inherits=\"context\")\nDocument([Field('title', 'string', None, None, None, None, None, None, None, None, True, None, None, None, [], None)], context, None)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Document.add_fields","title":"<code>add_fields(*fields)</code>","text":"<p>Add <code>Field</code> objects to the document.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list</code> <p>Fields to be added.</p> <code>()</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/package.html#vespa.package.Document.add_structs","title":"<code>add_structs(*structs)</code>","text":"<p>Add <code>Struct</code> objects to the document.</p> <p>Parameters:</p> Name Type Description Default <code>structs</code> <code>list</code> <p>Structs to be added.</p> <code>()</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/package.html#vespa.package.FieldSet","title":"<code>FieldSet(name, fields)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa field set.</p> <p>A fieldset groups fields together for searching. Check the Vespa documentation for more detailed information about field sets.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the fieldset.</p> required <code>fields</code> <code>list</code> <p>Field names to be included in the fieldset.</p> required <p>Returns:</p> Name Type Description <code>FieldSet</code> <code>None</code> <p>A field set instance.</p> Example <pre><code>FieldSet(name=\"default\", fields=[\"title\", \"body\"])\nFieldSet('default', ['title', 'body'])\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Function","title":"<code>Function(name, expression, args=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa rank function.</p> <p>Define a named function that can be referenced as a part of the ranking expression, or (if having no arguments) as a feature. Check the Vespa documentation for more detailed information about rank functions.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the function.</p> required <code>expression</code> <code>str</code> <p>String representing a Vespa expression.</p> required <code>args</code> <code>list</code> <p>List of arguments to be used in the function expression. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Function</code> <code>None</code> <p>A rank function instance.</p> Example <pre><code>    Function(\n        name=\"myfeature\",\n        expression=\"fieldMatch(bar) + freshness(foo)\",\n        args=[\"foo\", \"bar\"]\n    )\n    Function('myfeature', 'fieldMatch(bar) + freshness(foo)', ['foo', 'bar'])\n</code></pre> <p>It is possible to define functions with multi-line expressions: <pre><code>    Function(\n        name=\"token_type_ids\",\n        expression=\"tensor&lt;float&gt;(d0[1],d1[128])(\\n\"\n                   \"    if (d1 &lt; question_length,\\n\"\n                   \"        0,\\n\"\n                   \"    if (d1 &lt; question_length + doc_length,\\n\"\n                   \"        1,\\n\"\n                   \"        TOKEN_NONE\\n\"\n                   \"    )))\",\n    )\n    Function('token_type_ids', 'tensor&lt;float&gt;(d0[1],d1[128])(\\n    if (d1 &lt; question_length,\\n        0,\\n    if (d1 &lt; question_length + doc_length,\\n        1,\\n        TOKEN_NONE\\n    )))', None)\n</code></pre></p>"},{"location":"api/vespa/package.html#vespa.package.FirstPhaseRanking","title":"<code>FirstPhaseRanking(expression, keep_rank_count=None, rank_score_drop_limit=None)</code>","text":"<p>Create a Vespa first phase ranking configuration.</p> <p>This is the initial ranking performed on all matching documents. Check the Vespa documentation for more detailed information about first phase ranking configuration.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>Specify the ranking expression to be used for the first phase of ranking. Check also the Vespa documentation  for ranking expressions.</p> required <code>keep_rank_count</code> <code>int</code> <p>How many documents to keep the first phase top rank values for.  Default value is 10000.</p> <code>None</code> <code>rank_score_drop_limit</code> <code>float</code> <p>Drop all hits with a first phase rank score less than or equal to this floating point number.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FirstPhaseRanking</code> <code>None</code> <p>A first phase ranking configuration instance.</p> Example <pre><code>FirstPhaseRanking(\"myFeature * 10\")\nFirstPhaseRanking('myFeature * 10', None, None)\n\nFirstPhaseRanking(expression=\"myFeature * 10\", keep_rank_count=50, rank_score_drop_limit=10)\nFirstPhaseRanking('myFeature * 10', 50, 10)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.SecondPhaseRanking","title":"<code>SecondPhaseRanking(expression, rerank_count=100, rank_score_drop_limit=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa second phase ranking configuration.</p> <p>This is the optional reranking performed on the best hits from the first phase.  Check the Vespa documentation  for more detailed information about second phase ranking configuration.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>Specify the ranking expression to be used for the second phase of ranking. Check also the Vespa documentation for ranking expressions.</p> required <code>rerank_count</code> <code>int</code> <p>Specifies the number of hits to be reranked in the second phase. Default value is 100.</p> <code>100</code> <code>rank_score_drop_limit</code> <code>float</code> <p>Drop all hits with a first phase rank score less than or equal to this floating point number.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>SecondPhaseRanking</code> <code>None</code> <p>A second phase ranking configuration instance.</p> Example <pre><code>SecondPhaseRanking(expression=\"1.25 * bm25(title) + 3.75 * bm25(body)\", rerank_count=10)\nSecondPhaseRanking('1.25 * bm25(title) + 3.75 * bm25(body)', 10, None)\n\nSecondPhaseRanking(expression=\"1.25 * bm25(title) + 3.75 * bm25(body)\", rerank_count=10, rank_score_drop_limit=5)\nSecondPhaseRanking('1.25 * bm25(title) + 3.75 * bm25(body)', 10, 5)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.GlobalPhaseRanking","title":"<code>GlobalPhaseRanking(expression, rerank_count=100, rank_score_drop_limit=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa global phase ranking configuration.</p> <p>This is the optional reranking performed on the best hits from the content nodes phase(s).  Check the Vespa documentation  for more detailed information about global phase ranking configuration.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>Specify the ranking expression to be used for the global phase of ranking. Check also the Vespa documentation for ranking expressions.</p> required <code>rerank_count</code> <code>int</code> <p>Specifies the number of hits to be reranked in the global phase. Default value is 100.</p> <code>100</code> <code>rank_score_drop_limit</code> <code>float</code> <p>Drop all hits with a first phase rank score less than or equal to this floating point number.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GlobalPhaseRanking</code> <code>None</code> <p>A global phase ranking configuration instance.</p> Example <pre><code>    GlobalPhaseRanking(expression=\"1.25 * bm25(title) + 3.75 * bm25(body)\", rerank_count=10)\n    GlobalPhaseRanking('1.25 * bm25(title) + 3.75 * bm25(body)', 10, None)\n\n    GlobalPhaseRanking(expression=\"1.25 * bm25(title) + 3.75 * bm25(body)\", rerank_count=10, rank_score_drop_limit=5)\n    GlobalPhaseRanking('1.25 * bm25(title) + 3.75 * bm25(body)', 10, 5)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Mutate","title":"<code>Mutate(on_match, on_first_phase, on_second_phase, on_summary)</code>","text":"<p>               Bases: <code>object</code></p> <p>Enable mutating operations in rank profiles.</p> <p>Check the Vespa documentation for more detailed information about mutable attributes.</p> <p>Parameters:</p> Name Type Description Default <code>on_match</code> <code>dict</code> <p>Dictionary for the on-match phase containing 3 mandatory keys: - <code>attribute</code>: name of the mutable attribute to mutate. - <code>operation_string</code>: operation to perform on the mutable attribute. - <code>operation_value</code>: number to set, add, or subtract to/from the current value of the mutable attribute.</p> required <code>on_first_phase</code> <code>dict</code> <p>Dictionary for the on-first-phase phase containing 3 mandatory keys: - <code>attribute</code>: name of the mutable attribute to mutate. - <code>operation_string</code>: operation to perform on the mutable attribute. - <code>operation_value</code>: number to set, add, or subtract to/from the current value of the mutable attribute.</p> required <code>on_second_phase</code> <code>dict</code> <p>Dictionary for the on-second-phase phase containing 3 mandatory keys: - <code>attribute</code>: name of the mutable attribute to mutate. - <code>operation_string</code>: operation to perform on the mutable attribute. - <code>operation_value</code>: number to set, add, or subtract to/from the current value of the mutable attribute.</p> required <code>on_summary</code> <code>dict</code> <p>Dictionary for the on-summary phase containing 3 mandatory keys: - <code>attribute</code>: name of the mutable attribute to mutate. - <code>operation_string</code>: operation to perform on the mutable attribute. - <code>operation_value</code>: number to set, add, or subtract to/from the current value of the mutable attribute.</p> required Example <pre><code>enable_mutating_operations(\n    on_match={\n        'attribute': 'popularity',\n        'operation_string': 'add',\n        'operation_value': 5\n    },\n    on_first_phase={\n        'attribute': 'score',\n        'operation_string': 'subtract',\n        'operation_value': 3\n    }\n)\nenable_mutating_operations({'attribute': 'popularity', 'operation_string': 'add', 'operation_value': 5},\n                            {'attribute': 'score', 'operation_string': 'subtract', 'operation_value': 3})\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.MatchPhaseRanking","title":"<code>MatchPhaseRanking(attribute, order, max_hits)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa match phase ranking configuration.</p> <p>This is an optional phase that can be used to quickly select a subset of hits for further ranking. Check the Vespa documentation for more detailed information about match phase ranking configuration.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>The numeric attribute to use for filtering.</p> required <code>order</code> <code>str</code> <p>The sort order, either \"ascending\" or \"descending\".</p> required <code>max_hits</code> <code>int</code> <p>Maximum number of hits to pass to the next phase.</p> required Example <pre><code>MatchPhaseRanking(attribute=\"popularity\", order=\"descending\", max_hits=1000)\nMatchPhaseRanking('popularity', 'descending', 1000)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.RankProfile","title":"<code>RankProfile(name, first_phase, inherits=None, constants=None, functions=None, summary_features=None, match_features=None, second_phase=None, global_phase=None, match_phase=None, num_threads_per_search=None, **kwargs)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa rank profile.</p> <p>Rank profiles are used to specify an alternative ranking of the same data for different purposes, and to experiment with new rank settings. Check the Vespa documentation for more detailed information about rank profiles.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Rank profile name.</p> required <code>first_phase</code> <code>str</code> <p>The config specifying the first phase of ranking. More info about first phase ranking.</p> required <code>inherits</code> <code>str</code> <p>The inherits attribute is optional. If defined, it contains the name of another rank profile in the same schema. Values not defined in this rank profile will then be inherited.</p> <code>None</code> <code>constants</code> <code>dict</code> <p>Dict of constants available in ranking expressions, resolved and optimized at configuration time. More info about constants.</p> <code>None</code> <code>functions</code> <code>list</code> <p>List of <code>Function</code> objects representing rank functions to be included in the rank profile.</p> <code>None</code> <code>summary_features</code> <code>list</code> <p>List of rank features to be included with each hit. More info about summary features.</p> <code>None</code> <code>match_features</code> <code>list</code> <p>List of rank features to be included with each hit. More info about match features.</p> <code>None</code> <code>second_phase</code> <code>SecondPhaseRanking</code> <p>Config specifying the second phase of ranking. See <code>SecondPhaseRanking</code>.</p> <code>None</code> <code>global_phase</code> <code>GlobalPhaseRanking</code> <p>Config specifying the global phase of ranking. See <code>GlobalPhaseRanking</code>.</p> <code>None</code> <code>match_phase</code> <code>MatchPhaseRanking</code> <p>Config specifying the match phase of ranking. See <code>MatchPhaseRanking</code>.</p> <code>None</code> <code>num_threads_per_search</code> <code>int</code> <p>Overrides the global <code>persearch</code> value for this rank profile to a lower value.</p> <code>None</code> <code>weight</code> <code>list</code> <p>A list of tuples containing the field and their weight.</p> required <code>rank_type</code> <code>list</code> <p>A list of tuples containing a field and the rank-type-name. More info about rank-type.</p> required <code>rank_properties</code> <code>list</code> <p>A list of tuples containing a field and its configuration. More info about rank-properties.</p> required <code>mutate</code> <code>Mutate</code> <p>A <code>Mutate</code> object containing attributes to mutate on, mutation operation, and value. More info about mutate operation.</p> required Example <pre><code>RankProfile(name = \"default\", first_phase = \"nativeRank(title, body)\")\nRankProfile('default', 'nativeRank(title, body)', None, None, None, None, None, None, None, None, None, None, None, None, None)\n\nRankProfile(name = \"new\", first_phase = \"BM25(title)\", inherits = \"default\")\nRankProfile('new', 'BM25(title)', 'default', None, None, None, None, None, None, None, None, None, None, None, None)\n\nRankProfile(\n    name = \"new\",\n    first_phase = \"BM25(title)\",\n    inherits = \"default\",\n    constants={\"TOKEN_NONE\": 0, \"TOKEN_CLS\": 101, \"TOKEN_SEP\": 102},\n    summary_features=[\"BM25(title)\"]\n)\nRankProfile('new', 'BM25(title)', 'default', {'TOKEN_NONE': 0, 'TOKEN_CLS': 101, 'TOKEN_SEP': 102}, None, ['BM25(title)'], None, None, None, None, None, None, None, None, None)\n\nRankProfile(\n    name=\"bert\",\n    first_phase=\"bm25(title) + bm25(body)\",\n    second_phase=SecondPhaseRanking(expression=\"1.25 * bm25(title) + 3.75 * bm25(body)\", rerank_count=10),\n    inherits=\"default\",\n    constants={\"TOKEN_NONE\": 0, \"TOKEN_CLS\": 101, \"TOKEN_SEP\": 102},\n    functions=[\n        Function(\n            name=\"question_length\",\n            expression=\"sum(map(query(query_token_ids), f(a)(a &gt; 0)))\"\n        ),\n        Function(\n            name=\"doc_length\",\n            expression=\"sum(map(attribute(doc_token_ids), f(a)(a &gt; 0)))\"\n        )\n    ],\n    summary_features=[\"question_length\", \"doc_length\"]\n)\nRankProfile('bert', 'bm25(title) + bm25(body)', 'default', {'TOKEN_NONE': 0, 'TOKEN_CLS': 101, 'TOKEN_SEP': 102}, [Function('question_length', 'sum(map(query(query_token_ids), f(a)(a &gt; 0)))', None), Function('doc_length', 'sum(map(attribute(doc_token_ids), f(a)(a &gt; 0)))', None)], ['question_length', 'doc_length'], None, SecondPhaseRanking('1.25 * bm25(title) + 3.75 * bm25(body)', 10, None), None, None, None, None, None, None, None)\n\nRankProfile(\n    name = \"default\",\n    first_phase = \"nativeRank(title, body)\",\n    weight = [(\"title\", 200), (\"body\", 100)]\n)\nRankProfile('default', 'nativeRank(title, body)', None, None, None, None, None, None, None, None, None, [('title', 200), ('body', 100)], None, None, None)\n\nRankProfile(\n    name = \"default\",\n    first_phase = \"nativeRank(title, body)\",\n    rank_type = [(\"body\", \"about\")]\n)\nRankProfile('default', 'nativeRank(title, body)', None, None, None, None, None, None, None, None, None, None, [('body', 'about')], None, None)\n\nRankProfile(\n    name = \"default\",\n    first_phase = \"nativeRank(title, body)\",\n    rank_properties = [(\"fieldMatch(title).maxAlternativeSegmentations\", \"10\")]\n)\nRankProfile('default', 'nativeRank(title, body)', None, None, None, None, None, None, None, None, None, None, None, [('fieldMatch(title).maxAlternativeSegmentations', '10')], None)\n\nRankProfile(\n   name = \"default\",\n   first_phase = FirstPhaseRanking(expression=\"nativeRank(title, body)\", keep_rank_count=50)\n)\nRankProfile('default', FirstPhaseRanking('nativeRank(title, body)', 50, None), None, None, None, None, None, None, None, None, None, None, None, None, None)\n\nRankProfile(\n    name = \"default\",\n    first_phase = \"nativeRank(title, body)\",\n    num_threads_per_search = 2\n)\nRankProfile('default', 'nativeRank(title, body)', None, None, None, None, None, None, None, None, 2, None, None, None, None)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.OnnxModel","title":"<code>OnnxModel(model_name, model_file_path, inputs, outputs)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa ONNX model config.</p> <p>Vespa has support for advanced ranking models through its tensor API. If you have your model in the ONNX format, Vespa can import the models and use them directly. Check the Vespa documentation for more detailed information about field sets.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Unique model name to use as an ID when referencing the model.</p> required <code>model_file_path</code> <code>str</code> <p>ONNX model file path.</p> required <code>inputs</code> <code>dict</code> <p>Dict mapping the ONNX input names as specified in the ONNX file to valid Vespa inputs. These can be a document field (<code>attribute(field_name)</code>), a query parameter (<code>query(query_param)</code>), a constant (<code>constant(name)</code>), or a user-defined function (<code>function_name</code>).</p> required <code>outputs</code> <code>dict</code> <p>Dict mapping the ONNX output names as specified in the ONNX file to the name used in Vespa to specify the output. If omitted, the first output in the ONNX file will be used.</p> required Example <pre><code>OnnxModel(\n    model_name=\"bert\",\n    model_file_path=\"bert.onnx\",\n    inputs={\n        \"input_ids\": \"input_ids\",\n        \"token_type_ids\": \"token_type_ids\",\n        \"attention_mask\": \"attention_mask\",\n    },\n    outputs={\"logits\": \"logits\"},\n)\nOnnxModel('bert', 'bert.onnx', {'input_ids': 'input_ids', 'token_type_ids': 'token_type_ids', 'attention_mask': 'attention_mask'}, {'logits': 'logits'})\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Schema","title":"<code>Schema(name, document, fieldsets=None, rank_profiles=None, models=None, global_document=False, imported_fields=None, document_summaries=None, mode='index', inherits=None, **kwargs)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa Schema.</p> <p>Check the Vespa documentation for more detailed information about schemas.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Schema name.</p> required <code>document</code> <code>Document</code> <p>Vespa <code>Document</code> associated with the Schema.</p> required <code>fieldsets</code> <code>list</code> <p>A list of <code>FieldSet</code> associated with the Schema.</p> <code>None</code> <code>rank_profiles</code> <code>list</code> <p>A list of <code>RankProfile</code> associated with the Schema.</p> <code>None</code> <code>models</code> <code>list</code> <p>A list of <code>OnnxModel</code> associated with the Schema.</p> <code>None</code> <code>global_document</code> <code>bool</code> <p>Set to True to copy the documents to all content nodes. Defaults to False.</p> <code>False</code> <code>imported_fields</code> <code>list</code> <p>A list of <code>ImportedField</code> defining fields from global documents to be imported.</p> <code>None</code> <code>document_summaries</code> <code>list</code> <p>A list of <code>DocumentSummary</code> associated with the schema.</p> <code>None</code> <code>mode</code> <code>str</code> <p>Schema mode. Defaults to 'index'. Other options are 'store-only' and 'streaming'.</p> <code>'index'</code> <code>inherits</code> <code>str</code> <p>Schema to inherit from.</p> <code>None</code> <code>stemming</code> <code>str</code> <p>The default stemming setting. Defaults to 'best'.</p> required Example <pre><code>Schema(name=\"schema_name\", document=Document())\nSchema('schema_name', Document(None, None, None), None, None, [], False, None, [], None)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Schema.add_fields","title":"<code>add_fields(*fields)</code>","text":"<p>Add <code>Field</code> to the Schema's <code>Document</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list</code> <p>A list of <code>Field</code> objects to be added to the <code>Document</code>.</p> <code>()</code> Example <pre><code>schema.add_fields([Field(name=\"title\", type=\"string\"), Field(name=\"body\", type=\"text\")])\nschema.add_fields([Field('title', 'string'), Field('body', 'text')])\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Schema.add_field_set","title":"<code>add_field_set(field_set)</code>","text":"<p>Add a <code>FieldSet</code> to the Schema.</p> <p>Parameters:</p> Name Type Description Default <code>field_set</code> <code>list</code> <p>A list of <code>FieldSet</code> objects to be added to the Schema.</p> required"},{"location":"api/vespa/package.html#vespa.package.Schema.add_rank_profile","title":"<code>add_rank_profile(rank_profile)</code>","text":"<p>Add a <code>RankProfile</code> to the Schema.</p> <p>Parameters:</p> Name Type Description Default <code>rank_profile</code> <code>RankProfile</code> <p>The rank profile to be added to the Schema.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/package.html#vespa.package.Schema.add_model","title":"<code>add_model(model)</code>","text":"<p>Add an <code>OnnxModel</code> to the Schema.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>OnnxModel</code> <p>The ONNX model to be added to the Schema.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/package.html#vespa.package.Schema.add_imported_field","title":"<code>add_imported_field(imported_field)</code>","text":"<p>Add an <code>ImportedField</code> to the Schema.</p> <p>Parameters:</p> Name Type Description Default <code>imported_field</code> <code>ImportedField</code> <p>The imported field to be added to the Schema.</p> required"},{"location":"api/vespa/package.html#vespa.package.Schema.add_document_summary","title":"<code>add_document_summary(document_summary)</code>","text":"<p>Add a <code>DocumentSummary</code> to the Schema.</p> <p>Parameters:</p> Name Type Description Default <code>document_summary</code> <code>DocumentSummary</code> <p>The document summary to be added to the Schema.</p> required"},{"location":"api/vespa/package.html#vespa.package.QueryTypeField","title":"<code>QueryTypeField(name, type)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a field to be included in a <code>QueryProfileType</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Field name.</p> required <code>type</code> <code>str</code> <p>Field type.</p> required Example <pre><code>QueryTypeField(\n    name=\"ranking.features.query(title_bert)\",\n    type=\"tensor&lt;float&gt;(x[768])\"\n)\nQueryTypeField('ranking.features.query(title_bert)', 'tensor&lt;float&gt;(x[768])')\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.QueryProfileType","title":"<code>QueryProfileType(fields=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa Query Profile Type.</p> <p>Check the Vespa documentation for more detailed information about query profile types.</p> <p>An <code>ApplicationPackage</code> instance comes with a default <code>QueryProfile</code> named <code>default</code> that is associated with a <code>QueryProfileType</code> named <code>root</code>, meaning that you usually do not need to create those yourself, only add fields to them when required.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list[QueryTypeField]</code> <p>A list of <code>QueryTypeField</code>.</p> <code>None</code> Example <pre><code>QueryProfileType(\n    fields=[\n        QueryTypeField(\n            name=\"ranking.features.query(tensor_bert)\",\n            type=\"tensor&lt;float&gt;(x[768])\"\n        )\n    ]\n)\n# Output: QueryProfileType([QueryTypeField('ranking.features.query(tensor_bert)', 'tensor&lt;float&gt;(x[768])')])\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.QueryProfileType.add_fields","title":"<code>add_fields(*fields)</code>","text":"<p>Add <code>QueryTypeField</code> objects to the Query Profile Type.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>QueryTypeField</code> <p>Fields to be added.</p> <code>()</code> Example <pre><code>query_profile_type = QueryProfileType()\nquery_profile_type.add_fields(\n    QueryTypeField(\n        name=\"age\",\n        type=\"integer\"\n    ),\n    QueryTypeField(\n        name=\"profession\",\n        type=\"string\"\n    )\n)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.QueryField","title":"<code>QueryField(name, value)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a field to be included in a <code>QueryProfile</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Field name.</p> required <code>value</code> <code>Any</code> <p>Field value.</p> required Example <pre><code>QueryField(name=\"maxHits\", value=1000)\n# Output: QueryField('maxHits', 1000)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.QueryProfile","title":"<code>QueryProfile(fields=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa Query Profile.</p> <p>Check the Vespa documentation for more detailed information about query profiles.</p> <p>A <code>QueryProfile</code> is a named collection of query request parameters given in the configuration. The query request can specify a query profile whose parameters will be used as parameters of that request. The query profiles may optionally be type-checked. Type checking is turned on by referencing a <code>QueryProfileType</code> from the query profile.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list[QueryField]</code> <p>A list of <code>QueryField</code>.</p> <code>None</code> Example <pre><code>QueryProfile(fields=[QueryField(name=\"maxHits\", value=1000)])\n# Output: QueryProfile([QueryField('maxHits', 1000)])\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.QueryProfile.add_fields","title":"<code>add_fields(*fields)</code>","text":"<p>Add <code>QueryField</code> objects to the Query Profile.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>QueryField</code> <p>Fields to be added.</p> <code>()</code> Example <pre><code>query_profile = QueryProfile()\nquery_profile.add_fields(QueryField(name=\"maxHits\", value=1000))\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.ApplicationConfiguration","title":"<code>ApplicationConfiguration(name, value)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa Schema.</p> <p>Check the Config documentation for more detailed information about generic configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Configuration name.</p> required <code>value</code> <code>str | dict</code> <p>Either a string or a dictionary (which may be nested) of values.</p> required Example <pre><code>ApplicationConfiguration(\n    name=\"container.handler.observability.application-userdata\",\n    value={\"version\": \"my-version\"}\n)\n# Output: ApplicationConfiguration(name=\"container.handler.observability.application-userdata\")\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Parameter","title":"<code>Parameter(name, args=None, children=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa Component configuration parameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name.</p> required <code>args</code> <code>Any</code> <p>Parameter arguments.</p> <code>None</code> <code>children</code> <code>str | list[Parameter]</code> <p>Parameter children. Can be either a string or a list of <code>Parameter</code> objects for nested configs.</p> <code>None</code>"},{"location":"api/vespa/package.html#vespa.package.AuthClient","title":"<code>AuthClient(id, permissions, parameters=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa AuthClient.</p> <p>Check the Vespa documentation.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The auth client ID.</p> required <code>permissions</code> <code>list[str]</code> <p>List of permissions.</p> required <code>parameters</code> <code>list[Parameter]</code> <p>List of <code>Parameter</code> objects defining the configuration of the auth client.</p> <code>None</code> Example <pre><code>AuthClient(\n    id=\"token\",\n    permissions=[\"read\", \"write\"],\n    parameters=[Parameter(\"token\", {\"id\": \"my-token-id\"})],\n)\n# Output: AuthClient(id=\"token\", permissions=\"['read', 'write']\")\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Component","title":"<code>Component(id, cls=None, bundle=None, type=None, parameters=None)</code>","text":"<p>               Bases: <code>object</code></p>"},{"location":"api/vespa/package.html#vespa.package.Nodes","title":"<code>Nodes(count='1', parameters=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Specify node resources for a content or container cluster as part of a <code>ContainerCluster</code> or <code>ContentCluster</code>.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of nodes in a cluster.</p> <code>'1'</code> <code>parameters</code> <code>list[Parameter]</code> <p>List of <code>Parameter</code> objects defining the configuration of the cluster resources.</p> <code>None</code> Example <pre><code>ContainerCluster(\n    id=\"example_container\",\n    nodes=Nodes(\n        count=\"2\",\n        parameters=[\n            Parameter(\n                \"resources\", \n                {\"vcpu\": \"4.0\", \"memory\": \"16Gb\", \"disk\": \"125Gb\"},\n                children=[Parameter(\"gpu\", {\"count\": \"1\", \"memory\": \"16Gb\"})]\n            ),\n            Parameter(\"node\", {\"hostalias\": \"node1\", \"distribution-key\": \"0\"}),\n        ]\n    )\n)\n# Output: ContainerCluster(id=\"example_container\", version=\"1.0\", nodes=\"Nodes(count='2')\")\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Cluster","title":"<code>Cluster(id, version='1.0', nodes=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Base class for a cluster configuration. Should not be instantiated directly. Use subclasses <code>ContainerCluster</code> or <code>ContentCluster</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Cluster ID.</p> required <code>version</code> <code>str</code> <p>Cluster version.</p> <code>'1.0'</code> <code>nodes</code> <code>Nodes</code> <p><code>Nodes</code> that specifies node resources.</p> <code>None</code>"},{"location":"api/vespa/package.html#vespa.package.Cluster.to_xml","title":"<code>to_xml(root)</code>","text":"<p>Set up XML elements that are used in both container and content clusters.</p>"},{"location":"api/vespa/package.html#vespa.package.ContainerCluster","title":"<code>ContainerCluster(id, version='1.0', nodes=None, components=None, auth_clients=None)</code>","text":"<p>               Bases: <code>Cluster</code></p> <p>Defines the configuration of a container cluster.</p> <p>Parameters:</p> Name Type Description Default <code>components</code> <code>list[Component]</code> <p>List of <code>Component</code> that contains configurations for application components, e.g. embedders.</p> <code>None</code> <code>auth_clients</code> <code>list[AuthClient]</code> <p>List of <code>AuthClient</code> that contains configurations for authentication clients (e.g., mTLS/token).</p> <code>None</code> <code>nodes</code> <code>Nodes</code> <p><code>Nodes</code> that specifies the resources of the cluster.</p> <code>None</code> <p>If <code>ContainerCluster</code> is used, any <code>Component</code>s must be added to the <code>ContainerCluster</code>, rather than to the <code>ApplicationPackage</code>, in order to be included in the generated schema.</p> Example <pre><code>ContainerCluster(\n    id=\"example_container\",\n    components=[\n        Component(\n            id=\"e5\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    \"transformer-model\", \n                    {\"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\"}\n                ),\n                Parameter(\n                    \"tokenizer-model\", \n                    {\"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\"}\n                )\n            ]\n        )\n    ],\n    auth_clients=[AuthClient(id=\"mtls\", permissions=[\"read\", \"write\"])],\n    nodes=Nodes(count=\"2\", parameters=[Parameter(\"resources\", {\"vcpu\": \"4.0\", \"memory\": \"16Gb\", \"disk\": \"125Gb\"})])\n)\n# Output: ContainerCluster(id=\"example_container\", version=\"1.0\", nodes=\"Nodes(count='2')\", components=\"[Component(id='e5', type='hugging-face-embedder')]\", auth_clients=\"[AuthClient(id='mtls', permissions=['read', 'write'])]\")\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.ContentCluster","title":"<code>ContentCluster(id, document_name, version='1.0', nodes=None, min_redundancy='1')</code>","text":"<p>               Bases: <code>Cluster</code></p> <p>Defines the configuration of a content cluster.</p> <p>Parameters:</p> Name Type Description Default <code>document_name</code> <code>str</code> <p>Name of document.</p> required <code>min_redundancy</code> <code>int</code> <p>Minimum redundancy of the content cluster. Must be at least 2 for production deployments.</p> <code>'1'</code> Example <pre><code>ContentCluster(id=\"example_content\", document_name=\"doc\")\n# Output: ContentCluster(id=\"example_content\", version=\"1.0\", document_name=\"doc\")\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.ValidationID","title":"<code>ValidationID</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Collection of IDs that can be used in validation-overrides.xml.</p> <p>Taken from ValidationId.java.</p> <p><code>clusterSizeReduction</code> was not added as it will be removed in Vespa 9.</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.indexingChange","title":"<code>indexingChange = 'indexing-change'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Changing what tokens are expected and stored in field indexes</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.indexModeChange","title":"<code>indexModeChange = 'indexing-mode-change'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Changing the index mode (streaming, indexed, store-only) of documents</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.fieldTypeChange","title":"<code>fieldTypeChange = 'field-type-change'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Field type changes</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.tensorTypeChange","title":"<code>tensorTypeChange = 'tensor-type-change'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Tensor type change</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.resourcesReduction","title":"<code>resourcesReduction = 'resources-reduction'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Large reductions in node resources (&gt; 50% of the current max total resources)</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.contentTypeRemoval","title":"<code>contentTypeRemoval = 'schema-removal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Removal of a schema (causes deletion of all documents)</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.contentClusterRemoval","title":"<code>contentClusterRemoval = 'content-cluster-removal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Removal (or id change) of content clusters</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.deploymentRemoval","title":"<code>deploymentRemoval = 'deployment-removal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Removal of production zones from deployment.xml</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.globalDocumentChange","title":"<code>globalDocumentChange = 'global-document-change'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Changing global attribute for document types in content clusters</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.configModelVersionMismatch","title":"<code>configModelVersionMismatch = 'config-model-version-mismatch'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Internal use</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.skipOldConfigModels","title":"<code>skipOldConfigModels = 'skip-old-config-models'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Internal use</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.accessControl","title":"<code>accessControl = 'access-control'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Internal use, used in zones where there should be no access-control</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.globalEndpointChange","title":"<code>globalEndpointChange = 'global-endpoint-change'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Changing global endpoints</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.zoneEndpointChange","title":"<code>zoneEndpointChange = 'zone-endpoint-change'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Changing zone (possibly private) endpoint settings</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.redundancyIncrease","title":"<code>redundancyIncrease = 'redundancy-increase'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Increasing redundancy - may easily cause feed blocked</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.redundancyOne","title":"<code>redundancyOne = 'redundancy-one'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>redundancy=1 requires a validation override on first deployment</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.pagedSettingRemoval","title":"<code>pagedSettingRemoval = 'paged-setting-removal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>May cause content nodes to run out of memory</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.certificateRemoval","title":"<code>certificateRemoval = 'certificate-removal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Remove data plane certificates</p>"},{"location":"api/vespa/package.html#vespa.package.Validation","title":"<code>Validation(validation_id, until, comment=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Represents a validation to be overridden on application.</p> <p>Check the Vespa documentation for more detailed information about validations.</p> <p>Parameters:</p> Name Type Description Default <code>validation_id</code> <code>str</code> <p>ID of the validation.</p> required <code>until</code> <code>str</code> <p>The last day this change is allowed, as an ISO-8601-format date in UTC, e.g. 2016-01-30.         Dates may at most be 30 days in the future, but should be as close to now as possible for safety,         while allowing time for review and propagation to all deployed zones. <code>allow-tags</code> with dates in the past are ignored.</p> required <code>comment</code> <code>str</code> <p>Optional text explaining the reason for the change to humans.</p> <code>None</code>"},{"location":"api/vespa/package.html#vespa.package.DeploymentConfiguration","title":"<code>DeploymentConfiguration(environment, regions)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a DeploymentConfiguration, which defines how to generate a deployment.xml file (for use in production deployments).</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>str</code> <p>The environment to deploy to. Currently, only 'prod' is supported.</p> required <code>regions</code> <code>list[str]</code> <p>List of regions to deploy to, e.g. [\"us-east-1\", \"us-west-1\"].                 See Vespa documentation for more information.</p> required Example <pre><code>DeploymentConfiguration(environment=\"prod\", regions=[\"us-east-1\", \"us-west-1\"])\n# Output: DeploymentConfiguration(environment='prod', regions=['us-east-1', 'us-west-1'])\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.EmptyDeploymentConfiguration","title":"<code>EmptyDeploymentConfiguration()</code>","text":"<p>               Bases: <code>DeploymentConfiguration</code></p> <p>Create an EmptyDeploymentConfiguration, which creates an empty deployment.xml, used to delete production deployments.</p>"},{"location":"api/vespa/package.html#vespa.package.ServicesConfiguration","title":"<code>ServicesConfiguration(application_name, schemas=None, configurations=[], stateless_model_evaluation=False, components=[], auth_clients=[], clusters=[], services_config=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a ServicesConfiguration, adopting the VespaTag (VT) approach, rather than Jinja templates. Intended to be used in ApplicationPackage, to generate services.xml, based on either: - A passed <code>services_config</code> (VT) object, or - A set of configurations, schemas, components, auth_clients, and clusters (equivalent to the old approach).</p> <p>The latter will be done in code by calling <code>build_services_vt()</code> to generate the VT object.</p> <p>Parameters:</p> Name Type Description Default <code>application_name</code> <code>str</code> <p>Application name.</p> required <code>schemas</code> <code>Optional[List[Schema]]</code> <p>List of <code>Schema</code>s of the application.</p> <code>None</code> <code>configurations</code> <code>Optional[List[ApplicationConfiguration]]</code> <p>List of <code>ApplicationConfiguration</code> that contains configurations for the application.</p> <code>[]</code> <code>stateless_model_evaluation</code> <code>Optional[bool]</code> <p>Enable stateless model evaluation. Default is False.</p> <code>False</code> <code>components</code> <code>Optional[List[Component]]</code> <p>List of <code>Component</code> that contains configurations for application components.</p> <code>[]</code> <code>auth_clients</code> <code>Optional[List[AuthClient]]</code> <p>List of <code>AuthClient</code> that contains configurations for authentication clients.</p> <code>[]</code> <code>clusters</code> <code>Optional[List[Cluster]]</code> <p>List of <code>Cluster</code> that contains configurations for content or container clusters.</p> <code>[]</code> <code>services_config</code> <code>Optional[VT]</code> <p><code>VT</code> object that contains the services configuration.</p> <code>None</code> Example <pre><code>config = ServicesConfiguration(\n    application_name=\"myapp\",\n    schemas=[Schema(name=\"myschema\", document=Document())],\n    configurations=[ApplicationConfiguration(name=\"container.handler.observability.application-userdata\", value={\"version\": \"my-version\"})],\n    components=[Component(id=\"hf-embedder\", type=\"huggingface-embedder\")],\n    stateless_model_evaluation=True,\n)\nprint(str(config))\n# Output: &lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\n# &lt;services version=\"1.0\"&gt;...&lt;/services&gt;\n\nservices_config = ServicesConfiguration(\n    application_name=\"myapp\",\n    services_config=services(\n        container(id=\"myapp_default\", version=\"1.0\")(\n            component(\n                model(url=\"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/raw/main/tokenizer.json\"),\n                id=\"tokenizer\", type=\"hugging-face-tokenizer\"\n            ),\n            document_api(),\n            search(),\n        ),\n        content(id=\"myapp\", version=\"1.0\")(\n            min_redundancy(\"1\"),\n            documents(document(type=\"doc\", mode=\"index\")),\n            engine(proton(tuning(searchnode(requestthreads(persearch(\"4\"))))))\n        ),\n        version=\"1.0\", minimum_required_vespa_version=\"8.311.28\",\n    ),\n)\nprint(str(services_config))\n# Output: &lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\n# &lt;services version=\"1.0\" minimum-required-vespa-version=\"8.311.28\"&gt;...&lt;/services&gt;\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.ApplicationPackage","title":"<code>ApplicationPackage(name, schema=None, query_profile=None, query_profile_type=None, stateless_model_evaluation=False, create_schema_by_default=True, create_query_profile_by_default=True, configurations=None, validations=None, components=None, auth_clients=None, clusters=None, deployment_config=None, services_config=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create an application package.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Application name. Cannot contain '-' or '_'.</p> required <code>schema</code> <code>list</code> <p>List of Schema objects for the application. If None, a default Schema with the same name as the application will be created. Defaults to None.</p> <code>None</code> <code>query_profile</code> <code>QueryProfile</code> <p>QueryProfile of the application. If None, a default  QueryProfile with QueryProfileType 'root' will be created. Defaults to None.</p> <code>None</code> <code>query_profile_type</code> <code>QueryProfileType</code> <p>QueryProfileType of the application. If None, a default QueryProfileType 'root' will be created. Defaults to None.</p> <code>None</code> <code>stateless_model_evaluation</code> <code>bool</code> <p>Enable stateless model evaluation. Defaults to False.</p> <code>False</code> <code>create_schema_by_default</code> <code>bool</code> <p>Include a default Schema if none is provided in the schema  argument. Defaults to True.</p> <code>True</code> <code>create_query_profile_by_default</code> <code>bool</code> <p>Include a default QueryProfile and QueryProfileType  if not explicitly defined by the user. Defaults to True.</p> <code>True</code> <code>configurations</code> <code>list</code> <p>List of ApplicationConfiguration for the application. Defaults to None.</p> <code>None</code> <code>validations</code> <code>list</code> <p>Optional list of Validation objects to be overridden. Defaults to None.</p> <code>None</code> <code>components</code> <code>list</code> <p>List of Component objects for application components. Defaults to None.</p> <code>None</code> <code>clusters</code> <code>list</code> <p>List of Cluster objects for content or container clusters. If clusters is provided, any Component must be part of a cluster. Defaults to None.</p> <code>None</code> <code>auth_clients</code> <code>list</code> <p>List of AuthClient objects for client authorization. If clusters is passed,  pass the auth clients to the ContainerCluster instead. Defaults to None.</p> <code>None</code> <code>deployment_config</code> <code>DeploymentConfiguration</code> <p>Configuration for production deployments. Defaults to None.</p> <code>None</code> Example <p>To create a default application package:</p> <pre><code>ApplicationPackage(name=\"testapp\")\nApplicationPackage('testapp', [Schema('testapp', Document(None, None, None), None, None, [], False, None, [], None)],\n                QueryProfile(None), QueryProfileType(None))\n</code></pre> <p>This creates a default Schema, QueryProfile, and QueryProfileType, which can be populated with your application's specifics.</p>"},{"location":"api/vespa/package.html#vespa.package.ApplicationPackage.services_to_text","title":"<code>services_to_text</code>  <code>property</code>","text":"<p>Intention is to only use services_config, but keeping this until 100% compatibility is achieved through tests.</p>"},{"location":"api/vespa/package.html#vespa.package.ApplicationPackage.add_schema","title":"<code>add_schema(*schemas)</code>","text":"<p>Add Schema's to the application package.</p> <p>Parameters:</p> Name Type Description Default <code>schemas</code> <code>list</code> <p>Schemas to be added.</p> <code>()</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/package.html#vespa.package.ApplicationPackage.to_zip","title":"<code>to_zip()</code>","text":"<p>Return the application package as zipped bytes, to be used in a subsequent deploy.</p> <p>Returns:</p> Name Type Description <code>BytesIO</code> <code>BytesIO</code> <p>A buffer containing the zipped application package.</p>"},{"location":"api/vespa/package.html#vespa.package.ApplicationPackage.to_zipfile","title":"<code>to_zipfile(zfile)</code>","text":"<p>Export the application package as a deployable zipfile. See application packages for deployment options.</p> <p>Parameters:</p> Name Type Description Default <code>zfile</code> <code>str</code> <p>Filename to export to.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/package.html#vespa.package.ApplicationPackage.to_files","title":"<code>to_files(root)</code>","text":"<p>Export the application package as a directory tree.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Directory to export files to.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/package.html#vespa.package.validate_services","title":"<code>validate_services(xml_input)</code>","text":"<p>Validate an XML input against the RelaxNG schema file for services.xml</p> <p>Parameters:</p> Name Type Description Default <code>xml_input</code> <code>Path or str or Element</code> <p>The XML input to validate.</p> required <p>Returns:     True if the XML input is valid according to the RelaxNG schema, False otherwise.</p>"},{"location":"api/vespa/querybuilder/index.html","title":"Index","text":""},{"location":"api/vespa/querybuilder/index.html#vespa.querybuilder","title":"<code>vespa.querybuilder</code>","text":""},{"location":"api/vespa/querybuilder/builder/index.html","title":"Index","text":""},{"location":"api/vespa/querybuilder/builder/index.html#vespa.querybuilder.builder","title":"<code>vespa.querybuilder.builder</code>","text":""},{"location":"api/vespa/querybuilder/builder/builder.html","title":"Querybuilder","text":""},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder","title":"<code>vespa.querybuilder.builder.builder</code>","text":""},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.QueryField","title":"<code>QueryField(name)</code>","text":""},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Condition","title":"<code>Condition(expression)</code>","text":""},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Condition.all","title":"<code>all(*conditions)</code>  <code>classmethod</code>","text":"<p>Combine multiple conditions using logical AND.</p>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Condition.any","title":"<code>any(*conditions)</code>  <code>classmethod</code>","text":"<p>Combine multiple conditions using logical OR.</p>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query","title":"<code>Query(select_fields, prepend_yql=False)</code>","text":""},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.from_","title":"<code>from_(*sources)</code>","text":"<p>Specify the source schema(s) to query.</p> Example <pre><code>import vespa.querybuilder as qb\nfrom vespa.package import Schema, Document\n\nquery = qb.select(\"*\").from_(\"schema1\", \"schema2\")\nstr(query)\n'select * from schema1, schema2'\nquery = qb.select(\"*\").from_(Schema(name=\"schema1\", document=Document()), Schema(name=\"schema2\", document=Document()))\nstr(query)\n'select * from schema1, schema2'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sources</code> <code>Union[str, Schema]</code> <p>The source schema(s) to query.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>The Query object.</p>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.where","title":"<code>where(condition)</code>","text":"<p>Adds a where clause to filter query results.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#where</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <code>Union[Condition, bool]</code> <p>Filter condition that can be: - Condition object for complex queries - Boolean for simple true/false - QueryField for field-based filters</p> required <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>Self for method chaining</p> Example <p><pre><code>import vespa.querybuilder as qb\n\n# Using field conditions\nf1 = qb.QueryField(\"f1\")\nquery = qb.select(\"*\").from_(\"sd1\").where(f1.contains(\"v1\"))\nstr(query)\n'select * from sd1 where f1 contains \"v1\"'\n</code></pre> <pre><code># Using boolean\nquery = qb.select(\"*\").from_(\"sd1\").where(True)\nstr(query)\n'select * from sd1 where true'\n</code></pre> <pre><code># Using complex conditions\ncondition = f1.contains(\"v1\") &amp; qb.QueryField(\"f2\").contains(\"v2\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where f1 contains \"v1\" and f2 contains \"v2\"'\n</code></pre></p>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.order_by","title":"<code>order_by(field, ascending=True, annotations=None)</code>","text":"<p>Orders results by specified fields.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#order-by</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <p>Field names or QueryField objects to order by</p> required <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations like \"locale\", \"strength\", etc. See https://docs.vespa.ai/en/reference/sorting.html#special-sorting-attributes for details.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>Self for method chaining</p> Example <pre><code>import vespa.querybuilder as qb\n\n# Simple ordering\nquery = qb.select(\"*\").from_(\"sd1\").order_by(\"price\")\nstr(query)\n'select * from sd1 order by price asc'\n\n# Multiple fields with annotation\nquery = qb.select(\"*\").from_(\"sd1\").order_by(\n    \"price\", annotations={\"locale\": \"en_US\"}, ascending=False\n).order_by(\"name\", annotations={\"locale\": \"no_NO\"}, ascending=True)\nstr(query)\n'select * from sd1 order by {\"locale\":\"en_US\"}price desc, {\"locale\":\"no_NO\"}name asc'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.orderByAsc","title":"<code>orderByAsc(field, annotations=None)</code>","text":"<p>Convenience method for ordering results by a field in ascending order. See <code>order_by</code> for more information.</p>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.orderByDesc","title":"<code>orderByDesc(field, annotations=None)</code>","text":"<p>Convenience method for ordering results by a field in descending order. See <code>order_by</code> for more information.</p>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.set_limit","title":"<code>set_limit(limit)</code>","text":"<p>Sets maximum number of results to return.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#limit-offset</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Maximum number of hits to return</p> required <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>Self for method chaining</p> Example <pre><code>import vespa.querybuilder as qb\n\nf1 = qb.QueryField(\"f1\")\nquery = qb.select(\"*\").from_(\"sd1\").where(f1.contains(\"v1\")).set_limit(5)\nstr(query)\n'select * from sd1 where f1 contains \"v1\" limit 5'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.set_offset","title":"<code>set_offset(offset)</code>","text":"<p>Sets number of initial results to skip for pagination.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#limit-offset</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>Number of results to skip</p> required <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>Self for method chaining</p> Example <pre><code>import vespa.querybuilder as qb\n\nf1 = qb.QueryField(\"f1\")\nquery = qb.select(\"*\").from_(\"sd1\").where(f1.contains(\"v1\")).set_offset(10)\nstr(query)\n'select * from sd1 where f1 contains \"v1\" offset 10'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.set_timeout","title":"<code>set_timeout(timeout)</code>","text":"<p>Sets query timeout in milliseconds.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#timeout</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>Timeout in milliseconds</p> required <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>Self for method chaining</p> Example <pre><code>import vespa.querybuilder as qb\n\nf1 = qb.QueryField(\"f1\")\nquery = qb.select(\"*\").from_(\"sd1\").where(f1.contains(\"v1\")).set_timeout(500)\nstr(query)\n'select * from sd1 where f1 contains \"v1\" timeout 500'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.add_parameter","title":"<code>add_parameter(key, value)</code>","text":"<p>Adds a query parameter.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#parameter-substitution</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Parameter name</p> required <code>value</code> <code>Any</code> <p>Parameter value</p> required <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>Self for method chaining</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.userInput(\"@myvar\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition).add_parameter(\"myvar\", \"test\")\nstr(query)\n'select * from sd1 where userInput(@myvar)&amp;myvar=test'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.param","title":"<code>param(key, value)</code>","text":"<p>Alias for add_parameter().</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#parameter-substitution</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Parameter name</p> required <code>value</code> <code>Any</code> <p>Parameter value</p> required <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>Self for method chaining</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.userInput(\"@animal\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition).param(\"animal\", \"panda\")\nstr(query)\n'select * from sd1 where userInput(@animal)&amp;animal=panda'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.groupby","title":"<code>groupby(group_expression, continuations=[])</code>","text":"<p>Groups results by specified expression.</p> <p>For more information, see https://docs.vespa.ai/en/grouping.html</p> <p>Also see  for available methods to build group expressions. <p>Parameters:</p> Name Type Description Default <code>group_expression</code> <code>str</code> <p>Grouping expression</p> required <code>continuations</code> <code>List</code> <p>List of continuation tokens (see https://docs.vespa.ai/en/grouping.html#pagination)</p> <code>[]</code> <p>Returns:</p> Type Description <code>Query</code> <p>: Self for method chaining Example <pre><code>import vespa.querybuilder as qb\nfrom vespa.querybuilder import Grouping as G\n\n# Group by customer with sum of price\ngrouping = G.all(\n    G.group(\"customer\"),\n    G.each(G.output(G.sum(\"price\"))),\n)\nstr(grouping)\n'all(group(customer) each(output(sum(price))))'\nquery = qb.select(\"*\").from_(\"sd1\").groupby(grouping)\nstr(query)\n'select * from sd1 | all(group(customer) each(output(sum(price))))'\n\n# Group by year with count\ngrouping = G.all(\n    G.group(\"time.year(a)\"),\n    G.each(G.output(G.count())),\n)\nstr(grouping)\n'all(group(time.year(a)) each(output(count())))'\nquery = qb.select(\"*\").from_(\"purchase\").where(True).groupby(grouping)\nstr(query)\n'select * from purchase where true | all(group(time.year(a)) each(output(count())))'\n# With continuations\nquery = qb.select(\"*\").from_(\"purchase\").where(True).groupby(grouping, continuations=[\"foo\", \"bar\"])\nstr(query)\n\"select * from purchase where true | { 'continuations':['foo', 'bar'] }all(group(time.year(a)) each(output(count())))\"\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q","title":"<code>Q</code>","text":"<p>Wrapper class for QueryBuilder static methods. Methods are exposed as module-level functions. To use:     <pre><code>import vespa.querybuilder as qb\n\nquery = qb.select(\"*\").from_(\"sd1\") # or any of the other Q class methods\n</code></pre></p>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.select","title":"<code>select(fields)</code>  <code>staticmethod</code>","text":"<p>Creates a new query selecting specified fields.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#select</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>Union[str, List[str], List[QueryField]]</code> <p>Field names or QueryField objects to select</p> required <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>New query object</p> Example <pre><code>import vespa.querybuilder as qb\n\nquery = qb.select(\"*\").from_(\"sd1\")\nstr(query)\n'select * from sd1'\n\nquery = qb.select([\"title\", \"url\"])\nstr(query)\n'select title, url from *'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.any","title":"<code>any(*conditions)</code>  <code>staticmethod</code>","text":"<p>\"Combines multiple conditions with OR operator.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#or</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Condition</code> <p>Variable number of Condition objects to combine with OR</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>Combined condition using OR operators</p> Example <pre><code>import vespa.querybuilder as qb\n\nf1, f2 = qb.QueryField(\"f1\"), qb.QueryField(\"f2\")\ncondition = qb.any(f1 &gt; 10, f2 == \"v2\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where f1 &gt; 10 or f2 = \"v2\"'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.all","title":"<code>all(*conditions)</code>  <code>staticmethod</code>","text":"<p>Combines multiple conditions with AND operator.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#and</p> <p>Parameters:</p> Name Type Description Default <code>*conditions</code> <code>Condition</code> <p>Variable number of Condition objects to combine with AND</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>Combined condition using AND operators</p> Example <pre><code>import vespa.querybuilder as qb\n\nf1, f2 = qb.QueryField(\"f1\"), qb.QueryField(\"f2\")\ncondition = qb.all(f1 &gt; 10, f2 == \"v2\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where f1 &gt; 10 and f2 = \"v2\"'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.userQuery","title":"<code>userQuery(value='')</code>  <code>staticmethod</code>","text":"<p>Creates a userQuery operator for text search.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#userquery</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Optional query string. Default is empty string.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A userQuery condition</p> Example <pre><code>import vespa.querybuilder as qb\n\n# Basic userQuery\ncondition = qb.userQuery()\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where userQuery()'\n\n# UserQuery with search terms\ncondition = qb.userQuery(\"search terms\")\nquery = qb.select(\"*\").from_(\"documents\").where(condition)\nstr(query)\n'select * from documents where userQuery(\"search terms\")'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.dotProduct","title":"<code>dotProduct(field, weights, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a dot product calculation condition.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#dotproduct.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Field containing vectors</p> required <code>weights</code> <code>Union[List[float], Dict[str, float], str]</code> <p>Either list of numeric weights or dict mapping elements to weights or a parameter substitution string starting with '@'</p> required <code>annotations</code> <code>Optional[Dict]</code> <p>Optional modifiers like label</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A dot product calculation condition</p> Example <pre><code>import vespa.querybuilder as qb\n\n# Using dict weights with annotation\ncondition = qb.dotProduct(\n    \"weightedset_field\",\n    {\"feature1\": 1, \"feature2\": 2},\n    annotations={\"label\": \"myDotProduct\"}\n)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({label:\"myDotProduct\"}dotProduct(weightedset_field, {\"feature1\": 1, \"feature2\": 2}))'\n\n# Using list weights\ncondition = qb.dotProduct(\"weightedset_field\", [0.4, 0.6])\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where dotProduct(weightedset_field, [0.4, 0.6])'\n\n# Using parameter substitution\ncondition = qb.dotProduct(\"weightedset_field\", \"@myweights\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition).add_parameter(\"myweights\", [0.4, 0.6])\nstr(query)\n'select * from sd1 where dotProduct(weightedset_field, \"@myweights\")&amp;myweights=[0.4, 0.6]'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.weightedSet","title":"<code>weightedSet(field, weights, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a weighted set condition.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#weightedset.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Field containing weighted set data</p> required <code>weights</code> <code>Union[List[float], Dict[str, float], str]</code> <p>Either list of numeric weights or dict mapping elements to weights or a parameter substitution string starting with '@'</p> required <code>annotations</code> <code>Optional[Dict]</code> <p>Optional annotations like targetNumHits</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A weighted set condition</p> Example <pre><code>import vespa.querybuilder as qb\n\n# using map weights\ncondition = qb.weightedSet(\n    \"weightedset_field\",\n    {\"element1\": 1, \"element2\": 2},\n    annotations={\"targetNumHits\": 10}\n)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({targetNumHits:10}weightedSet(weightedset_field, {\"element1\": 1, \"element2\": 2}))'\n\n# using list weights\ncondition = qb.weightedSet(\"weightedset_field\", [0.4, 0.6])\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where weightedSet(weightedset_field, [0.4, 0.6])'\n\n# using parameter substitution\ncondition = qb.weightedSet(\"weightedset_field\", \"@myweights\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition).add_parameter(\"myweights\", [0.4, 0.6])\nstr(query)\n'select * from sd1 where weightedSet(weightedset_field, \"@myweights\")&amp;myweights=[0.4, 0.6]'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.nonEmpty","title":"<code>nonEmpty(condition)</code>  <code>staticmethod</code>","text":"<p>Creates a nonEmpty operator to check if a field has content.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#nonempty.</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <code>Union[Condition, QueryField]</code> <p>Field or condition to check</p> required <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A nonEmpty condition</p> Example <pre><code>import vespa.querybuilder as qb\n\nfield = qb.QueryField(\"title\")\ncondition = qb.nonEmpty(field)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where nonEmpty(title)'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.wand","title":"<code>wand(field, weights, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a Weighted AND (WAND) operator for efficient top-k retrieval.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#wand.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Field name to search</p> required <code>weights</code> <code>Union[List[float], Dict[str, float], str]</code> <p>Either list of numeric weights or dict mapping terms to weights or a parameter substitution string starting with '@'</p> required <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations like targetHits</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A WAND condition</p> Example <pre><code>import vespa.querybuilder as qb\n\n# Using list weights\ncondition = qb.wand(\"description\", weights=[0.4, 0.6])\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where wand(description, [0.4, 0.6])'\n\n# Using dict weights with annotation\nweights = {\"hello\": 0.3, \"world\": 0.7}\ncondition = qb.wand(\n    \"title\",\n    weights,\n    annotations={\"targetHits\": 100}\n)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({targetHits: 100}wand(title, {\"hello\": 0.3, \"world\": 0.7}))'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.weakAnd","title":"<code>weakAnd(*conditions, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a weakAnd operator for less strict AND matching.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#weakand.</p> <p>Parameters:</p> Name Type Description Default <code>*conditions</code> <code>Condition</code> <p>Variable number of conditions to combine</p> <code>()</code> <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations like targetHits</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A weakAnd condition</p> Example <pre><code>import vespa.querybuilder as qb\n\nf1, f2 = qb.QueryField(\"f1\"), qb.QueryField(\"f2\")\ncondition = qb.weakAnd(f1 == \"v1\", f2 == \"v2\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where weakAnd(f1 = \"v1\", f2 = \"v2\")'\n\n# With annotation\ncondition = qb.weakAnd(\n    f1 == \"v1\",\n    f2 == \"v2\",\n    annotations={\"targetHits\": 100}\n)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({\"targetHits\": 100}weakAnd(f1 = \"v1\", f2 = \"v2\"))'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.geoLocation","title":"<code>geoLocation(field, lat, lng, radius, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a geolocation search condition.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#geolocation.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Field containing location data</p> required <code>lat</code> <code>float</code> <p>Latitude coordinate</p> required <code>lon</code> <code>float</code> <p>Longitude coordinate</p> required <code>radius</code> <code>str</code> <p>Search radius (e.g. \"10km\")</p> required <code>annotations</code> <code>Optional[Dict]</code> <p>Optional settings like targetHits</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A geolocation search condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.geoLocation(\n    \"location_field\",\n    37.7749,\n    -122.4194,\n    \"10km\",\n    annotations={\"targetHits\": 100}\n)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({targetHits:100}geoLocation(location_field, 37.7749, -122.4194, \"10km\"))'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.nearestNeighbor","title":"<code>nearestNeighbor(field, query_vector, annotations={'targetHits': 100})</code>  <code>staticmethod</code>","text":"<p>Creates a nearest neighbor search condition.</p> <p>See https://docs.vespa.ai/en/reference/query-language-reference.html#nearestneighbor for more information.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Vector field to search in</p> required <code>query_vector</code> <code>str</code> <p>Query vector to compare against</p> required <code>annotations</code> <code>Dict[str, Any]</code> <p>Optional annotations to modify the behavior. Required annotation: targetHits (default: 10)</p> <code>{'targetHits': 100}</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A nearest neighbor search condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.nearestNeighbor(\n    field=\"dense_rep\",\n    query_vector=\"q_dense\",\n)\nquery = qb.select([\"id, text\"]).from_(\"m\").where(condition)\nstr(query)\n'select id, text from m where ({targetHits:100}nearestNeighbor(dense_rep, q_dense))'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.rank","title":"<code>rank(*queries)</code>  <code>staticmethod</code>","text":"<p>Creates a rank condition for combining multiple queries.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#rank</p> <p>Parameters:</p> Name Type Description Default <code>*queries</code> <p>Variable number of Query objects to combine</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A rank condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.rank(\n    qb.nearestNeighbor(\"field\", \"queryVector\"),\n    qb.QueryField(\"a\").contains(\"A\"),\n    qb.QueryField(\"b\").contains(\"B\"),\n    qb.QueryField(\"c\").contains(\"C\"),\n)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where rank(({targetHits:100}nearestNeighbor(field, queryVector)), a contains \"A\", b contains \"B\", c contains \"C\")'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.phrase","title":"<code>phrase(*terms, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a phrase search operator for exact phrase matching.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#phrase</p> <p>Parameters:</p> Name Type Description Default <code>*terms</code> <code>str</code> <p>Terms that make up the phrase</p> <code>()</code> <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A phrase condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.phrase(\"new\", \"york\", \"city\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where phrase(\"new\", \"york\", \"city\")'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.near","title":"<code>near(*terms, distance=None, annotations=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Creates a near search operator for finding terms within a specified distance.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#near</p> <p>Parameters:</p> Name Type Description Default <code>*terms</code> <code>str</code> <p>Terms to search for</p> <code>()</code> <code>distance</code> <code>Optional[int]</code> <p>Maximum word distance between terms. Will default to 2 if not specified.</p> <code>None</code> <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations</p> <code>None</code> <code>**kwargs</code> <p>Additional annotations</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A near condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.near(\"machine\", \"learning\", distance=5)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({distance:5}near(\"machine\", \"learning\"))'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.onear","title":"<code>onear(*terms, distance=None, annotations=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Creates an ordered near operator for ordered proximity search.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#onear</p> <p>Parameters:</p> Name Type Description Default <code>*terms</code> <code>str</code> <p>Terms to search for in order</p> <code>()</code> <code>distance</code> <code>Optional[int]</code> <p>Maximum word distance between terms. Will default to 2 if not specified.</p> <code>None</code> <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>An onear condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.onear(\"deep\", \"learning\", distance=3)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({distance:3}onear(\"deep\", \"learning\"))'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.sameElement","title":"<code>sameElement(*conditions)</code>  <code>staticmethod</code>","text":"<p>Creates a sameElement operator to match conditions in same array element.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#sameelement</p> <p>Parameters:</p> Name Type Description Default <code>*conditions</code> <code>Condition</code> <p>Conditions that must match in same element</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A sameElement condition</p> Example <pre><code>import vespa.querybuilder as qb\n\npersons = qb.QueryField(\"persons\")\nfirst_name = qb.QueryField(\"first_name\")\nlast_name = qb.QueryField(\"last_name\")\nyear_of_birth = qb.QueryField(\"year_of_birth\")\ncondition = persons.contains(\n    qb.sameElement(\n        first_name.contains(\"Joe\"),\n        last_name.contains(\"Smith\"),\n        year_of_birth &lt; 1940,\n    )\n )\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where persons contains sameElement(first_name contains \"Joe\", last_name contains \"Smith\", year_of_birth &lt; 1940)'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.equiv","title":"<code>equiv(*terms)</code>  <code>staticmethod</code>","text":"<p>Creates an equiv operator for matching equivalent terms.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#equiv</p> <p>Parameters:</p> Name Type Description Default <code>terms</code> <code>List[str]</code> <p>List of equivalent terms</p> <code>()</code> <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations</p> required <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>An equiv condition</p> Example <pre><code>import vespa.querybuilder as qb\n\nfieldName = qb.QueryField(\"fieldName\")\ncondition = fieldName.contains(qb.equiv(\"Snoop Dogg\", \"Calvin Broadus\"))\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where fieldName contains equiv(\"Snoop Dogg\", \"Calvin Broadus\")'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.uri","title":"<code>uri(value, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a uri operator for matching URIs.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#uri</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Field name containing URI</p> required <code>value</code> <code>str</code> <p>URI value to match</p> required <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A uri condition</p> Example <pre><code>import vespa.querybuilder as qb\n\nurl = \"vespa.ai/foo\"\ncondition = qb.uri(url)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where uri(\"vespa.ai/foo\")'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.fuzzy","title":"<code>fuzzy(value, annotations=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Creates a fuzzy operator for approximate string matching.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#fuzzy</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>str</code> <p>Term to fuzzy match</p> required <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations</p> <code>None</code> <code>**kwargs</code> <p>Optional parameters like maxEditDistance, prefixLength, etc.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A fuzzy condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.fuzzy(\"parantesis\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where fuzzy(\"parantesis\")'\n\n# With annotation\ncondition = qb.fuzzy(\"parantesis\", annotations={\"prefixLength\": 1, \"maxEditDistance\": 2})\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({prefixLength:1,maxEditDistance:2}fuzzy(\"parantesis\"))'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.userInput","title":"<code>userInput(value=None, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a userInput operator for query evaluation.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#userinput.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Optional[str]</code> <p>The input variable name, e.g. \"@myvar\"</p> <code>None</code> <code>annotations</code> <code>Optional[Dict]</code> <p>Optional annotations to modify the behavior</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A condition representing the userInput operator</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.userInput(\"@myvar\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where userInput(@myvar)'\n\n# With defaultIndex annotation\ncondition = qb.userInput(\"@myvar\").annotate({\"defaultIndex\": \"text\"})\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where {defaultIndex:\"text\"}userInput(@myvar)'\n\n# With parameter\ncondition = qb.userInput(\"@animal\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition).param(\"animal\", \"panda\")\nstr(query)\n'select * from sd1 where userInput(@animal)&amp;animal=panda'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.predicate","title":"<code>predicate(field, attributes=None, range_attributes=None)</code>  <code>staticmethod</code>","text":"<p>Creates a predicate condition for filtering documents based on specific attributes.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#predicate.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>The predicate field name</p> required <code>attributes</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of attribute key-value pairs</p> <code>None</code> <code>range_attributes</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of range attribute key-value pairs</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A condition representing the predicate operation</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.predicate(\n    \"predicate_field\",\n    attributes={\"gender\": \"Female\"},\n    range_attributes={\"age\": \"20L\"}\n)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where predicate(predicate_field,{\"gender\":\"Female\"},{\"age\":20L})'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.true","title":"<code>true()</code>  <code>staticmethod</code>","text":"<p>Creates a condition that is always true.</p> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A true condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.true()\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where true'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.false","title":"<code>false()</code>  <code>staticmethod</code>","text":"<p>Creates a condition that is always false.</p> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A false condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.false()\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where false'\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/index.html","title":"Index","text":""},{"location":"api/vespa/querybuilder/grouping/index.html#vespa.querybuilder.grouping","title":"<code>vespa.querybuilder.grouping</code>","text":""},{"location":"api/vespa/querybuilder/grouping/grouping.html","title":"Grouping","text":""},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping","title":"<code>vespa.querybuilder.grouping.grouping</code>","text":""},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping","title":"<code>Grouping</code>","text":"<p>A Pythonic DSL for building Vespa grouping expressions programmatically.</p> <p>This class provides a set of static methods that build grouping syntax strings which can be combined to form a valid Vespa \u201cselect=\u2026\u201d grouping expression.</p> <p>For a guide to grouping in vespa, see https://docs.vespa.ai/en/grouping.html. For the reference docs, see https://docs.vespa.ai/en/reference/grouping-syntax.html.</p> Minimal Example <pre><code>from vespa.querybuilder import Grouping as G\n\n# Build a simple grouping expression which groups on \"my_attribute\"\n# and outputs the count of matching documents under each group:\nexpr = G.all(\n    G.group(\"my_attribute\"),\n    G.each(\n        G.output(G.count())\n    )\n)\nprint(expr)\nall(group(my_attribute) each(output(count())))\n</code></pre> <p>In the above example, the \u201call(...)\u201d wraps the grouping operations at the top level. We first group on \u201cmy_attribute\u201d, then under \u201ceach(...)\u201d we add an output aggregator \u201ccount()\u201d. The \u201cprint\u201d output is the exact grouping expression string you would pass to Vespa in the \u201cselect\u201d query parameter.</p> <p>For multi-level (nested) grouping, you can nest additional calls to \u201cgroup(...)\u201d or \u201ceach(...)\u201d inside. For example:     <pre><code># Nested grouping:\n# 1) Group by 'category'\n# 2) Within each category, group by 'sub_category'\n# 3) Output the count() under each sub-category\nnested_expr = G.all(\n    G.group(\"category\"),\n    G.each(\n        G.group(\"sub_category\"),\n        G.each(\n            G.output(G.count())\n        )\n    )\n)\nprint(nested_expr)\nall(group(category) each(group(sub_category) each(output(count()))))\n</code></pre></p> <p>You may use any of the static methods below to build more advanced groupings, aggregations, or arithmetic/string expressions for sorting, filtering, or bucket definitions. Refer to Vespa documentation for the complete details.</p>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.all","title":"<code>all(*args)</code>  <code>staticmethod</code>","text":"<p>Corresponds to the \u201call(...)\u201d grouping block in Vespa, which means \u201cgroup all documents (no top-level grouping) and then do the enclosed operations\u201d.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>Sub-expressions to include within the <code>all(...)</code> block.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.all(G.group(\"my_attribute\"), G.each(G.output(G.count())))\nprint(expr)\nall(group(my_attribute) each(output(count())))\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.each","title":"<code>each(*args)</code>  <code>staticmethod</code>","text":"<p>Corresponds to the \u201ceach(...)\u201d grouping block in Vespa, which means \u201ccreate a group for each unique value and then do the enclosed operations\u201d.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>Sub-expressions to include within the <code>each(...)</code> block.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.each(\"output(count())\", \"output(avg(price))\")\nprint(expr)\neach(output(count()) output(avg(price)))\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.group","title":"<code>group(field)</code>  <code>staticmethod</code>","text":"<p>Defines a grouping step on a field or expression.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>The field or expression on which to group.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.group(\"my_map.key\")\nprint(expr)\ngroup(my_map.key)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.count","title":"<code>count()</code>  <code>staticmethod</code>","text":"<p>\u201ccount()\u201d aggregator.</p> <p>By default, returns a string 'count()'. Negative ordering or usage can be done by prefixing a minus, e.g.: order(-count()) in Vespa syntax.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'count()' or prefixed version if used with a minus operator.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.count()\nprint(expr)\ncount()\n\nsort_expr = f\"-{expr}\"\nprint(sort_expr)\n-count()\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.sum","title":"<code>sum(value)</code>  <code>staticmethod</code>","text":"<p>\u201csum(...)\u201d aggregator. Sums the given expression or field over all documents in the group.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int, float]</code> <p>The field or numeric expression to sum.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'sum(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.sum(\"my_numeric_field\")\nprint(expr)\nsum(my_numeric_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.avg","title":"<code>avg(value)</code>  <code>staticmethod</code>","text":"<p>\u201cavg(...)\u201d aggregator. Computes the average of the given expression or field for all documents in the group.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int, float]</code> <p>The field or numeric expression to average.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'avg(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.avg(\"my_numeric_field\")\nprint(expr)\navg(my_numeric_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.min","title":"<code>min(value)</code>  <code>staticmethod</code>","text":"<p>\u201cmin(...)\u201d aggregator. Keeps the minimum value of the expression or field among all documents in the group.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int, float]</code> <p>The field or numeric expression to find the minimum of.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'min(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.min(\"some_field\")\nprint(expr)\nmin(some_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.max","title":"<code>max(value)</code>  <code>staticmethod</code>","text":"<p>\u201cmax(...)\u201d aggregator. Keeps the maximum value of the expression or field among all documents in the group.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int, float]</code> <p>The field or numeric expression to find the maximum of.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'max(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.max(\"relevance()\")\nprint(expr)\nmax(relevance())\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.stddev","title":"<code>stddev(value)</code>  <code>staticmethod</code>","text":"<p>\u201cstddev(...)\u201d aggregator. Computes the population standard deviation for the expression or field among all documents in the group.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int, float]</code> <p>The field or numeric expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'stddev(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.stddev(\"my_numeric_field\")\nprint(expr)\nstddev(my_numeric_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.xor","title":"<code>xor(value)</code>  <code>staticmethod</code>","text":"<p>\u201cxor(...)\u201d aggregator. XORs all values of the expression or field together over the documents in the group.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int, float]</code> <p>The field or numeric expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'xor(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.xor(\"my_field\")\nprint(expr)\nxor(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.output","title":"<code>output(*args)</code>  <code>staticmethod</code>","text":"<p>Defines output aggregators to be collected for the grouping level.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Union[str, Expression]</code> <p>Multiple aggregator expressions, e.g., 'count()', 'sum(price)'.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Expression</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'output(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.output(G.count(), G.sum(\"price\"))\nprint(expr)\noutput(count(),sum(price))\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.order","title":"<code>order(*args)</code>  <code>staticmethod</code>","text":"<p>Defines an order(...) clause to sort groups by the given expressions or aggregators.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Union[str, Expression]</code> <p>Multiple expressions or aggregators to order by.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Expression</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'order(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.order(G.sum(G.relevance()), -G.count())\nprint(expr)\norder(sum(relevance()),-count())\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.precision","title":"<code>precision(value)</code>  <code>staticmethod</code>","text":"<p>Sets the \u201cprecision(...)\u201d for the grouping step.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>Precision value.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'precision(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.precision(1000)\nprint(expr)\nprecision(1000)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.add","title":"<code>add(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cadd(...)\u201d expression. Adds all arguments together in order.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions to be added.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'add(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.add(\"my_field\", \"5\", \"10\")\nprint(expr)\nadd(my_field, 5, 10)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.sub","title":"<code>sub(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201csub(...)\u201d expression. Subtracts each subsequent argument from the first.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions involved in subtraction.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'sub(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.sub(\"my_field\", \"2\")\nprint(expr)\nsub(my_field, 2)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.mul","title":"<code>mul(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cmul(...)\u201d expression. Multiplies all arguments in order.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions to multiply.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'mul(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.mul(\"my_field\", \"2\", \"3\")\nprint(expr)\nmul(my_field, 2, 3)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.div","title":"<code>div(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cdiv(...)\u201d expression. Divides the first argument by the second, etc.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions to divide in order.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'div(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.div(\"my_field\", \"2\")\nprint(expr)\ndiv(my_field, 2)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.mod","title":"<code>mod(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cmod(...)\u201d expression. Modulo the first argument by the second, result by the third, etc.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions to apply modulo on in order.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'mod(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.mod(\"my_field\", \"100\")\nprint(expr)\nmod(my_field,100)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.and_","title":"<code>and_(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cand(...)\u201d expression. Bitwise AND of the arguments in order.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions to apply bitwise AND.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'and(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.and_(\"fieldA\", \"fieldB\")\nprint(expr)\nand(fieldA, fieldB)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.or_","title":"<code>or_(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cor(...)\u201d expression. Bitwise OR of the arguments in order.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions to apply bitwise OR.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'or(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.or_(\"fieldA\", \"fieldB\")\nprint(expr)\nor(fieldA, fieldB)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.xor_expr","title":"<code>xor_expr(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cxor(...)\u201d bitwise expression.</p> <p>(Note: For aggregator use, see xor(...) aggregator method above.)</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions to apply bitwise XOR.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'xor(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.xor_expr(\"fieldA\", \"fieldB\")\nprint(expr)\nxor(fieldA, fieldB)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.strlen","title":"<code>strlen(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cstrlen(...)\u201d expression. Returns the number of bytes in the string.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The string field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'strlen(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.strlen(\"my_string_field\")\nprint(expr)\nstrlen(my_string_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.strcat","title":"<code>strcat(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cstrcat(...)\u201d expression. Concatenate all string arguments in order.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The string expressions to concatenate.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'strcat(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.strcat(\"fieldA\", \"_\", \"fieldB\")\nprint(expr)\nstrcat(fieldA,_,fieldB)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.todouble","title":"<code>todouble(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctodouble(...)\u201d expression. Convert argument to double.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'todouble(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.todouble(\"my_field\")\nprint(expr)\ntodouble(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.tolong","title":"<code>tolong(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctolong(...)\u201d expression. Convert argument to long.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'tolong(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.tolong(\"my_field\")\nprint(expr)\ntolong(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.tostring","title":"<code>tostring(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctostring(...)\u201d expression. Convert argument to string.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'tostring(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.tostring(\"my_field\")\nprint(expr)\ntostring(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.toraw","title":"<code>toraw(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctoraw(...)\u201d expression. Convert argument to raw data.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'toraw(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.toraw(\"my_field\")\nprint(expr)\ntoraw(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.cat","title":"<code>cat(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201ccat(...)\u201d expression. Concatenate the binary representation of arguments.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The binary expressions or fields to concatenate.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'cat(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.cat(\"fieldA\", \"fieldB\")\nprint(expr)\ncat(fieldA,fieldB)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.md5","title":"<code>md5(expr, width)</code>  <code>staticmethod</code>","text":"<p>\u201cmd5(...)\u201d expression.</p> <p>Does an MD5 over the binary representation of the argument, and keeps the lowest 'width' bits.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field to apply MD5 on.</p> required <code>width</code> <code>int</code> <p>The number of bits to keep.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'md5(expr, width)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.md5(\"my_field\", 16)\nprint(expr)\nmd5(my_field, 16)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.xorbit","title":"<code>xorbit(expr, width)</code>  <code>staticmethod</code>","text":"<p>\u201cxorbit(...)\u201d expression.</p> <p>Performs an XOR of 'width' bits over the binary representation of the argument. Width is rounded up to a multiple of 8.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field to apply xorbit on.</p> required <code>width</code> <code>int</code> <p>The number of bits for the XOR operation.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'xorbit(expr, width)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.xorbit(\"my_field\", 16)\nprint(expr)\nxorbit(my_field, 16)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.relevance","title":"<code>relevance()</code>  <code>staticmethod</code>","text":"<p>\u201crelevance()\u201d expression. Returns the computed rank (relevance) of a document.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'relevance()' as a Vespa expression string.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.relevance()\nprint(expr)\nrelevance()\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.array_at","title":"<code>array_at(array_name, index_expr)</code>  <code>staticmethod</code>","text":"<p>\u201carray.at(...)\u201d accessor expression. Returns a single element from the array at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>array_name</code> <code>str</code> <p>The name of the array.</p> required <code>index_expr</code> <code>Union[str, int]</code> <p>The index or expression that evaluates to an index.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'array.at(array_name, index)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.array_at(\"my_array\", 0)\nprint(expr)\narray.at(my_array, 0)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.zcurve_x","title":"<code>zcurve_x(expr)</code>  <code>staticmethod</code>","text":"<p>\u201czcurve.x(...)\u201d expression. Returns the X component of the given zcurve-encoded 2D point.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The zcurve-encoded field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'zcurve.x(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.zcurve_x(\"location_zcurve\")\nprint(expr)\nzcurve.x(location_zcurve)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.zcurve_y","title":"<code>zcurve_y(expr)</code>  <code>staticmethod</code>","text":"<p>\u201czcurve.y(...)\u201d expression. Returns the Y component of the given zcurve-encoded 2D point.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The zcurve-encoded field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'zcurve.y(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.zcurve_y(\"location_zcurve\")\nprint(expr)\nzcurve.y(location_zcurve)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_dayofmonth","title":"<code>time_dayofmonth(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.dayofmonth(...)\u201d expression. Returns the day of month (1-31).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.dayofmonth(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_dayofmonth(\"timestamp_field\")\nprint(expr)\ntime.dayofmonth(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_dayofweek","title":"<code>time_dayofweek(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.dayofweek(...)\u201d expression. Returns the day of week (0-6), Monday = 0.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.dayofweek(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_dayofweek(\"timestamp_field\")\nprint(expr)\ntime.dayofweek(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_dayofyear","title":"<code>time_dayofyear(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.dayofyear(...)\u201d expression. Returns the day of year (0-365).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.dayofyear(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_dayofyear(\"timestamp_field\")\nprint(expr)\ntime.dayofyear(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_hourofday","title":"<code>time_hourofday(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.hourofday(...)\u201d expression. Returns the hour of day (0-23).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.hourofday(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_hourofday(\"timestamp_field\")\nprint(expr)\ntime.hourofday(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_minuteofhour","title":"<code>time_minuteofhour(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.minuteofhour(...)\u201d expression. Returns the minute of hour (0-59).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.minuteofhour(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_minuteofhour(\"timestamp_field\")\nprint(expr)\ntime.minuteofhour(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_monthofyear","title":"<code>time_monthofyear(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.monthofyear(...)\u201d expression. Returns the month of year (1-12).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.monthofyear(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_monthofyear(\"timestamp_field\")\nprint(expr)\ntime.monthofyear(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_secondofminute","title":"<code>time_secondofminute(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.secondofminute(...)\u201d expression. Returns the second of minute (0-59).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.secondofminute(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_secondofminute(\"timestamp_field\")\nprint(expr)\ntime.secondofminute(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_year","title":"<code>time_year(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.year(...)\u201d expression. Returns the full year (e.g. 2009).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.year(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_year(\"timestamp_field\")\nprint(expr)\ntime.year(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_date","title":"<code>time_date(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.date(...)\u201d expression. Returns the date (e.g. 2009-01-10).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.date(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_date(\"timestamp_field\")\nprint(expr)\ntime.date(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_exp","title":"<code>math_exp(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.exp(...)\u201d expression. Returns e^expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'math.exp(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_exp(\"my_field\")\nprint(expr)\nmath.exp(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_log","title":"<code>math_log(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.log(...)\u201d expression. Returns the natural logarithm of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.log(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_log(\"my_field\")\nprint(expr)\nmath.log(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_log1p","title":"<code>math_log1p(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.log1p(...)\u201d expression. Returns the natural logarithm of (1 + expr).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.log1p(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_log1p(\"my_field\")\nprint(expr)\nmath.log1p(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_log10","title":"<code>math_log10(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.log10(...)\u201d expression. Returns the base-10 logarithm of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.log10(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_log10(\"my_field\")\nprint(expr)\nmath.log10(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_sqrt","title":"<code>math_sqrt(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.sqrt(...)\u201d expression. Returns the square root of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.sqrt(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_sqrt(\"my_field\")\nprint(expr)\nmath.sqrt(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_cbrt","title":"<code>math_cbrt(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.cbrt(...)\u201d expression. Returns the cube root of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.cbrt(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_cbrt(\"my_field\")\nprint(expr)\nmath.cbrt(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_sin","title":"<code>math_sin(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.sin(...)\u201d expression. Returns the sine of expr (argument in radians).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.sin(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_sin(\"my_field\")\nprint(expr)\nmath.sin(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_cos","title":"<code>math_cos(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.cos(...)\u201d expression. Returns the cosine of expr (argument in radians).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.cos(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_cos(\"my_field\")\nprint(expr)\nmath.cos(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_tan","title":"<code>math_tan(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.tan(...)\u201d expression. Returns the tangent of expr (argument in radians).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.tan(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_tan(\"my_field\")\nprint(expr)\nmath.tan(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_asin","title":"<code>math_asin(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.asin(...)\u201d expression. Returns the arcsine of expr (in radians).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.asin(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_asin(\"my_field\")\nprint(expr)\nmath.asin(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_acos","title":"<code>math_acos(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.acos(...)\u201d expression. Returns the arccosine of expr (in radians).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.acos(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_acos(\"my_field\")\nprint(expr)\nmath.acos(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_atan","title":"<code>math_atan(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.atan(...)\u201d expression. Returns the arctangent of expr (in radians).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.atan(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_atan(\"my_field\")\nprint(expr)\nmath.atan(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_sinh","title":"<code>math_sinh(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.sinh(...)\u201d expression. Returns the hyperbolic sine of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.sinh(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_sinh(\"my_field\")\nprint(expr)\nmath.sinh(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_cosh","title":"<code>math_cosh(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.cosh(...)\u201d expression. Returns the hyperbolic cosine of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.cosh(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_cosh(\"my_field\")\nprint(expr)\nmath.cosh(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_tanh","title":"<code>math_tanh(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.tanh(...)\u201d expression. Returns the hyperbolic tangent of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.tanh(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_tanh(\"my_field\")\nprint(expr)\nmath.tanh(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_asinh","title":"<code>math_asinh(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.asinh(...)\u201d expression. Returns the inverse hyperbolic sine of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.asinh(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_asinh(\"my_field\")\nprint(expr)\nmath.asinh(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_acosh","title":"<code>math_acosh(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.acosh(...)\u201d expression. Returns the inverse hyperbolic cosine of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.acosh(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_acosh(\"my_field\")\nprint(expr)\nmath.acosh(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_atanh","title":"<code>math_atanh(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.atanh(...)\u201d expression. Returns the inverse hyperbolic tangent of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.atanh(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_atanh(\"my_field\")\nprint(expr)\nmath.atanh(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_pow","title":"<code>math_pow(expr_x, expr_y)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.pow(...)\u201d expression. Returns expr_x^expr_y.</p> <p>Parameters:</p> Name Type Description Default <code>expr_x</code> <code>str</code> <p>The expression or field for the base.</p> required <code>expr_y</code> <code>str</code> <p>The expression or field for the exponent.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.pow(expr_x, expr_y)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_pow(\"my_field\", \"2\")\nprint(expr)\nmath.pow(my_field,2)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_hypot","title":"<code>math_hypot(expr_x, expr_y)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.hypot(...)\u201d expression. Returns the length of the hypotenuse given expr_x and expr_y.</p> <p>Parameters:</p> Name Type Description Default <code>expr_x</code> <code>str</code> <p>The expression or field for the first side of the triangle.</p> required <code>expr_y</code> <code>str</code> <p>The expression or field for the second side of the triangle.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.hypot(expr_x, expr_y)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_hypot(\"my_field_x\", \"my_field_y\")\nprint(expr)\nmath.hypot(my_field_x, my_field_y)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.size","title":"<code>size(expr)</code>  <code>staticmethod</code>","text":"<p>\u201csize(...)\u201d expression. Returns the number of elements if expr is a list; otherwise returns 1.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The list expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'size(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.size(\"my_array\")\nprint(expr)\nsize(my_array)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.sort","title":"<code>sort(expr)</code>  <code>staticmethod</code>","text":"<p>\u201csort(...)\u201d expression. Sorts the elements of the list argument in ascending order.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The list expression or field to sort.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'sort(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.sort(\"my_array\")\nprint(expr)\nsort(my_array)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.reverse","title":"<code>reverse(expr)</code>  <code>staticmethod</code>","text":"<p>\u201creverse(...)\u201d expression. Reverses the elements of the list argument.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The list expression or field to reverse.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'reverse(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.reverse(\"my_array\")\nprint(expr)\nreverse(my_array)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.fixedwidth","title":"<code>fixedwidth(value, bucket_width)</code>  <code>staticmethod</code>","text":"<p>\u201cfixedwidth(...)\u201d bucket expression. Maps the value of the first argument into consecutive buckets whose width is the second argument.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The field or expression to bucket.</p> required <code>bucket_width</code> <code>Union[int, float]</code> <p>The width of each bucket.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'fixedwidth(value, bucket_width)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.fixedwidth(\"my_field\",10)\nprint(expr)\nfixedwidth(my_field,10)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.predefined","title":"<code>predefined(value, buckets)</code>  <code>staticmethod</code>","text":"<p>\u201cpredefined(...)\u201d bucket expression. Maps the value into the provided list of buckets.</p> <p>Each 'bucket' must be a string representing the range, e.g.: 'bucket(-inf,0)', 'bucket[0,10)', 'bucket[10,inf)', etc.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The field or expression to bucket.</p> required <code>buckets</code> <code>List[str]</code> <p>A list of bucket definitions.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'predefined(value, ( ))'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.predefined(\"my_field\", [\"bucket(-inf,0)\", \"bucket[0,10)\", \"bucket[10,inf)\"])\nprint(expr)\npredefined(my_field,bucket(-inf,0),bucket[0,10),bucket[10,inf))\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.interpolatedlookup","title":"<code>interpolatedlookup(array_attr, lookup_expr)</code>  <code>staticmethod</code>","text":"<p>\u201cinterpolatedlookup(...)\u201d expression. Counts elements in a sorted array that are less than an expression, with linear interpolation if the expression is between element values.</p> <p>Parameters:</p> Name Type Description Default <code>array_attr</code> <code>str</code> <p>The sorted array field name.</p> required <code>lookup_expr</code> <code>str</code> <p>The expression or value to lookup.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'interpolatedlookup(array, expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.interpolatedlookup(\"my_sorted_array\", \"4.2\")\nprint(expr)\ninterpolatedlookup(my_sorted_array, 4.2)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.summary","title":"<code>summary(summary_class='')</code>  <code>staticmethod</code>","text":"<p>\u201csummary(...)\u201d hit aggregator. Produces a summary of the requested summary class.</p> <p>If no summary class is specified, \u201csummary()\u201d is used.</p> <p>Parameters:</p> Name Type Description Default <code>summary_class</code> <code>str</code> <p>Name of the summary class. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'summary(...)'.</p> Example <p><pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.summary()\nprint(expr)\nsummary()\n</code></pre> <pre><code>expr = G.summary(\"my_summary_class\")\nprint(expr)\nsummary(my_summary_class)\n</code></pre></p>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.as_","title":"<code>as_(expression, label)</code>  <code>staticmethod</code>","text":"<p>Appends an ' as(label)' part to a grouping block expression.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>The expression to be labeled.</p> required <code>label</code> <code>str</code> <p>The label to be used.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A Vespa grouping expression string of the form 'expression as(label)'</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.as_(G.each(G.output(G.count())), \"mylabel\")\nprint(expr)\neach(output(count())) as(mylabel)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.alias","title":"<code>alias(alias_name, expression)</code>  <code>staticmethod</code>","text":"<p>Defines an alias(...) grouping syntax. This lets you name an expression, so you can reference it later by $alias_name.</p> <p>Parameters:</p> Name Type Description Default <code>alias_name</code> <code>str</code> <p>The alias name.</p> required <code>expression</code> <code>str</code> <p>The expression to alias.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A Vespa grouping expression string of the form 'alias(alias_name, expression)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.alias(\"my_alias\", G.add(\"fieldA\", \"fieldB\"))\nprint(expr)\nalias(my_alias,add(fieldA, fieldB))\n</code></pre>"},{"location":"examples/index.html","title":"Examples","text":"<p>Here you can find a wide variety of examples that demonstrate how to use the Vespa Python API. These examples cover different use cases and functionalities, providing a practical understanding of how to interact with Vespa using Python.</p> <p>Check out the sidebar for full list of examples.</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html","title":"Matryoshka embeddings in Vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa ir_datasets openai pytrec_eval vespacli\n</pre> !pip3 install -U pyvespa ir_datasets openai pytrec_eval vespacli In\u00a0[1]: Copied! <pre>from openai import OpenAI\n\nopenai = OpenAI()\n\n\ndef embed(text, model=\"text-embedding-3-large\", dimensions=3072):\n    return (\n        openai.embeddings.create(input=[text], model=model, dimensions=dimensions)\n        .data[0]\n        .embedding\n    )\n</pre> from openai import OpenAI  openai = OpenAI()   def embed(text, model=\"text-embedding-3-large\", dimensions=3072):     return (         openai.embeddings.create(input=[text], model=model, dimensions=dimensions)         .data[0]         .embedding     ) <p>With these new embedding models, the API supports a <code>dimensions</code> parameter. Does this differ from just taking the first N dimensions?</p> In\u00a0[2]: Copied! <pre>test_input = \"This is just a test sentence.\"\n\nfull = embed(test_input)\nshort = embed(test_input, dimensions=8)\n\nprint(full[:8])\nprint(short)\n</pre> test_input = \"This is just a test sentence.\"  full = embed(test_input) short = embed(test_input, dimensions=8)  print(full[:8]) print(short) <pre>[0.0035371531266719103, 0.014166134409606457, -0.017565304413437843, 0.04296272248029709, 0.012746891938149929, -0.01731124334037304, -0.00855049304664135, 0.044189225882291794]\n[0.05076185241341591, 0.20329885184764862, -0.2520805299282074, 0.6165600419044495, 0.18293125927448273, -0.24843446910381317, -0.1227085217833519, 0.634161651134491]\n</pre> <p>Numerically, they are not the same. But looking more closely, they differ only by a scaling factor:</p> In\u00a0[12]: Copied! <pre>scale = short[0] / full[0]\nprint([x * scale for x in full[:8]])\nprint(short)\n</pre> scale = short[0] / full[0] print([x * scale for x in full[:8]]) print(short) <pre>[0.05076185241341591, 0.2032988673141365, -0.2520805173822377, 0.6165600695594861, 0.18293125124128834, -0.2484344748635628, -0.12270853156530777, 0.6341616780980419]\n[0.05076185241341591, 0.20329885184764862, -0.2520805299282074, 0.6165600419044495, 0.18293125927448273, -0.24843446910381317, -0.1227085217833519, 0.634161651134491]\n</pre> <p>It seems the shortened vector has been L2 normalized to have a magnitude of 1. By cosine similarity, they are equivalent:</p> In\u00a0[13]: Copied! <pre>from numpy.linalg import norm\nfrom numpy import dot\n\n\ndef cos_sim(e1, e2):\n    return dot(e1, e2) / (norm(e1) * norm(e2))\n\n\nprint(norm(short))\n\ncos_sim(short, full[:8])\n</pre> from numpy.linalg import norm from numpy import dot   def cos_sim(e1, e2):     return dot(e1, e2) / (norm(e1) * norm(e2))   print(norm(short))  cos_sim(short, full[:8]) <pre>0.9999999899058183\n</pre> Out[13]: <pre>0.9999999999999996</pre> <p>This is great, because it means that in a single API call we can get the full embeddings, and easily produce shortened embeddings just by slicing the list of numbers.</p> <p>Note that <code>text-embedding-3-large</code> and <code>text-embedding-3-small</code> do not produce compatible embeddings when sliced to the same size:</p> In\u00a0[14]: Copied! <pre>cos_sim(\n    embed(test_input, dimensions=1536),\n    embed(test_input, dimensions=1536, model=\"text-embedding-3-small\"),\n)\n</pre> cos_sim(     embed(test_input, dimensions=1536),     embed(test_input, dimensions=1536, model=\"text-embedding-3-small\"), ) Out[14]: <pre>-0.03217247156447633</pre> In\u00a0[15]: Copied! <pre>import ir_datasets\n\ndataset = ir_datasets.load(\"beir/trec-covid\")\nprint(\"Dataset has\", dataset.docs_count(), \"documents. Sample:\")\ndataset.docs_iter()[120]._asdict()\n</pre> import ir_datasets  dataset = ir_datasets.load(\"beir/trec-covid\") print(\"Dataset has\", dataset.docs_count(), \"documents. Sample:\") dataset.docs_iter()[120]._asdict() <pre>Dataset has 171332 documents. Sample:\n</pre> Out[15]: <pre>{'doc_id': 'z2u5frvq',\n 'text': 'The authors discuss humoral immune responses to HIV and approaches to designing vaccines that induce viral neutralizing and other potentially protective antibodies.',\n 'title': 'Antibody-Based HIV-1 Vaccines: Recent Developments and Future Directions: A summary report from a Global HIV Vaccine Enterprise Working Group',\n 'url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2100141/',\n 'pubmed_id': '18052607'}</pre> In\u00a0[16]: Copied! <pre>print(next(dataset.queries_iter()))\nprint(next(dataset.qrels_iter()))\n</pre> print(next(dataset.queries_iter())) print(next(dataset.qrels_iter())) <pre>BeirCovidQuery(query_id='1', text='what is the origin of COVID-19', query='coronavirus origin', narrative=\"seeking range of information about the SARS-CoV-2 virus's origin, including its evolution, animal source, and first transmission into humans\")\nTrecQrel(query_id='1', doc_id='005b2j4b', relevance=2, iteration='0')\n</pre> <p>We'll use these later to evaluate the result quality.</p> In\u00a0[33]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\nmy_schema = Schema(\n    name=\"my_schema\",\n    mode=\"index\",\n    document=Document(\n        fields=[\n            Field(name=\"doc_id\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"pubmed_id\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(\n                name=\"shortened\",\n                type=\"tensor&lt;float&gt;(x[256])\",\n                indexing=[\"attribute\", \"index\"],\n                attribute=[\"distance-metric: angular\"],\n            ),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;float&gt;(x[3072])\",\n                indexing=[\"attribute\"],\n                attribute=[\"paged\", \"distance-metric: angular\"],\n            ),\n        ],\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"text\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  my_schema = Schema(     name=\"my_schema\",     mode=\"index\",     document=Document(         fields=[             Field(name=\"doc_id\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"pubmed_id\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(                 name=\"shortened\",                 type=\"tensor(x[256])\",                 indexing=[\"attribute\", \"index\"],                 attribute=[\"distance-metric: angular\"],             ),             Field(                 name=\"embedding\",                 type=\"tensor(x[3072])\",                 indexing=[\"attribute\"],                 attribute=[\"paged\", \"distance-metric: angular\"],             ),         ],     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"text\"])], ) <p>The two fields of type <code>tensor&lt;float&gt;(x[3072/256])</code> are not in the dataset - they are tensor fields to hold the embeddings from OpenAI.</p> <ul> <li><p><code>shortened</code>: This field holds the embedding shortened to 256 dimensions, requiring only 8.3% of the memory. <code>index</code> here means we will build an HNSW Approximate Nearest Neighbor index, by which we can find the closest vectors while exploring only a very small subset of the documents.</p> </li> <li><p><code>embedding</code>: This field contains the full size embedding. It is paged: accesses to this field may require disk access, unless it has been cached by the kernel.</p> </li> </ul> <p>We must add the schema to a Vespa application package. This consists of configuration files, schemas, models, and possibly even custom code (plugins).</p> In\u00a0[34]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"matryoshka\"\nvespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema])\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"matryoshka\" vespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema]) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p>Vespa supports has a rich set of built-in rank-features, including many text-matching features such as:</p> <ul> <li>BM25,</li> <li>nativeRank and many more.</li> </ul> <p>Users can also define custom functions using ranking expressions.</p> <p>The following defines three runtime selectable Vespa ranking profiles:</p> <ul> <li><code>exact</code> uses the full-size embedding</li> <li><code>shortened</code> uses only 256 dimensions (exact, or using the approximate nearest neighbor HNSW index)</li> <li><code>rerank</code> uses the 256-dimension shortened embeddings (exact or ANN) in a first phase, and the full 3072-dimension embeddings in a second phase. By default the second phase is applied to the top 100 documents from the first phase.</li> </ul> In\u00a0[35]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\nexact = RankProfile(\n    name=\"exact\",\n    inputs=[(\"query(q3072)\", \"tensor&lt;float&gt;(x[3072])\")],\n    functions=[Function(name=\"cos_sim\", expression=\"closeness(field, embedding)\")],\n    first_phase=FirstPhaseRanking(expression=\"cos_sim\"),\n    match_features=[\"cos_sim\"],\n)\nmy_schema.add_rank_profile(exact)\n\n\nshortened = RankProfile(\n    name=\"shortened\",\n    inputs=[(\"query(q256)\", \"tensor&lt;float&gt;(x[256])\")],\n    functions=[Function(name=\"cos_sim_256\", expression=\"closeness(field, shortened)\")],\n    first_phase=FirstPhaseRanking(expression=\"cos_sim_256\"),\n    match_features=[\"cos_sim_256\"],\n)\nmy_schema.add_rank_profile(shortened)\n\n\nrerank = RankProfile(\n    name=\"rerank\",\n    inputs=[\n        (\"query(q3072)\", \"tensor&lt;float&gt;(x[3072])\"),\n        (\"query(q256)\", \"tensor&lt;float&gt;(x[256])\"),\n    ],\n    functions=[\n        Function(name=\"cos_sim_256\", expression=\"closeness(field, shortened)\"),\n        Function(\n            name=\"cos_sim_3072\",\n            expression=\"cosine_similarity(query(q3072), attribute(embedding), x)\",\n        ),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"cos_sim_256\"),\n    second_phase=SecondPhaseRanking(expression=\"cos_sim_3072\"),\n    match_features=[\"cos_sim_256\", \"cos_sim_3072\"],\n)\nmy_schema.add_rank_profile(rerank)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  exact = RankProfile(     name=\"exact\",     inputs=[(\"query(q3072)\", \"tensor(x[3072])\")],     functions=[Function(name=\"cos_sim\", expression=\"closeness(field, embedding)\")],     first_phase=FirstPhaseRanking(expression=\"cos_sim\"),     match_features=[\"cos_sim\"], ) my_schema.add_rank_profile(exact)   shortened = RankProfile(     name=\"shortened\",     inputs=[(\"query(q256)\", \"tensor(x[256])\")],     functions=[Function(name=\"cos_sim_256\", expression=\"closeness(field, shortened)\")],     first_phase=FirstPhaseRanking(expression=\"cos_sim_256\"),     match_features=[\"cos_sim_256\"], ) my_schema.add_rank_profile(shortened)   rerank = RankProfile(     name=\"rerank\",     inputs=[         (\"query(q3072)\", \"tensor(x[3072])\"),         (\"query(q256)\", \"tensor(x[256])\"),     ],     functions=[         Function(name=\"cos_sim_256\", expression=\"closeness(field, shortened)\"),         Function(             name=\"cos_sim_3072\",             expression=\"cosine_similarity(query(q3072), attribute(embedding), x)\",         ),     ],     first_phase=FirstPhaseRanking(expression=\"cos_sim_256\"),     second_phase=SecondPhaseRanking(expression=\"cos_sim_3072\"),     match_features=[\"cos_sim_256\", \"cos_sim_3072\"], ) my_schema.add_rank_profile(rerank) <p>For an example of a <code>hybrid</code> rank-profile which combines semantic search with traditional text retrieval such as BM25, see the previous blog post: Turbocharge RAG with LangChain and Vespa Streaming Mode for Sharded Data</p> In\u00a0[36]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[37]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 3 of dev-aws-us-east-1c for vespa-team.matryoshka. This may take a few minutes the first time.\nINFO    [15:51:53]  Deploying platform version 8.296.15 and application dev build 3 for dev-aws-us-east-1c of default ...\nINFO    [15:51:53]  Using CA signed certificate version 0\nINFO    [15:51:53]  Using 1 nodes in container cluster 'matryoshka_container'\nINFO    [15:51:57]  Session 282395 for tenant 'vespa-team' prepared and activated.\nINFO    [15:52:00]  ######## Details for all nodes ########\nINFO    [15:52:09]  h88969c.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [15:52:09]  --- platform vespa/cloud-tenant-rhel8:8.296.15 &lt;-- :\nINFO    [15:52:09]  --- logserver-container on port 4080 has not started \nINFO    [15:52:09]  --- metricsproxy-container on port 19092 has not started \nINFO    [15:52:09]  h88972f.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [15:52:09]  --- platform vespa/cloud-tenant-rhel8:8.296.15 &lt;-- :\nINFO    [15:52:09]  --- container-clustercontroller on port 19050 has not started \nINFO    [15:52:09]  --- metricsproxy-container on port 19092 has not started \nINFO    [15:52:09]  h90002a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [15:52:09]  --- platform vespa/cloud-tenant-rhel8:8.296.15 &lt;-- :\nINFO    [15:52:09]  --- storagenode on port 19102 has not started \nINFO    [15:52:09]  --- searchnode on port 19107 has not started \nINFO    [15:52:09]  --- distributor on port 19111 has not started \nINFO    [15:52:09]  --- metricsproxy-container on port 19092 has not started \nINFO    [15:52:09]  h90512a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [15:52:09]  --- platform vespa/cloud-tenant-rhel8:8.296.15 &lt;-- :\nINFO    [15:52:09]  --- container on port 4080 has not started \nINFO    [15:52:09]  --- metricsproxy-container on port 19092 has not started \nINFO    [15:53:11]  Found endpoints:\nINFO    [15:53:11]  - dev.aws-us-east-1c\nINFO    [15:53:11]   |-- https://e5ba4967.b2349765.z.vespa-app.cloud/ (cluster 'matryoshka_container')\nINFO    [15:53:12]  Installation succeeded!\nUsing mTLS (key,cert) Authentication against endpoint https://e5ba4967.b2349765.z.vespa-app.cloud//ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[44]: Copied! <pre>import concurrent.futures\n\n# only embed 100 docs while developing\nsample_docs = list(dataset.docs_iter())[:100]\n\n\ndef embed_doc(doc):\n    embedding = embed(\n        (doc.title + \" \" + doc.text)[:8192]\n    )  # we crop the ~25 documents which are longer than the context window\n    shortened = embedding[0:256]\n    return {\n        \"doc_id\": doc.doc_id,\n        \"text\": doc.text,\n        \"title\": doc.title,\n        \"url\": doc.url,\n        \"pubmed_id\": doc.pubmed_id,\n        \"shortened\": {\"type\": \"tensor&lt;float&gt;(x[256])\", \"values\": shortened},\n        \"embedding\": {\"type\": \"tensor&lt;float&gt;(x[3072])\", \"values\": embedding},\n    }\n\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n    my_docs_to_feed = list(executor.map(embed_doc, sample_docs))\n</pre> import concurrent.futures  # only embed 100 docs while developing sample_docs = list(dataset.docs_iter())[:100]   def embed_doc(doc):     embedding = embed(         (doc.title + \" \" + doc.text)[:8192]     )  # we crop the ~25 documents which are longer than the context window     shortened = embedding[0:256]     return {         \"doc_id\": doc.doc_id,         \"text\": doc.text,         \"title\": doc.title,         \"url\": doc.url,         \"pubmed_id\": doc.pubmed_id,         \"shortened\": {\"type\": \"tensor(x[256])\", \"values\": shortened},         \"embedding\": {\"type\": \"tensor(x[3072])\", \"values\": embedding},     }   with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:     my_docs_to_feed = list(executor.map(embed_doc, sample_docs)) In\u00a0[45]: Copied! <pre>from typing import Iterable\n\n\ndef vespa_feed(user: str) -&gt; Iterable[dict]:\n    for doc in reversed(my_docs_to_feed):\n        yield {\"fields\": doc, \"id\": doc[\"doc_id\"], \"groupname\": user}\n</pre> from typing import Iterable   def vespa_feed(user: str) -&gt; Iterable[dict]:     for doc in reversed(my_docs_to_feed):         yield {\"fields\": doc, \"id\": doc[\"doc_id\"], \"groupname\": user} <p>Now, we can feed to the Vespa instance (<code>app</code>), using the <code>feed_iterable</code> API, using the generator function above as input with a custom <code>callback</code> function.</p> In\u00a0[46]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Document {id} failed to feed with status code {response.status_code}, url={response.url} response={response.json}\"\n        )\n\n\napp.feed_iterable(\n    schema=\"my_schema\",\n    iter=vespa_feed(\"\"),\n    callback=callback,\n    max_queue_size=2000,\n    max_workers=32,\n    max_connections=64,\n)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Document {id} failed to feed with status code {response.status_code}, url={response.url} response={response.json}\"         )   app.feed_iterable(     schema=\"my_schema\",     iter=vespa_feed(\"\"),     callback=callback,     max_queue_size=2000,     max_workers=32,     max_connections=64, ) In\u00a0[47]: Copied! <pre>queries = []\nfor q in dataset.queries_iter():\n    queries.append({\"text\": q.text, \"embedding\": embed(q.text), \"id\": q.query_id})\n</pre> queries = [] for q in dataset.queries_iter():     queries.append({\"text\": q.text, \"embedding\": embed(q.text), \"id\": q.query_id}) In\u00a0[73]: Copied! <pre>import json\n\n\ndef query_exact(q):\n    return session.query(\n        yql=\"select doc_id, title from my_schema where ({targetHits: 10, approximate:false}nearestNeighbor(embedding,q3072)) limit 10\",\n        ranking=\"exact\",\n        timeout=10,\n        body={\"presentation.timing\": \"true\", \"input.query(q3072)\": q[\"embedding\"]},\n    )\n\n\ndef query_256(q):\n    return session.query(\n        yql=\"select doc_id from my_schema where ({targetHits: 10, approximate:false}nearestNeighbor(shortened,q256)) limit 10\",\n        ranking=\"shortened\",\n        timeout=10,\n        body={\"presentation.timing\": \"true\", \"input.query(q256)\": q[\"embedding\"][:256]},\n    )\n\n\ndef query_256_ann(q):\n    return session.query(\n        yql=\"select doc_id from my_schema where ({targetHits: 100, approximate:true}nearestNeighbor(shortened,q256)) limit 10\",\n        ranking=\"shortened\",\n        timeout=10,\n        body={\"presentation.timing\": \"true\", \"input.query(q256)\": q[\"embedding\"][:256]},\n    )\n\n\ndef query_rerank(q):\n    return session.query(\n        yql=\"select doc_id from my_schema where ({targetHits: 100, approximate:true}nearestNeighbor(shortened,q256)) limit 10\",\n        ranking=\"rerank\",\n        timeout=10,\n        body={\n            \"presentation.timing\": \"true\",\n            \"input.query(q256)\": q[\"embedding\"][:256],\n            \"input.query(q3072)\": q[\"embedding\"],\n        },\n    )\n\n\nprint(\"Sample query:\", queries[0][\"text\"])\nwith app.syncio() as session:\n    print(json.dumps(query_rerank(queries[0]).hits[0], indent=2))\n</pre> import json   def query_exact(q):     return session.query(         yql=\"select doc_id, title from my_schema where ({targetHits: 10, approximate:false}nearestNeighbor(embedding,q3072)) limit 10\",         ranking=\"exact\",         timeout=10,         body={\"presentation.timing\": \"true\", \"input.query(q3072)\": q[\"embedding\"]},     )   def query_256(q):     return session.query(         yql=\"select doc_id from my_schema where ({targetHits: 10, approximate:false}nearestNeighbor(shortened,q256)) limit 10\",         ranking=\"shortened\",         timeout=10,         body={\"presentation.timing\": \"true\", \"input.query(q256)\": q[\"embedding\"][:256]},     )   def query_256_ann(q):     return session.query(         yql=\"select doc_id from my_schema where ({targetHits: 100, approximate:true}nearestNeighbor(shortened,q256)) limit 10\",         ranking=\"shortened\",         timeout=10,         body={\"presentation.timing\": \"true\", \"input.query(q256)\": q[\"embedding\"][:256]},     )   def query_rerank(q):     return session.query(         yql=\"select doc_id from my_schema where ({targetHits: 100, approximate:true}nearestNeighbor(shortened,q256)) limit 10\",         ranking=\"rerank\",         timeout=10,         body={             \"presentation.timing\": \"true\",             \"input.query(q256)\": q[\"embedding\"][:256],             \"input.query(q3072)\": q[\"embedding\"],         },     )   print(\"Sample query:\", queries[0][\"text\"]) with app.syncio() as session:     print(json.dumps(query_rerank(queries[0]).hits[0], indent=2)) <pre>Sample query: what is the origin of COVID-19\n</pre> <pre>{\n  \"id\": \"index:matryoshka_content/0/16c7e8749fb82d3b5e37bedb\",\n  \"relevance\": 0.6591723960884718,\n  \"source\": \"matryoshka_content\",\n  \"fields\": {\n    \"matchfeatures\": {\n      \"cos_sim_256\": 0.5481410972571522,\n      \"cos_sim_3072\": 0.6591723960884718\n    },\n    \"doc_id\": \"beguhous\"\n  }\n}\n</pre> <p>Here's the top result from the first query. Notice the <code>matchfeatures</code> that returns the match-features from the rank-profile.</p> <p>Now for each method of querying, we'll run all our queries and note the rank of each document in the response:</p> In\u00a0[72]: Copied! <pre>global qt\n\n\ndef run_queries(query_function):\n    print(\n        \"\\nrun\",\n        query_function.__name__,\n    )\n    results = {}\n    for q in queries:\n        response = query_function(q)\n        assert response.is_successful()\n        print(\".\", end=\"\")\n        results[q[\"id\"]] = {}\n        for pos, hit in enumerate(response.hits, start=1):\n            global qt\n            qt += float(response.get_json()[\"timing\"][\"querytime\"])\n            results[q[\"id\"]][hit[\"fields\"][\"doc_id\"]] = pos\n    return results\n\n\nquery_functions = (query_exact, query_256, query_256_ann, query_rerank)\nruns = {}\n\nwith app.syncio() as session:\n    for f in query_functions:\n        qt = 0\n        runs[f.__name__] = run_queries(f)\n        print(\" avg query time {:.4f} s\".format(qt / len(queries)))\n</pre> global qt   def run_queries(query_function):     print(         \"\\nrun\",         query_function.__name__,     )     results = {}     for q in queries:         response = query_function(q)         assert response.is_successful()         print(\".\", end=\"\")         results[q[\"id\"]] = {}         for pos, hit in enumerate(response.hits, start=1):             global qt             qt += float(response.get_json()[\"timing\"][\"querytime\"])             results[q[\"id\"]][hit[\"fields\"][\"doc_id\"]] = pos     return results   query_functions = (query_exact, query_256, query_256_ann, query_rerank) runs = {}  with app.syncio() as session:     for f in query_functions:         qt = 0         runs[f.__name__] = run_queries(f)         print(\" avg query time {:.4f} s\".format(qt / len(queries))) <pre>\nrun query_exact\n</pre> <pre>.................................................. avg query time 2.7918 s\n\nrun query_256\n.................................................. avg query time 0.3040 s\n\nrun query_256_ann\n.................................................. avg query time 0.0252 s\n\nrun query_rerank\n.................................................. avg query time 0.0310 s\n</pre> <p>The query time numbers here are NOT a proper benchmark but can illustrate some significant trends for this case:</p> <ul> <li>Doing exact NN with 3072 dimensions is too slow and expensive for many use cases</li> <li>Reducing dimensionality to 256 reduces latency by an order of magnitude</li> <li>Using an ANN index improves query time by another order of magnitude</li> <li>Re-ranking the top 100 results with the full embedding causes only a slight increase</li> </ul> <p>We could use more cores per search or sharding over multiple nodes to improve latency and handle larger content volumes.</p> In\u00a0[62]: Copied! <pre>qrels = {}\n\nfor q in dataset.queries_iter():\n    qrels[q.query_id] = {}\n\nfor qrel in dataset.qrels_iter():\n    qrels[qrel.query_id][qrel.doc_id] = qrel.relevance\n</pre> qrels = {}  for q in dataset.queries_iter():     qrels[q.query_id] = {}  for qrel in dataset.qrels_iter():     qrels[qrel.query_id][qrel.doc_id] = qrel.relevance <p>With that done, we can check the scores for the first query:</p> In\u00a0[70]: Copied! <pre>for docid in runs[\"query_256_ann\"][\"1\"]:\n    score = qrels[\"1\"].get(docid)\n    print(docid, score or \"-\")\n</pre> for docid in runs[\"query_256_ann\"][\"1\"]:     score = qrels[\"1\"].get(docid)     print(docid, score or \"-\") <pre>beguhous 2\nk9lcpjyo 2\npl48ev5o 2\njwxt4ygt 2\ndv9m19yk 1\nft4rbcxf 1\nh8ahn8fw 2\n6y1gwszn 2\n3xusxrij -\n2tyt8255 1\n</pre> <p>A lot of '2', that is, 'highly relevant' results: Looks promising! Now we can use trec_eval to evaluate all the data for each run. The quality measure we use here is <code>nDCG@10</code> - Normalized Discounted Cumulative Gain, computed for the first 10 results of each query. The evaluations are per-query so we compute and report the average per run.</p> In\u00a0[71]: Copied! <pre>import pytrec_eval\n\n\ndef evaluate(run):\n    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.10\"})\n    evaluation = evaluator.evaluate(run)\n\n    sum = 0\n    for ev in evaluation:\n        sum += evaluation[ev][\"ndcg_cut_10\"]\n    return sum / len(evaluation)\n\n\nfor run in runs:\n    print(run, \"\\tndcg_cut_10: {:.4f}\".format(evaluate(runs[run])))\n</pre> import pytrec_eval   def evaluate(run):     evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.10\"})     evaluation = evaluator.evaluate(run)      sum = 0     for ev in evaluation:         sum += evaluation[ev][\"ndcg_cut_10\"]     return sum / len(evaluation)   for run in runs:     print(run, \"\\tndcg_cut_10: {:.4f}\".format(evaluate(runs[run]))) <pre>query_exact \tndcg_cut_10: 0.7870\nquery_256 \tndcg_cut_10: 0.7574\nquery_256_ann \tndcg_cut_10: 0.7552\nquery_rerank \tndcg_cut_10: 0.7886\n</pre> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#exploring-the-potential-of-openai-matryoshka-embeddings-with-vespa","title":"Exploring the potential of OpenAI Matryoshka \ud83e\ude86 embeddings with Vespa\u00b6","text":"<p>This notebook demonstrates the effectiveness of using the recently released (as of January 2024) OpenAI <code>text-embedding-3</code> embeddings with Vespa.</p> <p>Specifically, we are interested in the Matryoshka Representation Learning technique used in training, which lets us \"shorten embeddings (i.e. remove some numbers from the end of the sequence) without the embedding losing its concept-representing properties\". This allow us to trade off a small amount of accuracy in exchange for much smaller embedding sizes, so we can store more documents and search them faster.</p> <p>Exploring the potential of OpenAI Matryoshka \ud83e\ude86 embeddings with Vespa and Matryoshka \ud83e\udd1d Binary vectors: Slash vector search costs with Vespa are good reads on this subject.</p> <p>By using phased ranking, we can re-rank the top K results with the full embeddings in a second step. This produces accuracy on par with using the full embeddings!</p> <p>We'll use a standard information retrieval benchmark to evaluate result quality with different embedding sizes and retrieval/ranking strategies.</p> <p></p> <p>Let's get started! First, install a few dependencies:</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#examining-the-openai-embeddings","title":"Examining the OpenAI embeddings\u00b6","text":""},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#getting-a-sample-dataset","title":"Getting a sample dataset\u00b6","text":"<p>Let's download a dataset so we have some real data to embed:</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#queries","title":"Queries\u00b6","text":"<p>This dataset also comes with a set of queries, and query/document relevance judgements:</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#get-openai-embeddings-for-documents-in-the-dataset","title":"Get OpenAI embeddings for documents in the dataset\u00b6","text":"<p>When producing the embeddings, we concatenate the title and text into a single string. We could also have created two separate embedding fields for text and title, combining the rank scores for these fields in a Vespa rank expression.</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#feeding-the-dataset-and-embeddings-into-vespa","title":"Feeding the dataset and embeddings into Vespa\u00b6","text":"<p>Now that we have parsed the dataset and created an object with the fields that we want to add to Vespa, we must format the object into the format that PyVespa accepts. Notice the <code>fields</code>, <code>id</code> and <code>groupname</code> keys. The <code>groupname</code> is the key that is used to shard and co-locate the data and is only relevant when using Vespa with streaming mode.</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#embedding-the-queries","title":"Embedding the queries\u00b6","text":"<p>We need to obtain embeddings for the queries from OpenAI. If only using the shortened embedding for the query, you should specify this in the OpenAI API call to reduce latency.</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Now we can query our data. We'll do it in a few different ways, using the rank profiles we defined in the schema:</p> <ul> <li>Exhaustive (exact) nearest neighbor search with the full embeddings (3072 dimensions)</li> <li>Exhaustive (exact) nearest neighbor search with the shortened 256 dimensions</li> <li>Approximate nearest neighbor search, using the 256 dimension ANN HNSW index</li> <li>Approximate nearest neighbor search, using the 256 dimension ANN HNSW index in the first phase, then reranking top 100 hits with the full embeddings</li> </ul> <p>The query request uses the Vespa Query API and the <code>Vespa.query()</code> function supports passing any of the Vespa query API parameters.</p> <p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> </ul>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#evaluating-the-query-results","title":"Evaluating the query results\u00b6","text":"<p>We need to get the query relevance judgements into the format supported by pytrec_eval:</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#conclusions","title":"Conclusions\u00b6","text":"<p>What do the numbers mean? They are good, highly relevant results. This is no great surprise, as the OpenAI embedding models are reported to score high on the Massive Text Embedding Benchmark, of which our BEIR/TREC-COVID dataset is a part.</p> <p>More interesting to us, querying with the first 256 dimensions still gives quite good results, while requiring only 8.3% of the memory. We also note that although the HNSW index is an approximation, result quality is impacted very little, while producing the results an order of magnitude faster.</p> <p>When adding a second phase to re-rank the top 100 hits using the full embeddings, the results are as good as the exact search, while retaining the lower latency, giving us the best of both worlds.</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#summary","title":"Summary\u00b6","text":"<p>For those interested in learning more about Vespa, join the Vespa community on Slack to exchange ideas, seek assistance, or stay in the loop on the latest Vespa developments.</p> <p>We can now delete the cloud instance:</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html","title":"Billion scale vector search with cohere embeddings cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa cohere==4.57 vespacli\n</pre> !pip3 install -U pyvespa cohere==4.57 vespacli In\u00a0[3]: Copied! <pre>import cohere\n\n# Make sure that the environment variable CO_API_KEY is set to your API key\nco = cohere.Client()\n</pre> import cohere  # Make sure that the environment variable CO_API_KEY is set to your API key co = cohere.Client() In\u00a0[4]: Copied! <pre>documents = [\n    \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",\n    \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\",\n    \"Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.\",\n    \"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\",\n]\n</pre> documents = [     \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",     \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\",     \"Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.\",     \"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\", ] In\u00a0[5]: Copied! <pre># Compute the  embeddings of our sample documents.\n# Set input_type to \"search_document\" and embedding_types to \"binary\" and \"int8\"\nembeddings = co.embed(\n    texts=documents,\n    model=\"embed-english-v3.0\",\n    input_type=\"search_document\",\n    embedding_types=[\"binary\", \"int8\"],\n)\n</pre> # Compute the  embeddings of our sample documents. # Set input_type to \"search_document\" and embedding_types to \"binary\" and \"int8\" embeddings = co.embed(     texts=documents,     model=\"embed-english-v3.0\",     input_type=\"search_document\",     embedding_types=[\"binary\", \"int8\"], ) In\u00a0[6]: Copied! <pre>print(embeddings)\n</pre> print(embeddings) <pre>cohere.Embeddings {\n\tresponse_type: embeddings_by_type\n\tembeddings: cohere.EmbeddingsByType {\n\tfloat: None\n\tint8: [[-23, -22, -52, 18, -42, -48, 2, -8, 6, 44, 73, 9, 3, -44, -25, 15, 19, 3, 18, -19, 6, 17, 0, -62, -14, 46, -8, -14, 20, 22, 10, -40, 10, 48, -20, 40, -8, 8, 29, 0, -27, 11, -39, -28, -93, 33, -89, 4, 15, -41, -12, -2, 7, -23, -15, -21, 47, -9, 88, -107, -91, -50, 65, 27, 5, 5, 52, 27, -15, -4, 14, -7, 6, -1, -17, 13, 17, 74, 26, -9, -4, -1, 56, -15, -7, 6, -17, -25, -23, -38, -38, 78, -61, -27, -53, -20, -3, -8, -9, -18, 9, -24, 14, -13, -40, 90, 40, 24, -48, -7, -11, -116, 36, -56, -15, -1, -6, 31, 31, 8, 44, 80, 36, -35, -24, -13, -36, -64, 44, -11, -35, 46, -43, -68, -40, 12, 32, -8, -1, 58, -9, -4, 49, 3, 9, 44, 45, -33, -52, -25, -53, 27, -67, 22, 33, 29, -32, 36, 37, 83, -17, 19, 66, -17, 4, -57, -57, 20, 19, -20, -3, 18, 43, -16, -8, 29, -45, -39, -42, 121, 73, -49, -128, 127, -19, 41, -10, 55, 38, 13, -66, 1, -52, -35, 59, 6, -60, -35, 20, -11, -20, 58, -50, 27, -1, -27, 0, 33, 36, 39, -22, -6, 0, -43, -34, -4, -2, -27, -37, -19, -48, 30, -59, 33, -79, 27, -51, 38, -46, 7, 99, 0, 46, 21, -39, -13, -1, -87, 22, 65, 42, -47, -66, -109, 73, 77, 47, -79, -17, 28, 8, -2, -2, -36, -12, 35, -41, 25, -1, 13, -17, 57, 98, -31, -26, -23, -3, -8, -13, -33, 22, -13, 6, 63, -64, -12, 5, -11, 0, 27, 5, 50, 35, -7, 11, 64, 9, 30, 31, -14, 2, 53, 23, 54, 21, -19, -30, 90, -20, -16, -69, -5, -7, -79, 6, -2, 23, 8, 18, -11, -14, 8, 21, 16, -14, -58, -37, -8, -86, -34, -22, 7, -39, -14, 5, 27, -78, -2, -5, -39, 42, 1, 4, 22, 16, -7, -2, 48, -26, -68, -48, -37, -7, -26, -27, 44, 9, 4, -33, 47, -59, -19, 10, 44, 56, -123, -38, 111, -25, -10, 18, 29, 8, -41, -26, -51, 9, 20, 68, 46, -44, 45, -67, 0, 41, 35, 39, -28, 13, 21, 25, 71, -28, 32, -18, 59, 14, 7, 10, -40, 20, 72, 51, -26, -18, -25, -35, 39, -34, 23, 127, -24, -26, 77, -88, 104, 45, 37, 31, -36, 23, -34, -1, -50, 0, -35, -45, -8, 40, 1, -51, 71, -60, 4, -18, -26, 19, 20, 1, 30, 6, -20, -13, 3, 23, 88, -14, -12, -31, -36, 51, 15, -4, 13, 5, -42, 17, 29, 13, 23, -17, 8, 23, 25, -36, -60, 22, 57, 4, 2, 29, -36, -41, 34, 12, -34, 46, 10, -28, 31, 18, 11, 4, 3, 7, 19, -30, 25, -56, 7, 7, 0, 64, -35, -33, 19, -72, -35, -20, -79, -81, 2, -1, -54, -17, -6, -24, 97, 47, -46, 48, -12, -33, -20, 43, 7, -16, 45, -5, 27, -7, -8, 19, 43, -43, 15, -21, 35, -35, -18, -39, -21, 18, 4, 13, 12, 12, 57, 0, -11, 121, 15, 58, 29, -86, 11, -42, 17, 47, -18, -27, -29, -26, 55, -19, 20, -6, 34, 0, -9, 4, 7, 27, -17, -35, -4, -20, 11, 4, 36, 5, -7, 27, -40, 127, 23, -30, -111, 37, -15, -35, -22, 5, -17, -23, -36, -23, 45, -38, 16, 47, 5, -49, 52, -28, -20, -6, -51, -50, -53, 33, 4, 16, -63, -2, 13, -36, -37, -19, -9, -42, 46, -14, -22, 72, 93, 106, -27, -5, 13, -23, -47, 4, 25, -6, -30, 22, -45, -96, -34, 22, -44, 43, 40, -2, -9, -45, 15, -11, 23, 18, 0, -44, 11, 25, -30, -29, -6, -19, -20, 47, 35, 39, -24, -19, 25, 19, -11, -13, 2, -50, -4, -9, -22, 17, -2, -65, 37, 15, 30, 15, 107, -47, 28, 11, 18, -22, 53, -41, 58, 8, -14, -28, -8, -10, 11, -18, 20, -38, 4, 0, -18, -13, 26, -51, 20, -23, 23, 52, 5, -3, 25, 3, 27, 28, 60, 1, -13, -21, -14, 10, 7, 12, 21, 0, -5, -39, 7, 3, -2, 4, 42, -45, -12, 38, 0, -10, -7, -39, 6, -37, 24, 17, -37, 26, 13, -60, -22, 27, 36, 5, 54, -21, -19, 30, -79, 17, 19, -24, 17, 111, -54, 61, -56, 7, 86, 17, 60, 11, 26, -6, 59, 16, 21, 25, -17, 13, 15, 7, -13, -83, -2, -17, 39, 21, 60, 33, 40, -69, 36, 14, 19, -3, -2, -37, 14, -4, -40, -9, 3, 49, 16, 54, -6, 3, -11, -4, 4, -6, 25, -65, 47, -25, -29, -41, 31, 57, -35, 30, -7, -3, -27, -36, -23, -34, 39, -2, -25, 2, 58, 11, 16, -14, -55, -7, -7, -110, -14, -47, -85, 77, 71, -10, 6, 13, -72, -32, 69, 7, -27, 9, -41, -40, -28, 30, -12, 26, -58, 74, -1, -50, 37, -81, -41, 42, -49, -22, 25, 0, 86, -8, -4, -1, -17, 1, 58, 12, -34, -42, -24, -33, 23, 2, 23, 3, -44, -33, -19, 14, -70, 7, 25, -13, -90, -57, -29, -11, -46, -34, 6, 14, 79, 108, 26, 31, 3, -9, 27, 66, 2, 41, -17, -19, 62, 23, 48, -20, 6, -88, 74, -59, -53, 67, -77, -32, 1, -3, -43, 22, -45, -34, 20, 60, 58, -65, -48, 116, 76, 127, 24, -29, 59, 10, -20, -57, -19, -3, 35, 19, 3, 34, 6, 55, 27, 35, -4, -55, 32, 22, -4, -12, -34, -50, -16, 0, -22, 75, -48, -51, -26, -12, 1, -9, -17, -26, -4, -60, -128, -3, -19, -23, -17, -4, -5, -5, 37, -8, -21, -1, -16, 49, 6, -31, -21, -18, -13, 33, -11, -29, 16, -31, 41, -19, 0, 57, -4, -9, 16, 27, -27, 6, 104, -53, 39, -6, -8, 3, 0, 4, 39, -46, 33, 10, 26, -19, 53, 41, 31, 15, 12, 2, 44, -67, -18, -88, -29, 27, 3, 55, -8, -6, 38, 0, 13], [-18, -43, -29, 10, -43, -28, 0, -20, -10, 81, 107, 17, 35, -44, -27, 54, -4, 31, 17, -23, 19, -18, -41, -67, 31, -74, -39, 18, -4, 2, 13, 37, -7, 51, -7, 42, 9, 11, 43, 22, 12, 0, -32, -20, -39, 9, -21, -28, 27, -33, -11, 25, 11, -44, -24, -38, 109, -75, 73, -125, -89, -59, 103, 43, 20, -14, -24, 8, -3, 55, 22, -23, -4, 8, -25, -1, 28, 37, 28, 0, -27, 13, 40, -8, -43, -16, -39, -13, 9, 7, -11, 42, -32, 63, 2, -42, 2, 14, -34, -30, 17, -45, 21, -19, -41, 123, 32, 55, -63, -7, 11, -128, 28, 7, -29, -18, -17, 51, 7, 46, 25, 70, 61, -86, -7, -8, -27, -92, 88, 8, -20, 19, -42, -29, -19, 5, -10, 38, -8, 68, -45, -51, 46, 0, 5, 39, 35, -16, 2, -56, -16, 16, -26, 6, 21, -12, -28, 6, 53, 31, -35, -5, 20, 20, -1, 0, 46, 14, 33, 30, 29, -4, 56, 8, -21, -3, -3, -122, -24, 127, 71, 5, -128, 83, 30, -52, -14, -49, 29, 23, -21, 4, -45, -22, 16, 39, -64, 29, -2, -31, 18, 10, 27, 2, 4, 18, -13, 31, 91, 23, -37, -2, 2, -32, -69, 14, -7, 8, -38, 47, -45, 6, -52, -2, -24, 44, -50, 28, 18, 21, 98, -20, -25, 53, -2, 16, 68, 29, 14, -23, -4, -91, -40, 40, -30, 46, 17, 11, 37, 24, -18, -65, 13, -110, 39, 13, 15, 69, -78, 31, -39, 54, 43, -4, -21, 13, -36, -21, -62, 51, 56, -66, 8, 59, -80, 23, -13, 6, -2, 38, -17, 55, 11, -17, -19, -20, 23, -5, -13, -47, 31, -16, -21, 15, -26, -35, -39, 1, 28, -15, -52, 63, -3, 8, -9, 1, -20, 4, 0, -34, -19, 27, 17, -9, -11, -42, -10, 0, -66, -34, -7, -21, 17, -1, -11, 1, -7, 10, -5, 7, 127, 72, 37, 0, 49, -14, 28, -32, -11, 20, 31, 30, 0, -71, -50, 66, 9, 25, -28, 29, -43, -40, -27, -13, -1, -78, 23, 46, -33, -22, 1, -11, -22, -16, 36, -26, -24, 7, 5, 2, -29, 30, -87, -21, -5, 49, 0, -50, 23, -13, -11, 29, 24, 44, 3, 30, -44, -9, 13, 3, -10, -16, 16, -27, 54, -28, 6, 110, -99, -21, 127, 2, -1, 52, -86, 94, 23, 36, 22, -18, -14, 5, -59, 0, -26, -22, -103, 0, -18, 17, -50, 99, -72, 28, 48, 47, 9, -48, 51, 40, 45, -15, -34, -6, 14, 103, -1, 48, -21, 0, 41, -9, -6, 66, -11, 4, -33, -2, 52, 0, 16, -8, 58, 3, -33, -9, 50, 51, 20, 43, 64, 0, -53, 39, -41, -20, 98, 5, -49, 18, -39, 25, 5, 30, -9, 57, -31, 3, -41, 32, -2, 11, 33, -27, -47, 36, -76, -34, -4, -47, -51, -19, 31, -30, 14, -14, -30, 100, 42, -52, 47, -24, -77, -1, 45, 9, 20, 52, 4, 83, 44, 5, 45, 49, -15, -3, 81, 2, 22, -23, -39, -27, 20, 32, -14, 10, -21, 17, 13, 32, 77, -9, 45, 29, -51, -24, -4, 29, 22, -50, 7, 10, -25, -2, -20, 30, -35, 27, -12, -1, -9, -15, 27, -5, -29, -85, -52, -20, 16, 68, 48, -23, -6, -20, 92, 19, -63, -128, 30, -9, -51, -36, 54, 45, -24, -41, 10, 36, -32, -28, -2, 25, 3, 44, -61, 32, 33, -7, -31, -2, 20, 7, -31, -17, -2, 19, 63, 26, 61, -4, -18, 23, 3, -26, -11, 59, 45, -22, 14, 7, -11, -30, -21, 48, -18, -25, 2, -20, -25, -38, 15, -4, 5, 8, 18, -37, -42, -56, -41, -10, -67, -2, -54, -86, -4, 49, 1, -2, -21, -11, 59, 14, 10, -59, -62, -15, -15, -19, 3, 6, -19, -1, -46, 4, 51, -17, -32, 37, 1, 13, 19, 114, -11, 6, 21, -12, 1, 21, -22, 10, -31, 11, -51, -39, -4, 2, 78, 30, 28, -97, -8, -53, -12, 15, -19, 30, -15, -2, 43, 15, 53, 93, 0, 55, 23, 19, -23, -51, -3, 1, 2, -26, 14, -27, -15, 61, 26, -16, 9, 4, 12, 24, -16, -14, 43, -20, 35, 34, -18, -14, -33, -20, 4, -29, 20, -6, -37, -21, 13, 40, -36, 30, 12, -34, -3, -52, -5, 4, -58, -21, 57, -29, 11, -48, -15, 12, -4, 26, -22, -43, 4, 41, -58, 25, 10, 17, -33, 33, -60, 30, -50, 20, 34, 12, 20, 65, -57, -1, -16, 41, 26, 92, 20, 16, -128, -11, 2, -30, -41, -17, 35, 9, 67, 3, -21, -13, 17, 19, 8, 26, -37, 47, 10, 33, 34, 35, 14, -12, 55, -5, -65, -14, -84, 5, -30, 35, -5, -27, 22, 34, 16, 32, 0, 33, -12, 72, -68, 0, -50, -50, 20, 37, -43, 74, 0, -11, 15, -43, 23, -49, -29, -35, 61, 47, 85, -3, -9, -53, 41, -20, -14, 11, -59, 1, -1, -56, -12, 11, 19, 4, 38, -76, -23, 18, 20, -15, -25, -55, -69, 53, 54, -82, -60, 6, 17, -74, -40, -25, -40, -83, 0, 33, -73, -101, -50, -39, -26, 43, -67, 1, -51, -11, 24, -26, 23, -43, 7, 11, 73, 3, 59, 2, -7, 89, 54, 58, -37, 1, -84, 88, -65, -56, -11, -60, -58, 21, 25, -54, 16, -42, -58, -14, 24, 17, -41, 27, 48, 40, 79, 13, -38, 31, -8, 15, 4, 6, 3, -33, -59, 45, 55, -7, -12, 0, 10, -22, -17, 35, 4, -7, 4, -23, -34, -23, -3, 58, 50, -32, -72, -37, -56, 36, -46, 6, 3, 12, -39, 20, 13, 37, 15, 9, 0, -28, -21, -14, 4, 17, 57, 1, -24, -12, -1, -14, -11, -47, -13, 0, -36, -44, -4, -43, -48, -33, -4, 8, -19, 12, -23, 24, -10, 18, 19, 38, -5, -6, 54, -28, 41, -19, -28, -19, 17, -26, -41, 9, -15, 90, 33, 20, -6, 82, -60, -40, -84, -36, -3, -62, 6, -34, -10, -31, -31, 20], [5, 6, -45, -35, -3, -37, -11, -10, -18, 36, 26, 57, -17, -33, 25, -7, -3, 66, 44, 8, 1, 40, -34, -57, 2, -34, -7, 12, 9, 2, 20, -4, 19, 37, 2, 58, -16, 28, 50, -19, 20, -14, -59, -48, -98, 61, -61, 48, 7, -50, -4, -35, 11, -32, -38, -37, 60, -75, 47, -48, -56, -41, 69, 12, 3, -1, 85, 32, -35, -21, 23, 17, 12, -33, -6, -30, -2, -14, 39, 12, 34, 64, -5, -65, 24, -60, -14, -20, -58, -27, -48, -33, -87, 6, 11, -23, -33, -87, 3, 22, 52, -50, 73, -2, -33, 99, 4, 86, -9, 8, 18, -104, 40, -12, -64, -19, -3, -5, 11, -18, -4, 13, 77, -36, 32, 7, -56, -34, 65, -40, -24, 76, -62, -88, -58, 32, 4, 22, 23, 8, -2, -43, 39, 39, -6, -24, 8, 14, 13, 22, -42, 21, -36, 4, 26, 30, -25, 32, 0, 66, 12, 7, 16, 8, -16, -21, 9, 7, 29, -26, -4, 12, 55, 21, 16, 69, -2, -53, -50, 127, 98, -3, -128, 116, -8, -27, 11, 19, -4, 10, 16, 8, -38, -22, 66, 11, -37, -19, 30, -62, 29, 47, -34, -12, -57, -16, -14, 35, 47, 38, -16, -7, -6, -4, -18, -24, -23, -48, -25, -17, -7, 22, -27, 64, -23, 1, -29, 5, -32, 14, 18, 16, 23, 37, -1, 21, 46, -50, 19, 21, 49, -71, -17, -17, 34, 16, 33, -31, -4, 69, -57, 39, 3, -43, -22, 69, -50, 33, -32, 8, -7, 112, 84, 17, -23, -4, 1, 0, -9, -14, 26, -22, 17, 102, -47, -15, 26, -22, -32, 40, -22, 29, -7, -26, -21, -55, 11, -4, 16, -9, 39, 14, 8, 36, -13, -32, -88, 38, 22, -19, -69, 43, -15, -15, 8, 4, -29, 21, 23, -13, -55, 9, 23, -32, 21, -37, -23, -4, -55, -3, 28, -28, 19, -48, -1, -20, -59, 2, -30, 42, -9, 47, 24, 100, -12, 9, -9, 12, -41, -10, -49, -11, 16, -64, 21, 49, 33, 27, -46, 68, -75, -44, 3, 41, 62, -81, -31, 72, 13, -30, -28, 27, -22, 16, -12, -24, 8, 25, 16, 11, -64, 34, -13, -11, 8, 29, 16, -29, 16, 20, 38, 44, 22, 13, 12, 29, -23, -26, 25, -25, -8, 27, 41, -23, 10, -7, -45, 0, -63, 16, 127, -21, -8, 52, -59, 74, 55, 40, 18, 2, -12, -9, -42, -8, -11, -9, -71, 1, -2, 27, -50, 80, -62, 21, -4, 16, -25, 10, -8, -9, 0, -32, -8, -3, -11, 57, -5, 37, 0, -41, 52, 29, -20, 18, -18, -22, 46, 29, 36, 8, 21, -25, 42, 9, -30, 49, 22, 13, -3, 33, 35, 25, -75, -13, -33, -77, 95, -2, 1, -16, -49, 92, -27, 7, 13, 77, -13, -13, -42, 17, -57, 19, -30, -12, -45, 28, -45, -13, -8, 0, -16, 2, 47, -28, -9, 30, -38, 127, 39, -30, 15, -18, -16, 10, 14, -9, 41, 27, 18, 63, 14, 3, 45, 5, -24, 41, -36, 46, -32, -28, 4, -10, 18, 35, 0, -15, -15, -24, -29, 32, 43, 16, 23, -14, 7, -13, -54, 11, 69, 40, -2, -9, -26, 82, 0, -24, -27, 38, -94, 54, -31, -22, 20, -27, -13, -128, -39, -22, 47, 78, 36, 6, -4, -45, 33, 17, -37, -103, 55, -41, -42, -46, -17, -29, 8, 11, 25, 60, 10, -28, 54, -65, -62, -10, -40, 26, 30, 13, -24, -25, -17, -4, -15, -54, 19, 48, -47, -38, -3, 6, 1, 18, -6, -13, -1, 63, 111, -18, -10, -11, -6, -62, -19, 53, -25, 9, 75, -50, -42, -43, 2, 26, 5, 0, -25, -62, -21, -27, -25, -1, 1, 19, -47, -37, -16, 13, -23, -40, -3, 19, 23, 38, 43, -102, 71, 5, -13, 52, -18, -29, -68, 2, -48, 28, 54, 9, -12, -14, -37, 3, 50, 63, -109, 63, 8, 21, -28, 58, -30, -2, 22, -14, -37, 28, 9, -48, 14, 1, -94, 10, -8, 6, 45, -5, -39, 26, -43, 5, 50, 55, -5, 6, 9, 11, 4, 29, -4, -6, -16, -56, 6, 16, 0, 14, 8, 39, -35, 10, 38, 28, 43, -11, -128, -36, 22, 2, -32, 28, 30, -13, 1, -40, 5, 13, 24, -52, -16, 16, 15, 15, -41, 91, -32, -43, 28, -21, -32, -2, -57, 25, 70, -32, 37, -25, -19, 67, 2, 53, 10, -25, -9, 79, -20, -20, 30, -35, -31, -36, -26, 20, -71, 25, 18, -18, 8, -31, 12, 118, -27, 43, 24, 15, 5, 49, -66, 21, -33, -40, 14, 52, 35, 72, 36, -38, 24, 0, -20, -19, 5, -32, -60, 51, 29, 21, -1, 40, 82, 24, 88, 8, -45, -29, -83, -37, -39, 42, -10, -34, 36, -7, 19, -18, -10, 0, -39, 35, -98, -27, -35, -15, 28, 44, -7, 13, -31, -24, 25, -29, 13, 62, 8, 14, 64, 61, 107, -42, 51, -86, 70, 0, -18, 60, -76, -17, 11, -70, -11, 32, -15, 60, 14, -43, -14, 0, 7, 20, 7, -24, -1, -28, -42, 66, 29, -2, 1, -10, -60, 2, -1, -48, 8, 78, -25, -62, -57, 29, -41, 46, -36, 41, -78, -19, -10, -33, 6, -19, -18, -12, 44, 10, 22, -52, 1, 10, 37, 50, -24, -15, -50, 13, -22, -29, 44, -74, -50, 20, 44, -18, 50, -42, -53, 25, 35, 46, -30, -39, 76, 12, 127, 21, -9, 10, 59, -43, -11, 22, 45, -20, 8, 17, 10, -27, 14, -11, 43, -22, -108, 72, -1, -1, -37, -29, 6, 50, -15, -12, 76, -51, -91, -48, -45, -9, -14, -31, 28, 16, -47, -41, 8, 10, 20, -17, -19, -35, 13, -8, -5, 4, 80, 46, 20, 35, -44, 39, -22, -54, -11, -9, -38, -28, 9, 6, -4, 3, -24, -63, 43, 56, 2, 9, -12, 127, -65, 22, -30, -9, -41, -43, -23, 50, -43, 61, 24, 81, -35, 36, 53, 30, -23, 43, -38, 43, -40, 13, -18, 0, 0, 3, -52, -45, 8, 46, -16, 38], [-47, -19, -104, 29, -32, -72, 0, 5, -53, 69, 56, 17, 0, -38, -10, 10, -44, 69, 20, -17, -2, -45, -19, -128, 34, -4, -64, 21, -23, -9, 13, 21, 28, 55, -52, 39, -1, 24, 0, 30, 2, 5, 28, 3, -30, 19, -33, -47, 27, -35, -29, -28, 5, -41, -40, -60, 43, -21, 49, -92, -60, -22, 59, 65, 35, -10, 24, 35, -76, 31, 35, -58, -4, -13, -47, 4, 12, -7, 18, -14, -36, 47, -8, -35, -28, -15, -41, -18, -72, -38, -39, 36, -128, 20, 44, -26, -8, 14, 1, 17, -20, -23, 1, -38, -30, 85, 61, 81, -16, 4, 2, -39, 40, -77, -22, 26, 24, 48, 56, 41, 25, 99, -37, -16, 41, 50, 16, -61, -25, -18, -34, 48, 60, 20, -16, 0, 28, -17, 28, 12, 49, -46, 13, 47, -7, 10, 19, 15, 19, -26, 8, -24, -22, 12, -5, 7, -28, -4, 32, 21, 38, 16, 16, -1, -15, -32, -32, 12, 9, 47, 9, -5, -60, -39, -35, -14, -9, -10, -65, 127, 93, -48, -118, 63, 58, -71, 21, -58, -32, 37, -21, 33, 1, 0, -21, -27, -17, 12, -17, 0, -50, 64, -39, 52, 11, 24, -11, 33, 0, 0, -4, -37, 23, -52, -11, 31, -8, 1, -30, -1, -8, 6, -33, 43, 34, 27, -36, 7, 39, 19, 8, 30, 11, -12, -33, 33, 103, -15, 0, 8, 28, -66, -45, -18, 69, 64, 27, -22, 27, 60, -35, -49, 12, -86, -29, 50, 51, 71, -25, 27, -50, 72, 11, -1, -59, 63, 13, 29, -29, 8, 32, 41, -13, 60, -39, 58, -27, 0, -5, 48, 16, 65, 36, -11, 18, 59, 18, -7, -19, -41, 25, 6, 9, -9, 0, -30, 18, 73, 2, -31, -74, -32, -43, -44, 38, -18, 25, -4, 31, 31, -66, 35, 21, -39, 20, -36, -41, -6, -71, -11, -76, -57, 16, -51, -20, 28, -60, 34, 7, -1, 102, 58, 46, 3, 32, -37, -4, 32, -26, -4, -27, -56, -9, -47, -59, 62, 1, 18, -26, 18, -28, -30, 26, 46, 13, -18, -19, 53, -32, -27, -48, 13, -39, -1, 15, -15, -11, 29, 33, -5, 25, -21, -4, -16, 27, -46, 4, -40, 51, 21, 13, 0, 14, -16, 56, -28, -68, -22, 60, 52, 20, -29, 52, -27, -25, -28, -33, 34, -30, 39, 15, -16, -30, 19, -44, 4, 5, -29, 14, 34, -38, -62, -54, -72, 8, 2, -98, -33, 71, 36, -85, 71, -68, 51, -7, 3, 2, -9, 59, 62, 19, 5, -17, 4, 19, 9, -21, -24, 10, -20, 68, 3, -18, 59, -11, -2, -4, 20, 17, -7, 35, -12, 30, 20, -48, 72, 73, 30, 15, 91, 35, 19, -13, 0, -43, 12, 43, 8, -32, -2, -17, 35, -1, 51, 2, 100, -37, 44, -112, 33, -64, 36, 20, -50, -63, 31, -75, -69, 11, 1, -68, -1, 34, -19, 3, 39, -12, 38, 7, -54, -8, 0, 26, 21, 6, -8, -9, 2, -16, 18, -37, -19, 76, -18, 16, -31, 112, -42, 53, -5, -9, -13, 55, 19, -8, -39, 61, -11, -13, 3, 56, 33, 30, 4, -25, -46, -14, -32, 13, 9, 11, 4, -46, 38, -9, 39, -47, 27, -1, 59, -49, -80, 49, -32, -17, -44, 14, -40, 3, 17, 32, 17, -23, -39, 32, 11, -38, -87, 118, -81, -52, -59, -1, -24, 39, 11, 10, 24, 16, 18, 8, 29, 0, 27, 7, -21, 7, -38, 8, -31, -14, -15, 27, -83, -18, -21, -28, -15, -6, 0, 22, 16, 0, -15, -34, 21, 62, 35, 2, 39, -13, -64, 30, -1, 31, 9, -5, 24, -84, -6, -14, -38, 15, -76, -39, -27, -21, -73, -64, 9, -84, 10, -46, -92, -29, 79, -8, -45, -3, 38, 32, 20, -13, -35, -4, 0, -24, -12, -17, 4, 2, 4, -11, -11, 12, 14, -41, 49, 1, 6, -37, 49, 5, -6, -31, 48, 82, 54, 8, -1, 8, 69, -13, -42, 12, 24, 16, 15, 8, -61, -4, -6, 54, -34, -20, 14, 11, 8, 21, 0, 8, 43, -14, 29, 54, 30, -38, -46, 15, 22, 8, -22, 23, -32, -7, 22, 15, -6, -3, -12, 5, 47, -16, -8, 35, -45, -31, -40, -71, 22, -41, 22, 7, -37, 6, 24, -62, 14, 19, 40, -34, 53, -26, -44, 27, -23, 8, 42, 34, -4, 39, -33, 64, -13, 27, 59, 8, 87, 61, 43, 16, 43, -3, 4, 33, -3, 10, -39, -20, -8, -71, 11, 5, 44, -8, 40, -44, 6, -2, 0, -44, 65, -10, -4, -34, -32, 13, -32, -19, -16, 10, 0, 15, 12, -1, 8, 51, -5, -20, -7, 29, 47, -39, -1, -3, 37, 14, -8, 86, -15, -15, -94, -7, -25, 6, 27, -29, 12, 10, 44, 4, -36, 9, -17, 108, 35, -72, -9, -56, -55, 54, 53, -28, 32, 37, -6, 5, 5, -1, 26, 39, 14, -29, -24, 54, 13, 0, -128, 95, 0, -38, 40, -90, -7, -46, -23, 36, 11, 30, 26, 25, -49, -101, 44, 20, 59, 12, -40, -77, -35, 27, -10, 32, 18, 2, -107, -50, -60, -19, -128, 4, 76, 9, -71, -35, 7, 5, -50, -6, 9, -101, -68, -1, -11, 2, -95, 19, -10, 34, -22, 48, 11, 8, 78, 34, 8, -58, -14, -31, 29, -35, 3, -39, -42, 4, -12, -11, -41, 1, -33, 24, 30, 70, 48, -37, -9, 127, 61, 127, -5, -19, 30, 4, -9, -29, 10, -57, -7, 5, -16, -17, -26, 9, 47, 34, -3, 2, 17, 11, 32, 1, 20, -31, -22, 24, 8, 51, 3, -25, 44, 41, 32, -75, 14, -25, -32, -45, -28, 41, 64, 37, 12, 18, -34, -42, -59, -10, 40, 46, 4, -1, 0, -31, 20, -15, -56, 18, -19, 16, 51, -36, 44, -61, 15, 9, 1, -22, -14, -12, 4, -21, 13, 9, 31, 12, 10, 62, -45, 35, 23, 3, 0, -19, 0, 10, -10, -44, 35, -10, -89, -48, 41, 12, -32, -7, -27, -13, -23, -67, -34, -6, -16, -33, 16]]\n\tuint8: None\n\tbinary: [[-110, 121, 110, -50, 87, -59, 8, 35, 114, 30, -92, -112, -118, -16, 7, 96, 17, 19, 97, -9, -23, 25, -103, -35, -78, -45, 72, -123, -41, 67, 14, -31, -42, -126, 75, 111, 62, -64, 57, 64, -52, -66, -64, -12, 100, 99, 87, 61, -5, 5, 23, 34, -75, -66, -16, 91, 92, 121, 55, 117, 100, -112, -24, 84, 84, -65, 61, -31, -45, 7, 44, 8, -35, -125, 16, -50, -52, 11, -105, -32, 102, -62, -3, 86, -107, 21, 95, 15, 27, -79, -20, 114, 90, 125, 110, -97, -15, -98, 21, -102, -124, 112, -115, 26, -86, -55, 67, 7, 11, -127, 125, 103, -46, -55, 79, -31, 126, -32, 33, -128, -124, -80, 21, 27, -49, -9, 112, 103], [-110, -7, -24, 23, -33, 68, 24, 35, 22, -50, -32, 86, 74, -14, 71, 96, 81, -45, 105, -25, -73, 108, -99, 13, -76, 125, 73, -44, -34, -34, -105, 75, 86, -58, 85, -30, -92, -27, -39, 0, -91, -2, 30, -12, -116, 9, 81, 39, 76, 44, 87, 20, -107, 110, -75, 20, 44, 125, -75, 85, -28, -118, -24, 127, 78, -75, 108, -20, -48, 3, 12, 12, 71, -29, -98, -26, 68, 11, 0, -104, 96, 70, -3, 53, -98, -108, 127, -102, -17, -84, -88, 88, -54, -45, -11, -4, -4, 15, -67, 122, -108, 117, -115, 40, 98, -47, 102, -103, 3, -123, -85, 119, -48, -24, 95, -34, -26, -24, -31, -9, 99, 64, -128, -43, 74, -91, 80, -95], [64, -14, -4, 30, 118, 5, 8, 35, 51, 3, 72, -122, -70, -10, 2, -20, 17, 115, -67, -11, 115, 31, -103, -73, -78, 65, 64, -123, -41, 91, 14, -39, -41, -78, 73, -62, 60, -28, 89, 32, 33, -35, -62, 116, 102, -45, 83, 63, 73, 37, 23, 64, -43, -46, -106, 83, 109, 92, -87, -15, -60, -39, -23, 63, 84, 56, -6, -15, 20, 3, 76, 3, 104, -16, -79, 70, -123, 15, -125, -111, 109, -105, -99, 82, -19, -27, 95, -113, 94, -74, 57, 82, -102, -7, -95, -21, -3, -66, 73, 95, -124, 37, -115, -81, 107, -55, -25, 6, 19, -107, -120, 111, -110, -23, 79, -26, 106, -61, -96, -77, 9, 116, -115, -67, -63, -9, -43, 77], [-109, -7, -32, 19, 87, 116, 8, 35, 54, -102, -64, -106, -14, -10, 31, 78, -99, 59, -6, -45, 97, 96, -103, 37, 69, -35, 9, -59, 95, 25, 14, 73, 86, -9, -43, 110, -70, 96, 45, 32, -91, 62, -64, -12, 100, -55, 34, 62, 14, 5, 22, 67, -75, -17, -14, 81, 45, 125, -15, -11, -28, 75, -25, 20, 42, -78, -4, -67, -44, 11, 76, 3, 127, 40, 0, 103, 75, -62, -123, -111, 68, -13, -10, -5, -66, -89, 119, -70, -29, -95, -19, 82, 106, 127, -24, -11, -48, 15, -29, -102, -115, 107, -115, 55, -69, -61, 103, 11, 3, 25, -118, 63, -108, 11, 78, -28, 14, 124, 119, -61, 97, 84, 53, 69, 123, 89, -104, -127]]\n\tubinary: None\n}\n\tmeta: {'api_version': {'version': '1'}, 'billed_units': {'input_tokens': 106}}\n}\n</pre> <p>As we can see from the above, we got multiple vector representations for the same input strings.</p> In\u00a0[54]: Copied! <pre>print(\n    \"int8 dimensionality: {}, binary dimensionality {}\".format(\n        len(embeddings.embeddings.int8[0]), len(embeddings.embeddings.binary[0])\n    )\n)\n</pre> print(     \"int8 dimensionality: {}, binary dimensionality {}\".format(         len(embeddings.embeddings.int8[0]), len(embeddings.embeddings.binary[0])     ) ) <pre>int8 dimensionality: 1024, binary dimensionality 128\n</pre> In\u00a0[8]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\nmy_schema = Schema(\n    name=\"doc\",\n    mode=\"index\",\n    document=Document(\n        fields=[\n            Field(name=\"doc_id\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"binary_vector\",\n                type=\"tensor&lt;int8&gt;(x[128])\",\n                indexing=[\"attribute\", \"index\"],\n                attribute=[\"distance-metric: hamming\"],\n            ),\n            Field(\n                name=\"int8_vector\",\n                type=\"tensor&lt;int8&gt;(x[1024])\",\n                indexing=[\"attribute\"],\n                attribute=[\"paged\"],\n            ),\n        ]\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  my_schema = Schema(     name=\"doc\",     mode=\"index\",     document=Document(         fields=[             Field(name=\"doc_id\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"binary_vector\",                 type=\"tensor(x[128])\",                 indexing=[\"attribute\", \"index\"],                 attribute=[\"distance-metric: hamming\"],             ),             Field(                 name=\"int8_vector\",                 type=\"tensor(x[1024])\",                 indexing=[\"attribute\"],                 attribute=[\"paged\"],             ),         ]     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])], ) <p>We must add the schema to a Vespa application package. This consists of configuration files, schemas, models, and possibly even custom code (plugins).</p> In\u00a0[9]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"coherebillion\"\nvespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema])\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"coherebillion\" vespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema]) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p><code>unpack_bits</code>, unpacks the binary representation into a 1024-dimensional float vector doc.</p> <p>We define three tensor inputs that we intend to send with the query request.</p> In\u00a0[10]: Copied! <pre>from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function\n\n\nrerank = RankProfile(\n    name=\"rerank\",\n    inputs=[\n        (\"query(q_binary)\", \"tensor&lt;int8&gt;(x[128])\"),\n        (\"query(q_full)\", \"tensor&lt;float&gt;(x[1024])\"),\n        (\"query(q_int8)\", \"tensor&lt;int8&gt;(x[1024])\"),\n    ],\n    functions=[\n        Function(  # this returns a tensor&lt;float&gt;(x[1024]) with values -1 or 1\n            name=\"unpack_binary_representation\",\n            expression=\"2*unpack_bits(attribute(binary_vector)) -1\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(\n        expression=\"sum(query(q_full)*unpack_binary_representation )\"  # phase 1 ranking using the float query and the unpacked float version of the binary_vector\n    ),\n    second_phase=SecondPhaseRanking(\n        expression=\"cosine_similarity(query(q_int8),attribute(int8_vector),x)\",  # phase 2 using the int8 vector representations\n        rerank_count=30,  # number of hits to rerank, upper bound on number of random IO operations\n    ),\n    match_features=[\n        \"distance(field, binary_vector)\",\n        \"closeness(field, binary_vector)\",\n        \"firstPhase\",\n    ],\n)\nmy_schema.add_rank_profile(rerank)\n</pre> from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function   rerank = RankProfile(     name=\"rerank\",     inputs=[         (\"query(q_binary)\", \"tensor(x[128])\"),         (\"query(q_full)\", \"tensor(x[1024])\"),         (\"query(q_int8)\", \"tensor(x[1024])\"),     ],     functions=[         Function(  # this returns a tensor(x[1024]) with values -1 or 1             name=\"unpack_binary_representation\",             expression=\"2*unpack_bits(attribute(binary_vector)) -1\",         )     ],     first_phase=FirstPhaseRanking(         expression=\"sum(query(q_full)*unpack_binary_representation )\"  # phase 1 ranking using the float query and the unpacked float version of the binary_vector     ),     second_phase=SecondPhaseRanking(         expression=\"cosine_similarity(query(q_int8),attribute(int8_vector),x)\",  # phase 2 using the int8 vector representations         rerank_count=30,  # number of hits to rerank, upper bound on number of random IO operations     ),     match_features=[         \"distance(field, binary_vector)\",         \"closeness(field, binary_vector)\",         \"firstPhase\",     ], ) my_schema.add_rank_profile(rerank) In\u00a0[39]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() In\u00a0[41]: Copied! <pre>for i, doc in enumerate(documents):\n    response = app.feed_data_point(\n        schema=\"doc\",\n        data_id=str(i),\n        fields={\n            \"doc_id\": str(i),\n            \"text\": doc,\n            \"binary_vector\": embeddings.embeddings.binary[i],\n            \"int8_vector\": embeddings.embeddings.int8[i],\n        },\n    )\n    assert response.is_successful()\n</pre> for i, doc in enumerate(documents):     response = app.feed_data_point(         schema=\"doc\",         data_id=str(i),         fields={             \"doc_id\": str(i),             \"text\": doc,             \"binary_vector\": embeddings.embeddings.binary[i],             \"int8_vector\": embeddings.embeddings.int8[i],         },     )     assert response.is_successful() In\u00a0[30]: Copied! <pre>query = \"Who discovered x-ray?\"\n\n# Make sure to set input_type=\"search_query\" when getting the embeddings for the query.\n# We ask for 3 versions (float, binary, and int8) of the embeddings.\nquery_emb = co.embed(\n    [query],\n    model=\"embed-english-v3.0\",\n    input_type=\"search_query\",\n    embedding_types=[\"float\", \"binary\", \"int8\"],\n)\n</pre> query = \"Who discovered x-ray?\"  # Make sure to set input_type=\"search_query\" when getting the embeddings for the query. # We ask for 3 versions (float, binary, and int8) of the embeddings. query_emb = co.embed(     [query],     model=\"embed-english-v3.0\",     input_type=\"search_query\",     embedding_types=[\"float\", \"binary\", \"int8\"], ) In\u00a0[\u00a0]: Copied! <pre>print(query_emb)\n</pre> print(query_emb) <p>Now, we use Vespa's nearestNeighbor query operator to expose up to 1000 hits to ranking using the configured distance-metric (hamming distance).</p> <p>This is the retrieve logic, or phase-0 search as it only uses the hamming distance. See phased ranking for more on phased ranking pipelines.</p> <p>The hits that are near in hamming space, are exposed to the flexibility of the Vespa ranking framework:</p> <ul> <li>the first-phase uses the unpacked version of the binary vector and computes the dot product against the float query version</li> <li>The second phase and final phase re-ranks the 30 best from the the previous phase, here using cosine similarity between the int8 embedding representations</li> </ul> In\u00a0[47]: Copied! <pre>response = app.query(\n    yql=\"select * from doc where {targetHits:1000}nearestNeighbor(binary_vector,q_binary)\",\n    ranking=\"rerank\",\n    body={\n        \"input.query(q_binary)\": query_emb.embeddings.binary[0],\n        \"input.query(q_full)\": query_emb.embeddings.float[0],\n        \"input.query(q_int8)\": query_emb.embeddings.int8[0],\n    },\n)\nassert response.is_successful()\n</pre> response = app.query(     yql=\"select * from doc where {targetHits:1000}nearestNeighbor(binary_vector,q_binary)\",     ranking=\"rerank\",     body={         \"input.query(q_binary)\": query_emb.embeddings.binary[0],         \"input.query(q_full)\": query_emb.embeddings.float[0],         \"input.query(q_int8)\": query_emb.embeddings.int8[0],     }, ) assert response.is_successful() In\u00a0[48]: Copied! <pre>response.hits\n</pre> response.hits Out[48]: <pre>[{'id': 'id:doc:doc::3',\n  'relevance': 0.45650564242263414,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.0030303030303030303,\n    'distance(field,binary_vector)': 329.0,\n    'firstPhase': 4.905200004577637},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::3',\n   'doc_id': '3',\n   'text': 'Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity'}},\n {'id': 'id:doc:doc::1',\n  'relevance': 0.337421116422118,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.002544529262086514,\n    'distance(field,binary_vector)': 391.99999999999994,\n    'firstPhase': 3.7868080139160156},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::1',\n   'doc_id': '1',\n   'text': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.'}},\n {'id': 'id:doc:doc::2',\n  'relevance': 0.280400768492745,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.0026595744680851063,\n    'distance(field,binary_vector)': 375.0,\n    'firstPhase': 3.854860305786133},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::2',\n   'doc_id': '2',\n   'text': 'Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.'}},\n {'id': 'id:doc:doc::0',\n  'relevance': 0.2570603626828106,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.0024390243902439024,\n    'distance(field,binary_vector)': 409.0,\n    'firstPhase': 2.845644474029541},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::0',\n   'doc_id': '0',\n   'text': 'Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.'}}]</pre> <p>The <code>relevance</code> is the cosine similarity between the int8 vector representations calculated in the second-phase. Note also that we return the <code>hamming</code> distance and the firstPhase score which is the query, unpacked binary dot product.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#billion-scale-vector-search-with-cohere-binary-embeddings-in-vespa","title":"Billion-scale vector search with Cohere binary embeddings in Vespa\u00b6","text":"<p>Cohere just released a new embedding API with support for binary and <code>int8</code> vectors. Read the announcement in the blog post: Cohere int8 &amp; binary Embeddings - Scale Your Vector Database to Large Datasets.</p> <p>We are excited to announce that Cohere Embed is the first embedding model that natively supports int8 and binary embeddings.</p> <p>This is huge because:</p> <ul> <li>Binarization reduces the storage (disk/memory) footprint from 1024 floats (4096 bytes) per vector to 128 bytes.</li> <li>Faster distance calculations using hamming distance that Vespa natively supports for bits packed into <code>int8</code> tensor cells. More on hamming distance in Vespa.</li> <li>Multiple vector representations allow for coarse retrieval in hamming space and subsequent phases using higher-resolution representations.</li> <li>Drastically reduces the deployment due to tiered storage economics.</li> </ul> <p>Vespa supports <code>hamming</code> distance with and without HNSW indexing.</p> <p>For those wanting to learn more about binary vectors, we recommend our 2021 blog series on Billion-scale vector search with Vespa and Billion-scale vector search with Vespa - part two.</p> <p>This notebook demonstrates using the Cohere embeddings with a coarse-to-fine search and re-ranking pipeline that reduces costs, but offers the same retrieval (nDCG) accuracy.</p> <ul> <li>The packed binary vector representation is stored in memory, with an optional HNSW index using hamming distance.</li> <li>The <code>int8</code> vector representation is stored on disk using Vespa's paged option.</li> </ul> <p>At query time:</p> <ul> <li>Retrieve in hamming space (1000 candidates) as the coarse-level search using the compact binary representation.</li> <li>Re-rank by using a dot product between the float version of the query vector (1024 dims) against an unpacked float version of the binary embedding (also 1024 dims)</li> <li>A re-ranking phase using the 1024 dimensional int8 representations. This stage pages the vector data from the disk using Vespa's paged option (unless it is already cached).</li> </ul> <p></p> <p>Install the dependencies:</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#examining-the-cohere-embeddings","title":"Examining the Cohere embeddings\u00b6","text":"<p>Let us check out the Cohere embedding API and how we can obtain vector embeddings with different precisions for the same text input (without additional cost). See also Cohere embed API doc.</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#some-sample-documents","title":"Some sample documents\u00b6","text":"<p>Define a few sample documents that we want to embed</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>First, we define a Vespa schema with the fields we want to store and their type.</p> <p>Notice the <code>binary_vector</code> field that defines an indexed (dense) Vespa tensor with the dimension name <code>x[128]</code>.</p> <p>Indexing specifies <code>index</code> which means that Vespa will build HNSW graph for searching this vector field.</p> <p>Also, notice the configuration of the distance-metric.</p> <p>We also want to store the <code>int8_vector</code> on disk; we use <code>paged</code> to signalize this.</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#feed-our-sample-documents-and-their-binary-embedding-representation","title":"Feed our sample documents and their binary embedding representation\u00b6","text":"<p>With few documents, we use the synchronous API. Read more in reads and writes.</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> <li>Practical Nearest Neighbor Search Guide</li> </ul> <p>We now need to invoke the embed API again to embed the query text; we ask for all three representations:</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#conclusions","title":"Conclusions\u00b6","text":"<p>These new Cohere binary embeddings are a huge step forward for cost-efficient vector search at scale and integrate perfectly with Vespa features for building out vector search at scale.</p> <p>Storing the <code>int8</code> vector representation on disk using the paged attribute option enables phased retrieval and ranking close to the data. First, one can use the compact in-memory binary representation for the coarse-level search to efficiently find a limited number of candidates. Then, the candidates from the coarse search can be re-scored and re-ranked using a more advanced scoring function using a finer resolution.</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#clean-up","title":"Clean up\u00b6","text":"<p>We can now delete the cloud instance:</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html","title":"chat with your pdfs using colbert langchain and Vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa langchain langchain-community langchain-openai pypdf==5.0.1 openai vespacli\n</pre> !pip3 install -U pyvespa langchain langchain-community langchain-openai pypdf==5.0.1 openai vespacli In\u00a0[2]: Copied! <pre>def sample_pdfs():\n    return [\n        {\n            \"title\": \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\",\n            \"url\": \"https://arxiv.org/pdf/2112.01488.pdf\",\n            \"authors\": \"Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia\",\n        },\n        {\n            \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",\n            \"url\": \"https://arxiv.org/pdf/2004.12832.pdf\",\n            \"authors\": \"Omar Khattab, Matei Zaharia\",\n        },\n        {\n            \"title\": \"On Approximate Nearest Neighbour Selection for Multi-Stage Dense Retrieval\",\n            \"url\": \"https://arxiv.org/pdf/2108.11480.pdf\",\n            \"authors\": \"Craig Macdonald, Nicola Tonellotto\",\n        },\n        {\n            \"title\": \"A Study on Token Pruning for ColBERT\",\n            \"url\": \"https://arxiv.org/pdf/2112.06540.pdf\",\n            \"authors\": \"Carlos Lassance, Maroua Maachou, Joohee Park, St\u00e9phane Clinchant\",\n        },\n        {\n            \"title\": \"Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval\",\n            \"url\": \"https://arxiv.org/pdf/2106.11251.pdf\",\n            \"authors\": \"Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis\",\n        },\n    ]\n</pre> def sample_pdfs():     return [         {             \"title\": \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\",             \"url\": \"https://arxiv.org/pdf/2112.01488.pdf\",             \"authors\": \"Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia\",         },         {             \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",             \"url\": \"https://arxiv.org/pdf/2004.12832.pdf\",             \"authors\": \"Omar Khattab, Matei Zaharia\",         },         {             \"title\": \"On Approximate Nearest Neighbour Selection for Multi-Stage Dense Retrieval\",             \"url\": \"https://arxiv.org/pdf/2108.11480.pdf\",             \"authors\": \"Craig Macdonald, Nicola Tonellotto\",         },         {             \"title\": \"A Study on Token Pruning for ColBERT\",             \"url\": \"https://arxiv.org/pdf/2112.06540.pdf\",             \"authors\": \"Carlos Lassance, Maroua Maachou, Joohee Park, St\u00e9phane Clinchant\",         },         {             \"title\": \"Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval\",             \"url\": \"https://arxiv.org/pdf/2106.11251.pdf\",             \"authors\": \"Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis\",         },     ] In\u00a0[3]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\npdf_schema = Schema(\n    name=\"pdf\",\n    mode=\"streaming\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n            Field(name=\"title\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"authors\", type=\"array&lt;string&gt;\", indexing=[\"summary\", \"index\"]),\n            Field(\n                name=\"metadata\",\n                type=\"map&lt;string,string&gt;\",\n                indexing=[\"summary\", \"index\"],\n            ),\n            Field(name=\"page\", type=\"int\", indexing=[\"summary\", \"attribute\"]),\n            Field(name=\"contexts\", type=\"array&lt;string&gt;\", indexing=[\"summary\", \"index\"]),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;bfloat16&gt;(context{}, x[384])\",\n                indexing=[\n                    \"input contexts\",\n                    'for_each { (input title || \"\") . \" \" . ( _ || \"\") }',\n                    \"embed e5\",\n                    \"attribute\",\n                ],\n                attribute=[\"distance-metric: angular\"],\n                is_document_field=False,\n            ),\n            Field(\n                name=\"colbert\",\n                type=\"tensor&lt;int8&gt;(context{}, token{}, v[16])\",\n                indexing=[\"input contexts\", \"embed colbert context\", \"attribute\"],\n                is_document_field=False,\n            ),\n        ],\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"contexts\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  pdf_schema = Schema(     name=\"pdf\",     mode=\"streaming\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),             Field(name=\"title\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"authors\", type=\"array\", indexing=[\"summary\", \"index\"]),             Field(                 name=\"metadata\",                 type=\"map\",                 indexing=[\"summary\", \"index\"],             ),             Field(name=\"page\", type=\"int\", indexing=[\"summary\", \"attribute\"]),             Field(name=\"contexts\", type=\"array\", indexing=[\"summary\", \"index\"]),             Field(                 name=\"embedding\",                 type=\"tensor(context{}, x[384])\",                 indexing=[                     \"input contexts\",                     'for_each { (input title || \"\") . \" \" . ( _ || \"\") }',                     \"embed e5\",                     \"attribute\",                 ],                 attribute=[\"distance-metric: angular\"],                 is_document_field=False,             ),             Field(                 name=\"colbert\",                 type=\"tensor(context{}, token{}, v[16])\",                 indexing=[\"input contexts\", \"embed colbert context\", \"attribute\"],                 is_document_field=False,             ),         ],     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"contexts\"])], ) <p>The above defines our <code>pdf</code> schema using mode <code>streaming</code>. Most fields are straightforward, but take a note of:</p> <ul> <li><code>metadata</code> using <code>map&lt;string,string&gt;</code> - here we can store and match over page level metadata extracted by the PDF parser.</li> <li><code>contexts</code> using <code>array&lt;string&gt;</code>, these are the context-sized text parts that we use langchain document transformers for.</li> <li>The <code>embedding</code> field of type <code>tensor&lt;bfloat16&gt;(context{},x[384])</code> allows us to store and search the 384-dimensional embeddings per context in the same document</li> <li>The <code>colbert</code> field of type <code>tensor&lt;int8&gt;(context{}, token{}, v[16])</code> stores the ColBERT embeddings, retaining a (quantized) per-token representation of the text.</li> </ul> <p>The observant reader might have noticed the <code>e5</code> and <code>colbert</code> arguments to the <code>embed</code> expression in the above <code>embedding</code> field. The <code>e5</code> argument references a component of the type hugging-face-embedder, and <code>colbert</code> references the new cobert-embedder. We configure the application package and its name with the <code>pdf</code> schema and the <code>e5</code> and <code>colbert</code> embedder components.</p> In\u00a0[4]: Copied! <pre>from vespa.package import ApplicationPackage, Component, Parameter\n\nvespa_app_name = \"pdfs\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name,\n    schema=[pdf_schema],\n    components=[\n        Component(\n            id=\"e5\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    name=\"transformer-model\",\n                    args={\n                        \"url\": \"https://huggingface.co/intfloat/e5-small-v2/resolve/main/model.onnx\"\n                    },\n                ),\n                Parameter(\n                    name=\"tokenizer-model\",\n                    args={\n                        \"url\": \"https://huggingface.co/intfloat/e5-small-v2/raw/main/tokenizer.json\"\n                    },\n                ),\n                Parameter(\n                    name=\"prepend\",\n                    args={},\n                    children=[\n                        Parameter(name=\"query\", args={}, children=\"query: \"),\n                        Parameter(name=\"document\", args={}, children=\"passage: \"),\n                    ],\n                ),\n            ],\n        ),\n        Component(\n            id=\"colbert\",\n            type=\"colbert-embedder\",\n            parameters=[\n                Parameter(\n                    name=\"transformer-model\",\n                    args={\n                        \"url\": \"https://huggingface.co/colbert-ir/colbertv2.0/resolve/main/model.onnx\"\n                    },\n                ),\n                Parameter(\n                    name=\"tokenizer-model\",\n                    args={\n                        \"url\": \"https://huggingface.co/colbert-ir/colbertv2.0/raw/main/tokenizer.json\"\n                    },\n                ),\n            ],\n        ),\n    ],\n)\n</pre> from vespa.package import ApplicationPackage, Component, Parameter  vespa_app_name = \"pdfs\" vespa_application_package = ApplicationPackage(     name=vespa_app_name,     schema=[pdf_schema],     components=[         Component(             id=\"e5\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(                     name=\"transformer-model\",                     args={                         \"url\": \"https://huggingface.co/intfloat/e5-small-v2/resolve/main/model.onnx\"                     },                 ),                 Parameter(                     name=\"tokenizer-model\",                     args={                         \"url\": \"https://huggingface.co/intfloat/e5-small-v2/raw/main/tokenizer.json\"                     },                 ),                 Parameter(                     name=\"prepend\",                     args={},                     children=[                         Parameter(name=\"query\", args={}, children=\"query: \"),                         Parameter(name=\"document\", args={}, children=\"passage: \"),                     ],                 ),             ],         ),         Component(             id=\"colbert\",             type=\"colbert-embedder\",             parameters=[                 Parameter(                     name=\"transformer-model\",                     args={                         \"url\": \"https://huggingface.co/colbert-ir/colbertv2.0/resolve/main/model.onnx\"                     },                 ),                 Parameter(                     name=\"tokenizer-model\",                     args={                         \"url\": \"https://huggingface.co/colbert-ir/colbertv2.0/raw/main/tokenizer.json\"                     },                 ),             ],         ),     ], ) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p>Vespa supports phased ranking and has a rich set of built-in rank-features, including many text-matching features such as:</p> <ul> <li>BM25.</li> <li>nativeRank and many more.</li> </ul> <p>Users can also define custom functions using ranking expressions. The following defines a <code>colbert</code> Vespa ranking profile which uses the <code>e5</code> embedding in the first phase, and the <code>max_sim</code> function in the second phase. The <code>max_sim</code> function performs the late interaction for the ColBERT ranking, and is by default applied to the top 100 documents from the first phase.</p> In\u00a0[5]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\ncolbert = RankProfile(\n    name=\"colbert\",\n    inputs=[\n        (\"query(q)\", \"tensor&lt;float&gt;(x[384])\"),\n        (\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\"),\n    ],\n    functions=[\n        Function(name=\"cos_sim\", expression=\"closeness(field, embedding)\"),\n        Function(\n            name=\"max_sim_per_context\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(colbert)) , v\n                        ),\n                        max, token\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(\n            name=\"max_sim\", expression=\"reduce(max_sim_per_context, max, context)\"\n        ),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"cos_sim\"),\n    second_phase=SecondPhaseRanking(expression=\"max_sim\"),\n    match_features=[\"cos_sim\", \"max_sim\", \"max_sim_per_context\"],\n)\npdf_schema.add_rank_profile(colbert)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  colbert = RankProfile(     name=\"colbert\",     inputs=[         (\"query(q)\", \"tensor(x[384])\"),         (\"query(qt)\", \"tensor(querytoken{}, v[128])\"),     ],     functions=[         Function(name=\"cos_sim\", expression=\"closeness(field, embedding)\"),         Function(             name=\"max_sim_per_context\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(colbert)) , v                         ),                         max, token                     ),                     querytoken                 )             \"\"\",         ),         Function(             name=\"max_sim\", expression=\"reduce(max_sim_per_context, max, context)\"         ),     ],     first_phase=FirstPhaseRanking(expression=\"cos_sim\"),     second_phase=SecondPhaseRanking(expression=\"max_sim\"),     match_features=[\"cos_sim\", \"max_sim\", \"max_sim_per_context\"], ) pdf_schema.add_rank_profile(colbert) <p>Using match-features, Vespa returns selected features along with the highest scoring documents. Here, we include <code>max_sim_per_context</code> which we can later use to select the top N scoring contexts for each page.</p> <p>For an example of a <code>hybrid</code> rank-profile which combines semantic search with traditional text retrieval such as BM25, see the previous blog post: Turbocharge RAG with LangChain and Vespa Streaming Mode for Sharded Data</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[12]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 1 of dev-aws-us-east-1c for vespa-team.pdfs. This may take a few minutes the first time.\nINFO    [19:04:30]  Deploying platform version 8.314.57 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [19:04:30]  Using CA signed certificate version 1\nINFO    [19:04:30]  Using 1 nodes in container cluster 'pdfs_container'\nINFO    [19:04:35]  Session 285265 for tenant 'vespa-team' prepared and activated.\nINFO    [19:04:39]  ######## Details for all nodes ########\nINFO    [19:04:44]  h88969d.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [19:04:44]  --- platform vespa/cloud-tenant-rhel8:8.314.57 &lt;-- :\nINFO    [19:04:44]  --- container-clustercontroller on port 19050 has not started \nINFO    [19:04:44]  --- metricsproxy-container on port 19092 has not started \nINFO    [19:04:44]  h88978a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [19:04:44]  --- platform vespa/cloud-tenant-rhel8:8.314.57 &lt;-- :\nINFO    [19:04:44]  --- logserver-container on port 4080 has not started \nINFO    [19:04:44]  --- metricsproxy-container on port 19092 has not started \nINFO    [19:04:44]  h90615b.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [19:04:44]  --- platform vespa/cloud-tenant-rhel8:8.314.57 &lt;-- :\nINFO    [19:04:44]  --- storagenode on port 19102 has not started \nINFO    [19:04:44]  --- searchnode on port 19107 has not started \nINFO    [19:04:44]  --- distributor on port 19111 has not started \nINFO    [19:04:44]  --- metricsproxy-container on port 19092 has not started \nINFO    [19:04:44]  h91135a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [19:04:44]  --- platform vespa/cloud-tenant-rhel8:8.314.57 &lt;-- :\nINFO    [19:04:44]  --- container on port 4080 has not started \nINFO    [19:04:44]  --- metricsproxy-container on port 19092 has not started \nINFO    [19:05:52]  Waiting for convergence of 10 services across 4 nodes\nINFO    [19:05:52]  1/1 nodes upgrading platform\nINFO    [19:05:52]  1 application services still deploying\nDEBUG   [19:05:52]  h91135a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nDEBUG   [19:05:52]  --- platform vespa/cloud-tenant-rhel8:8.314.57 &lt;-- :\nDEBUG   [19:05:52]  --- container on port 4080 has not started \nDEBUG   [19:05:52]  --- metricsproxy-container on port 19092 has config generation 285265, wanted is 285265\nINFO    [19:06:21]  Found endpoints:\nINFO    [19:06:21]  - dev.aws-us-east-1c\nINFO    [19:06:21]   |-- https://bac3e5ad.c81e7b13.z.vespa-app.cloud/ (cluster 'pdfs_container')\nINFO    [19:06:22]  Installation succeeded!\nUsing mTLS (key,cert) Authentication against endpoint https://bac3e5ad.c81e7b13.z.vespa-app.cloud//ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[13]: Copied! <pre>from langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1024,  # chars, not llm tokens\n    chunk_overlap=0,\n    length_function=len,\n    is_separator_regex=False,\n)\n</pre> from langchain_community.document_loaders import PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter  text_splitter = RecursiveCharacterTextSplitter(     chunk_size=1024,  # chars, not llm tokens     chunk_overlap=0,     length_function=len,     is_separator_regex=False, ) <p>The following iterates over the <code>sample_pdfs</code> and performs the following:</p> <ul> <li>Load the URL and extract the text into pages. A page is the retrievable unit we will use in Vespa</li> <li>For each page, use the text splitter to split the text into contexts. The contexts are represented as an <code>array&lt;string&gt;</code> in the Vespa schema</li> <li>Create the page level Vespa <code>fields</code>, note that we duplicate some content like the title and URL into the page level representation.</li> </ul> In\u00a0[14]: Copied! <pre>import hashlib\nimport unicodedata\n\n\ndef remove_control_characters(s):\n    return \"\".join(ch for ch in s if unicodedata.category(ch)[0] != \"C\")\n\n\nmy_docs_to_feed = []\nfor pdf in sample_pdfs():\n    url = pdf[\"url\"]\n    loader = PyPDFLoader(url)\n    pages = loader.load_and_split()\n    for index, page in enumerate(pages):\n        source = page.metadata[\"source\"]\n        chunks = text_splitter.transform_documents([page])\n        text_chunks = [chunk.page_content for chunk in chunks]\n        text_chunks = [remove_control_characters(chunk) for chunk in text_chunks]\n        page_number = index + 1\n        vespa_id = f\"{url}#{page_number}\"\n        hash_value = hashlib.sha1(vespa_id.encode()).hexdigest()\n        fields = {\n            \"title\": pdf[\"title\"],\n            \"url\": url,\n            \"page\": page_number,\n            \"id\": hash_value,\n            \"authors\": [a.strip() for a in pdf[\"authors\"].split(\",\")],\n            \"contexts\": text_chunks,\n            \"metadata\": page.metadata,\n        }\n        my_docs_to_feed.append(fields)\n</pre> import hashlib import unicodedata   def remove_control_characters(s):     return \"\".join(ch for ch in s if unicodedata.category(ch)[0] != \"C\")   my_docs_to_feed = [] for pdf in sample_pdfs():     url = pdf[\"url\"]     loader = PyPDFLoader(url)     pages = loader.load_and_split()     for index, page in enumerate(pages):         source = page.metadata[\"source\"]         chunks = text_splitter.transform_documents([page])         text_chunks = [chunk.page_content for chunk in chunks]         text_chunks = [remove_control_characters(chunk) for chunk in text_chunks]         page_number = index + 1         vespa_id = f\"{url}#{page_number}\"         hash_value = hashlib.sha1(vespa_id.encode()).hexdigest()         fields = {             \"title\": pdf[\"title\"],             \"url\": url,             \"page\": page_number,             \"id\": hash_value,             \"authors\": [a.strip() for a in pdf[\"authors\"].split(\",\")],             \"contexts\": text_chunks,             \"metadata\": page.metadata,         }         my_docs_to_feed.append(fields) <p>Now that we have parsed the input PDFs and created a list of pages that we want to add to Vespa, we must format the list into the format that PyVespa accepts. Notice the <code>fields</code>, <code>id</code> and <code>groupname</code> keys. The <code>groupname</code> is the key that is used to shard and co-locate the data and is only relevant when using Vespa with streaming mode.</p> In\u00a0[15]: Copied! <pre>from typing import Iterable\n\n\ndef vespa_feed(user: str) -&gt; Iterable[dict]:\n    for doc in my_docs_to_feed:\n        yield {\"fields\": doc, \"id\": doc[\"id\"], \"groupname\": user}\n</pre> from typing import Iterable   def vespa_feed(user: str) -&gt; Iterable[dict]:     for doc in my_docs_to_feed:         yield {\"fields\": doc, \"id\": doc[\"id\"], \"groupname\": user} In\u00a0[16]: Copied! <pre>my_docs_to_feed[0]\n</pre> my_docs_to_feed[0] Out[16]: <pre>{'title': 'ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction',\n 'url': 'https://arxiv.org/pdf/2112.01488.pdf',\n 'page': 1,\n 'id': 'a731a839198de04fa3d1a3cee6890d0d170ab025',\n 'authors': ['Keshav Santhanam',\n  'Omar Khattab',\n  'Jon Saad-Falcon',\n  'Christopher Potts',\n  'Matei Zaharia'],\n 'contexts': ['ColBERTv2:Effective and Ef\ufb01cient Retrieval via Lightweight Late InteractionKeshav Santhanam\u2217Stanford UniversityOmar Khattab\u2217Stanford UniversityJon Saad-FalconGeorgia Institute of TechnologyChristopher PottsStanford UniversityMatei ZahariaStanford UniversityAbstractNeural information retrieval (IR) has greatlyadvanced search and other knowledge-intensive language tasks. While many neuralIR methods encode queries and documentsinto single-vector representations, lateinteraction models produce multi-vector repre-sentations at the granularity of each token anddecompose relevance modeling into scalabletoken-level computations. This decompositionhas been shown to make late interaction moreeffective, but it in\ufb02ates the space footprint ofthese models by an order of magnitude. In thiswork, we introduce ColBERTv2, a retrieverthat couples an aggressive residual compres-sion mechanism with a denoised supervisionstrategy to simultaneously improve the quality',\n  'and space footprint of late interaction. Weevaluate ColBERTv2 across a wide rangeof benchmarks, establishing state-of-the-artquality within and outside the training domainwhile reducing the space footprint of lateinteraction models by 6\u201310 \u00d7.1 IntroductionNeural information retrieval (IR) has quickly domi-nated the search landscape over the past 2\u20133 years,dramatically advancing not only passage and doc-ument search (Nogueira and Cho, 2019) but alsomany knowledge-intensive NLP tasks like open-domain question answering (Guu et al., 2020),multi-hop claim veri\ufb01cation (Khattab et al., 2021a),and open-ended generation (Paranjape et al., 2022).Many neural IR methods follow a single-vectorsimilarity paradigm: a pretrained language modelis used to encode each query and each documentinto a single high-dimensional vector, and rele-vance is modeled as a simple dot product betweenboth vectors. An alternative is late interaction , in-troduced in ColBERT (Khattab and Zaharia, 2020),',\n  'where queries and documents are encoded at a \ufb01ner-granularity into multi-vector representations, and\u2217Equal contribution.relevance is estimated using rich yet scalable in-teractions between these two sets of vectors. Col-BERT produces an embedding for every token inthe query (and document) and models relevanceas the sum of maximum similarities between eachquery vector and all vectors in the document.By decomposing relevance modeling into token-level computations, late interaction aims to reducethe burden on the encoder: whereas single-vectormodels must capture complex query\u2013document re-lationships within one dot product, late interactionencodes meaning at the level of tokens and del-egates query\u2013document matching to the interac-tion mechanism. This added expressivity comesat a cost: existing late interaction systems imposean order-of-magnitude larger space footprint thansingle-vector models, as they must store billionsof small vectors for Web-scale collections. Con-',\n  'sidering this challenge, it might seem more fruit-ful to focus instead on addressing the fragility ofsingle-vector models (Menon et al., 2022) by in-troducing new supervision paradigms for negativemining (Xiong et al., 2020), pretraining (Gao andCallan, 2021), and distillation (Qu et al., 2021).Indeed, recent single-vector models with highly-tuned supervision strategies (Ren et al., 2021b; For-mal et al., 2021a) sometimes perform on-par oreven better than \u201cvanilla\u201d late interaction models,and it is not necessarily clear whether late inter-action architectures\u2014with their \ufb01xed token-levelinductive biases\u2014admit similarly large gains fromimproved supervision.In this work, we show that late interaction re-trievers naturally produce lightweight token rep-resentations that are amenable to ef\ufb01cient storageoff-the-shelf and that they can bene\ufb01t drasticallyfrom denoised supervision. We couple those inColBERTv2 ,1a new late-interaction retriever that'],\n 'metadata': {'source': 'https://arxiv.org/pdf/2112.01488.pdf', 'page': 0}}</pre> <p>Now, we can feed to the Vespa instance (<code>app</code>), using the <code>feed_iterable</code> API, using the generator function above as input with a custom <code>callback</code> function. Vespa also performs embedding inference during this step using the built-in Vespa embedding functionality.</p> In\u00a0[17]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Document {id} failed to feed with status code {response.status_code}, url={response.url} response={response.json}\"\n        )\n\n\napp.feed_iterable(\n    schema=\"pdf\", iter=vespa_feed(\"jo-bergum\"), namespace=\"personal\", callback=callback\n)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Document {id} failed to feed with status code {response.status_code}, url={response.url} response={response.json}\"         )   app.feed_iterable(     schema=\"pdf\", iter=vespa_feed(\"jo-bergum\"), namespace=\"personal\", callback=callback ) <p>Notice the <code>schema</code> and <code>namespace</code> arguments. PyVespa transforms the input operations to Vespa document v1 requests.</p> <p></p> In\u00a0[18]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select id,title,page,contexts from pdf where ({targetHits:10}nearestNeighbor(embedding,q))\",\n    groupname=\"jo-bergum\",\n    ranking=\"colbert\",\n    query=\"why is colbert effective?\",\n    body={\n        \"presentation.format.tensors\": \"short-value\",\n        \"input.query(q)\": 'embed(e5, \"why is colbert effective?\")',\n        \"input.query(qt)\": 'embed(colbert, \"why is colbert effective?\")',\n    },\n    timeout=\"2s\",\n)\nassert response.is_successful()\nprint(json.dumps(response.hits[0], indent=2))\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select id,title,page,contexts from pdf where ({targetHits:10}nearestNeighbor(embedding,q))\",     groupname=\"jo-bergum\",     ranking=\"colbert\",     query=\"why is colbert effective?\",     body={         \"presentation.format.tensors\": \"short-value\",         \"input.query(q)\": 'embed(e5, \"why is colbert effective?\")',         \"input.query(qt)\": 'embed(colbert, \"why is colbert effective?\")',     },     timeout=\"2s\", ) assert response.is_successful() print(json.dumps(response.hits[0], indent=2)) <pre>{\n  \"id\": \"id:personal:pdf:g=jo-bergum:55ea3f735cb6748a2eddb9f76d3f0e7fff0c31a8\",\n  \"relevance\": 103.17699432373047,\n  \"source\": \"pdfs_content.pdf\",\n  \"fields\": {\n    \"matchfeatures\": {\n      \"cos_sim\": 0.6534222205340683,\n      \"max_sim\": 103.17699432373047,\n      \"max_sim_per_context\": {\n        \"0\": 74.16375732421875,\n        \"1\": 103.17699432373047\n      }\n    },\n    \"id\": \"55ea3f735cb6748a2eddb9f76d3f0e7fff0c31a8\",\n    \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",\n    \"page\": 18,\n    \"contexts\": [\n      \"at least once. While ColBERT encodes each document with BERTexactly once, existing BERT-based rankers would repeat similarcomputations on possibly hundreds of documents for each query.Se/t_ting Dimension( m) Bytes/Dim Space(GiBs) MRR@10Re-rank Cosine 128 4 286 34.9End-to-end L2 128 2 154 36.0Re-rank L2 128 2 143 34.8Re-rank Cosine 48 4 54 34.4Re-rank Cosine 24 2 27 33.9Table 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.Table 4 reports the space footprint of ColBERT under variousse/t_tings as we reduce the embeddings dimension and/or the bytesper dimension. Interestingly, the most space-e\\ufb03cient se/t_ting, thatis, re-ranking with cosine similarity with 24-dimensional vectorsstored as 2-byte /f_loats, is only 1% worse in MRR@10 than the mostspace-consuming one, while the former requires only 27 GiBs torepresent the MS MARCO collection.5 CONCLUSIONSIn this paper, we introduced ColBERT, a novel ranking model thatemploys contextualized late interaction over deep LMs (in particular,\",\n      \"BERT) for e\\ufb03cient retrieval. By independently encoding queriesand documents into /f_ine-grained representations that interact viacheap and pruning-friendly computations, ColBERT can leveragethe expressiveness of deep LMs while greatly speeding up queryprocessing. In addition, doing so allows using ColBERT for end-to-end neural retrieval directly from a large document collection. Ourresults show that ColBERT is more than 170 \\u00d7faster and requires14,000\\u00d7fewer FLOPs/query than existing BERT-based models, allwhile only minimally impacting quality and while outperformingevery non-BERT baseline.Acknowledgments. OK was supported by the Eltoukhy FamilyGraduate Fellowship at the Stanford School of Engineering. /T_hisresearch was supported in part by a\\ufb03liate members and othersupporters of the Stanford DAWN project\\u2014Ant Financial, Facebook,Google, Infosys, NEC, and VMware\\u2014as well as Cisco, SAP, and the\"\n    ]\n  }\n}\n</pre> <p>Notice the <code>matchfeatures</code> that returns the configured match-features from the rank-profile, including all the context similarities.</p> In\u00a0[25]: Copied! <pre>from langchain_core.documents import Document\nfrom langchain_core.retrievers import BaseRetriever\nfrom typing import List\n\n\nclass VespaStreamingColBERTRetriever(BaseRetriever):\n    app: Vespa\n    user: str\n    pages: int = 5\n    chunks_per_page: int = 3\n    chunk_similarity_threshold: float = 0.8\n\n    def _get_relevant_documents(self, query: str) -&gt; List[Document]:\n        response: VespaQueryResponse = self.app.query(\n            yql=\"select id, url, title, page, authors, contexts from pdf where userQuery() or ({targetHits:20}nearestNeighbor(embedding,q))\",\n            groupname=self.user,\n            ranking=\"colbert\",\n            query=query,\n            hits=self.pages,\n            body={\n                \"presentation.format.tensors\": \"short-value\",\n                \"input.query(q)\": f'embed(e5, \"query: {query} \")',\n                \"input.query(qt)\": f'embed(colbert, \"{query}\")',\n            },\n            timeout=\"2s\",\n        )\n        if not response.is_successful():\n            raise ValueError(\n                f\"Query failed with status code {response.status_code}, url={response.url} response={response.json}\"\n            )\n        return self._parse_response(response)\n\n    def _parse_response(self, response: VespaQueryResponse) -&gt; List[Document]:\n        documents: List[Document] = []\n        for hit in response.hits:\n            fields = hit[\"fields\"]\n            chunks_with_scores = self._get_chunk_similarities(fields)\n            ## Best k chunks from each page\n            best_chunks_on_page = \" ### \".join(\n                [\n                    chunk\n                    for chunk, score in chunks_with_scores[0 : self.chunks_per_page]\n                    if score &gt; self.chunk_similarity_threshold\n                ]\n            )\n            documents.append(\n                Document(\n                    id=fields[\"id\"],\n                    page_content=best_chunks_on_page,\n                    title=fields[\"title\"],\n                    metadata={\n                        \"title\": fields[\"title\"],\n                        \"url\": fields[\"url\"],\n                        \"page\": fields[\"page\"],\n                        \"authors\": fields[\"authors\"],\n                        \"features\": fields[\"matchfeatures\"],\n                    },\n                )\n            )\n        return documents\n\n    def _get_chunk_similarities(self, hit_fields: dict) -&gt; List[tuple]:\n        match_features = hit_fields[\"matchfeatures\"]\n        similarities = match_features[\"max_sim_per_context\"]\n        chunk_scores = []\n        for i in range(0, len(similarities)):\n            chunk_scores.append(similarities.get(str(i), 0))\n        chunks = hit_fields[\"contexts\"]\n        chunks_with_scores = list(zip(chunks, chunk_scores))\n        return sorted(chunks_with_scores, key=lambda x: x[1], reverse=True)\n</pre> from langchain_core.documents import Document from langchain_core.retrievers import BaseRetriever from typing import List   class VespaStreamingColBERTRetriever(BaseRetriever):     app: Vespa     user: str     pages: int = 5     chunks_per_page: int = 3     chunk_similarity_threshold: float = 0.8      def _get_relevant_documents(self, query: str) -&gt; List[Document]:         response: VespaQueryResponse = self.app.query(             yql=\"select id, url, title, page, authors, contexts from pdf where userQuery() or ({targetHits:20}nearestNeighbor(embedding,q))\",             groupname=self.user,             ranking=\"colbert\",             query=query,             hits=self.pages,             body={                 \"presentation.format.tensors\": \"short-value\",                 \"input.query(q)\": f'embed(e5, \"query: {query} \")',                 \"input.query(qt)\": f'embed(colbert, \"{query}\")',             },             timeout=\"2s\",         )         if not response.is_successful():             raise ValueError(                 f\"Query failed with status code {response.status_code}, url={response.url} response={response.json}\"             )         return self._parse_response(response)      def _parse_response(self, response: VespaQueryResponse) -&gt; List[Document]:         documents: List[Document] = []         for hit in response.hits:             fields = hit[\"fields\"]             chunks_with_scores = self._get_chunk_similarities(fields)             ## Best k chunks from each page             best_chunks_on_page = \" ### \".join(                 [                     chunk                     for chunk, score in chunks_with_scores[0 : self.chunks_per_page]                     if score &gt; self.chunk_similarity_threshold                 ]             )             documents.append(                 Document(                     id=fields[\"id\"],                     page_content=best_chunks_on_page,                     title=fields[\"title\"],                     metadata={                         \"title\": fields[\"title\"],                         \"url\": fields[\"url\"],                         \"page\": fields[\"page\"],                         \"authors\": fields[\"authors\"],                         \"features\": fields[\"matchfeatures\"],                     },                 )             )         return documents      def _get_chunk_similarities(self, hit_fields: dict) -&gt; List[tuple]:         match_features = hit_fields[\"matchfeatures\"]         similarities = match_features[\"max_sim_per_context\"]         chunk_scores = []         for i in range(0, len(similarities)):             chunk_scores.append(similarities.get(str(i), 0))         chunks = hit_fields[\"contexts\"]         chunks_with_scores = list(zip(chunks, chunk_scores))         return sorted(chunks_with_scores, key=lambda x: x[1], reverse=True) <p>That's it! We can give our newborn retriever a spin for the user\u00a0<code>jo-bergum</code> by</p> In\u00a0[26]: Copied! <pre>vespa_hybrid_retriever = VespaStreamingColBERTRetriever(\n    app=app, user=\"jo-bergum\", pages=1, chunks_per_page=3\n)\n</pre> vespa_hybrid_retriever = VespaStreamingColBERTRetriever(     app=app, user=\"jo-bergum\", pages=1, chunks_per_page=3 ) In\u00a0[27]: Copied! <pre>vespa_hybrid_retriever.get_relevant_documents(\"what is the maxsim operator in colbert?\")\n</pre> vespa_hybrid_retriever.get_relevant_documents(\"what is the maxsim operator in colbert?\") Out[27]: <pre>[Document(page_content='ture that precisely does so. As illustrated, every query embeddinginteracts with all document embeddings via a MaxSim operator,which computes maximum similarity (e.g., cosine similarity), andthe scalar outputs of these operators are summed across queryterms. /T_his paradigm allows ColBERT to exploit deep LM-basedrepresentations while shi/f_ting the cost of encoding documents of-/f_line and amortizing the cost of encoding the query once acrossall ranked documents. Additionally, it enables ColBERT to lever-age vector-similarity search indexes (e.g., [ 1,15]) to retrieve thetop-kresults directly from a large document collection, substan-tially improving recall over models that only re-rank the output ofterm-based retrieval.As Figure 1 illustrates, ColBERT can serve queries in tens orfew hundreds of milliseconds. For instance, when used for re-ranking as in \u201cColBERT (re-rank)\u201d, it delivers over 170 \u00d7speedup(and requires 14,000 \u00d7fewer FLOPs) relative to existing BERT-based ### models, while being more e\ufb00ective than every non-BERT baseline(\u00a74.2 &amp; 4.3). ColBERT\u2019s indexing\u2014the only time it needs to feeddocuments through BERT\u2014is also practical: it can index the MSMARCO collection of 9M passages in about 3 hours using a singleserver with four GPUs ( \u00a74.5), retaining its e\ufb00ectiveness with a spacefootprint of as li/t_tle as few tens of GiBs. Our extensive ablationstudy ( \u00a74.4) shows that late interaction, its implementation viaMaxSim operations, and crucial design choices within our BERT-based encoders are all essential to ColBERT\u2019s e\ufb00ectiveness.Our main contributions are as follows.(1)We propose late interaction (\u00a73.1) as a paradigm for e\ufb03cientand e\ufb00ective neural ranking.(2)We present ColBERT ( \u00a73.2 &amp; 3.3), a highly-e\ufb00ective modelthat employs novel BERT-based query and document en-coders within the late interaction paradigm.', metadata={'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'url': 'https://arxiv.org/pdf/2004.12832.pdf', 'page': 4, 'authors': ['Omar Khattab', 'Matei Zaharia'], 'features': {'cos_sim': 0.6664045997289173, 'max_sim': 124.19231414794922, 'max_sim_per_context': {'0': 124.19231414794922, '1': 92.21265411376953}}})]</pre> <p>Finally, we can connect our custom retriever with the complete flexibility and power of the [LangChain] LLM framework. The following uses LangChain Expression Language, or LCEL, a declarative way to compose chains.</p> <p>We have several steps composed into a chain:</p> <ul> <li>The prompt template and LLM model, in this case using OpenAI</li> <li>The retriever that provides the retrieved context for the question</li> <li>The formatting of the retrieved context</li> </ul> In\u00a0[28]: Copied! <pre>vespa_hybrid_retriever = VespaStreamingColBERTRetriever(\n    app=app, user=\"jo-bergum\", chunks_per_page=3\n)\n</pre> vespa_hybrid_retriever = VespaStreamingColBERTRetriever(     app=app, user=\"jo-bergum\", chunks_per_page=3 ) In\u00a0[30]: Copied! <pre>from langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema import StrOutputParser\nfrom langchain.schema.runnable import RunnablePassthrough\n\nprompt_template = \"\"\"\nAnswer the question based only on the following context.\nCite the page number and the url of the document you are citing.\n\n{context}\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(prompt_template)\nmodel = ChatOpenAI(model=\"gpt-4-0125-preview\")\n\n\ndef format_prompt_context(docs) -&gt; str:\n    context = []\n    for d in docs:\n        context.append(f\"{d.metadata['title']} by {d.metadata['authors']}\\n\")\n        context.append(f\"url: {d.metadata['url']}\\n\")\n        context.append(f\"page: {d.metadata['page']}\\n\")\n        context.append(f\"{d.page_content}\\n\\n\")\n    return \"\".join(context)\n\n\nchain = (\n    {\n        \"context\": vespa_hybrid_retriever | format_prompt_context,\n        \"question\": RunnablePassthrough(),\n    }\n    | prompt\n    | model\n    | StrOutputParser()\n)\n</pre> from langchain_openai import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough  prompt_template = \"\"\" Answer the question based only on the following context. Cite the page number and the url of the document you are citing.  {context} Question: {question} \"\"\" prompt = ChatPromptTemplate.from_template(prompt_template) model = ChatOpenAI(model=\"gpt-4-0125-preview\")   def format_prompt_context(docs) -&gt; str:     context = []     for d in docs:         context.append(f\"{d.metadata['title']} by {d.metadata['authors']}\\n\")         context.append(f\"url: {d.metadata['url']}\\n\")         context.append(f\"page: {d.metadata['page']}\\n\")         context.append(f\"{d.page_content}\\n\\n\")     return \"\".join(context)   chain = (     {         \"context\": vespa_hybrid_retriever | format_prompt_context,         \"question\": RunnablePassthrough(),     }     | prompt     | model     | StrOutputParser() ) In\u00a0[31]: Copied! <pre>chain.invoke(\"what is colbert?\")\n</pre> chain.invoke(\"what is colbert?\") Out[31]: <pre>'ColBERT, introduced by Omar Khattab and Matei Zaharia, is a novel ranking model that employs contextualized late interaction over deep language models (LMs), specifically focusing on BERT (Bidirectional Encoder Representations from Transformers) for efficient and effective passage search. It achieves this by independently encoding queries and documents into fine-grained representations that interact via cheap and pruning-friendly computations. This approach allows ColBERT to leverage the expressiveness of deep LMs while significantly speeding up query processing compared to existing BERT-based models. ColBERT also enables end-to-end neural retrieval directly from a large document collection, offering more than 170 times faster performance and requiring 14,000 times fewer FLOPs (floating-point operations) per query than previous BERT-based models, with minimal impact on quality. It outperforms every non-BERT baseline in effectiveness (https://arxiv.org/pdf/2004.12832.pdf, page 18).\\n\\nColBERT differentiates itself with a mechanism that delays the query-document interaction, which allows for pre-computation of document representations for cheap neural re-ranking and supports practical end-to-end neural retrieval through pruning via vector-similarity search. This method preserves the effectiveness of state-of-the-art models that condition most of their computations on the joint query-document pair, making ColBERT a scalable solution for passage search challenges (https://arxiv.org/pdf/2004.12832.pdf, page 6).'</pre> In\u00a0[32]: Copied! <pre>chain.invoke(\"what is the colbert maxsim operator\")\n</pre> chain.invoke(\"what is the colbert maxsim operator\") Out[32]: <pre>'The ColBERT MaxSim operator is a mechanism for computing the maximum similarity between query embeddings and document embeddings. It operates by calculating the maximum similarity (e.g., cosine similarity) for each query embedding with all document embeddings, and then summing the scalar outputs of these operations across query terms. This paradigm enables the efficient and effective retrieval of documents by allowing for the interaction between deep language model-based representations of queries and documents to occur in a late stage of the processing pipeline, thereby shifting the cost of encoding documents offline and amortizing the cost of encoding the query across all ranked documents. Additionally, the MaxSim operator facilitates the use of vector-similarity search indexes to directly retrieve the top-k results from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval. This operator is a key component of ColBERT\\'s approach to efficient and effective passage search.\\n\\nSource: \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\" by Omar Khattab and Matei Zaharia, page 4, https://arxiv.org/pdf/2004.12832.pdf'</pre> In\u00a0[33]: Copied! <pre>chain.invoke(\n    \"What is the difference between colbert and single vector representational models?\"\n)\n</pre> chain.invoke(     \"What is the difference between colbert and single vector representational models?\" ) Out[33]: <pre>'The main difference between ColBERT and single-vector representational models lies in their approach to handling document and query representations for information retrieval tasks. ColBERT utilizes a multi-vector representation for both queries and documents, whereas single-vector models encode each query and each document into a single, dense vector.\\n\\n1. **Multi-Vector vs. Single-Vector Representations**: ColBERT leverages a late interaction mechanism that allows for fine-grained matching between the multiple embeddings of query terms and document tokens. This approach enables capturing the nuanced semantics of the text by considering the contextualized representation of each term separately. On the other hand, single-vector models compress the entire content of a document or a query into a single dense vector, which might lead to a loss of detail and context specificity.\\n\\n2. **Efficiency and Effectiveness**: While single-vector models might be simpler and potentially faster in some scenarios due to their straightforward matching mechanism (e.g., cosine similarity between query and document vectors), this simplicity could come at the cost of effectiveness. ColBERT, with its detailed interaction between term-level vectors, can offer more accurate retrieval results because it preserves and utilizes the rich semantic relationships within and across the text of queries and documents. However, ColBERT\\'s detailed approach initially required more storage and computational resources compared to single-vector models. Nonetheless, advancements like ColBERTv2 have significantly improved the efficiency, achieving competitive storage requirements and reducing the computational cost while maintaining or even enhancing retrieval effectiveness.\\n\\n3. **Compression and Storage**: Initial versions of multi-vector models like ColBERT required significantly more storage space compared to single-vector models due to storing multiple vectors per document. However, with the introduction of techniques like residual compression in ColBERTv2, the storage requirements have been drastically reduced to levels competitive with single-vector models. Single-vector models, while naturally more storage-efficient, can also be compressed, but aggressive compression might exacerbate the loss in quality.\\n\\n4. **Search Quality and Compression**: Despite the potential for aggressive compression in single-vector models, such approaches often lead to a more pronounced loss in quality compared to late interaction methods like ColBERTv2. ColBERTv2, even when employing compression techniques to reduce its storage footprint, can achieve higher quality across systems, showcasing the robustness of its retrieval capabilities even when optimizing for space efficiency.\\n\\nIn summary, the difference between ColBERT and single-vector representational models is primarily in their approach to encoding and matching queries and documents, with ColBERT focusing on detailed, term-level interactions for improved accuracy, and single-vector models emphasizing simplicity and compactness, which might come at the cost of retrieval effectiveness.\\n\\nCitations:\\n- Santhanam et al., \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction,\" p. 14, 15, 17, https://arxiv.org/pdf/2112.01488.pdf'</pre> In\u00a0[34]: Copied! <pre>chain.invoke(\"Why does ColBERT work better for longer documents?\")\n</pre> chain.invoke(\"Why does ColBERT work better for longer documents?\") Out[34]: <pre>\"ColBERT is designed to efficiently handle the interaction between query and document representations through a mechanism called late interaction, which is particularly beneficial when dealing with longer documents. This is because ColBERT independently encodes queries and documents into fine-grained representations using BERT, and then employs a cheap yet powerful interaction step that models their fine-grained similarity. This approach allows for the pre-computation of document representations offline, significantly speeding up query processing by avoiding the need to feed each query-document pair through a massive neural network at query time.\\n\\nFor longer documents, the benefits of this approach are twofold:\\n\\n1. **Efficiency in Handling Long Documents**: Since ColBERT encodes document representations offline, it can efficiently manage longer documents without a proportional increase in computational cost at query time. This is unlike traditional BERT-based models that might require more computational resources to process longer documents due to their size and complexity.\\n\\n2. **Effectiveness in Capturing Fine-Grained Semantics**: The fine-grained representations and the late interaction mechanism enable ColBERT to effectively capture the nuances and detailed semantics of longer documents. This is crucial for maintaining high retrieval quality, as longer documents often contain more information and require a more nuanced understanding to match relevant queries accurately.\\n\\nThus, ColBERT's architecture, which leverages the strengths of BERT for deep language understanding while introducing efficiencies through late interaction, makes it particularly adept at handling longer documents. It achieves this by pre-computing and efficiently utilizing detailed semantic representations of documents, enabling both high-quality retrieval and significant speed-ups in query processing times compared to traditional BERT-based models.\\n\\nReference: ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT by ['Omar Khattab', 'Matei Zaharia'] (https://arxiv.org/pdf/2004.12832.pdf), page 4.\"</pre> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#chat-with-your-pdfs-with-colbert-langchain-and-vespa","title":"Chat with your pdfs with ColBERT, langchain, and Vespa\u00b6","text":"<p>This notebook illustrates using Vespa streaming mode to build cost-efficient RAG applications over naturally sharded data. It also demonstrates how you can now use ColBERT ranking natively in Vespa, which can now handle the ColBERT embedding process for you with no custom code!</p> <p>You can read more about Vespa vector streaming search in these blog posts:</p> <ul> <li>Announcing vector streaming search: AI assistants at scale without breaking the bank</li> <li>Yahoo Mail turns to Vespa to do RAG at scale</li> <li>Hands-On RAG guide for personal data with Vespa and LLamaIndex</li> <li>Turbocharge RAG with LangChain and Vespa Streaming Mode for Sharded Data</li> </ul>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#tldr-vespa-streaming-mode-for-partitioned-data","title":"TLDR; Vespa streaming mode for partitioned data\u00b6","text":"<p>Vespa's streaming search solution enables you to integrate a user ID (or any sharding key) into the Vespa document ID. This setup allows Vespa to efficiently group each user's data on a small set of nodes and the same disk chunk. Streaming mode enables low latency searches on a user's data without keeping data in memory.</p> <p>The key benefits of streaming mode:</p> <ul> <li>Eliminating compromises in precision introduced by approximate algorithms</li> <li>Achieve significantly higher write throughput, thanks to the absence of index builds required for supporting approximate search.</li> <li>Optimize efficiency by storing documents, including tensors and data, on disk, benefiting from the cost-effective economics of storage tiers.</li> <li>Storage cost is the primary cost driver of Vespa streaming mode; no data is in memory. Avoiding memory usage lowers deployment costs significantly.</li> </ul>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#connecting-langchain-retriever-with-vespa-for-context-retrieval-from-pdf-documents","title":"Connecting LangChain Retriever with Vespa for Context Retrieval from PDF Documents\u00b6","text":"<p>In this notebook, we seamlessly integrate a custom LangChain retriever with a Vespa app, leveraging Vespa's streaming mode to extract meaningful context from PDF documents.</p> <p>The workflow</p> <ul> <li>Define and deploy a Vespa application package using PyVespa.</li> <li>Utilize LangChain PDF Loaders to download and parse PDF files.</li> <li>Leverage LangChain Document Transformers to convert each PDF page into multiple model context-sized parts.</li> <li>Feed the transformer representation to the running Vespa instance</li> <li>Employ Vespa's built-in ColBERT embedder functionality (using an open-source embedding model) for embedding the contexts, resulting in a multi-vector representation per context</li> <li>Develop a custom Retriever to enable seamless retrieval for any unstructured text query.</li> </ul> <p></p> <p></p> <p>Let's get started! First, install dependencies:</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#sample-data","title":"Sample data\u00b6","text":"<p>We love ColBERT, so we'll use a few COlBERT related papers as examples of PDFs in this notebook.</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#processing-pdfs-with-langchain","title":"Processing PDFs with LangChain\u00b6","text":"<p>LangChain has a rich set of document loaders that can be used to load and process various file formats. In this notebook, we use the PyPDFLoader.</p> <p>We also want to split the extracted text into contexts using a text splitter. Most text embedding models have limited input lengths (typically less than 512 language model tokens, so splitting the text into multiple contexts that each fits into the context limit of the embedding model is a common strategy.</p> <p>For embedding text data, models based on the Transformer architecture have become the de facto standard. A challenge with Transformer-based models is their input length limitation due to the quadratic self-attention computational complexity. For example, a popular open-source text embedding model like e5 has an absolute maximum input length of 512 wordpiece tokens. In addition to the technical limitation, trying to fit more tokens than used during fine-tuning of the model will impact the quality of the vector representation.</p> <p>One can view this text embedding encoding as a lossy compression technique, where variable-length texts are compressed into a fixed dimensional vector representation.</p> <p>Although this compressed representation is very useful, it can be imprecise especially as the size of the text increases. By adding the ColBERT embedding, we also retain token-level information which retains more of the original meaning of the text and allows the richer late interaction between the query and the document text.</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Now, we can also query our data. With streaming mode, we must pass the <code>groupname</code> parameter, or the request will fail with an error.</p> <p>The query request uses the Vespa Query API and the <code>Vespa.query()</code> function supports passing any of the Vespa query API parameters.</p> <p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> </ul> <p>Sample query request for <code>why is colbert effective?</code> for the user <code>jo-bergum</code>:</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#langchain-retriever","title":"LangChain Retriever\u00b6","text":"<p>We use the LangChain Retriever interface so that we can connect our Vespa app with the flexibility and power of the LangChain LLM framework.</p> <p>A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.</p> <p>The retriever interface fits perfectly with Vespa, as Vespa can support a wide range of features and ways to retrieve and rank content. The following implements a custom retriever <code>VespaStreamingColBERTRetriever</code> that takes the following arguments:</p> <ul> <li><code>app:Vespa</code> The Vespa application we retrieve from. This could be a Vespa Cloud instance or a local instance, for example running on a laptop.</li> <li><code>user:str</code> The user that that we want to retrieve for, this argument maps to the Vespa streaming mode groupname parameter</li> <li><code>pages:int</code> The target number of PDF pages we want to retrieve for a given query</li> <li><code>chunks_per_page</code> The is the target number of relevant text chunks that are associated with the page</li> <li><code>chunk_similarity_threshold</code> - The chunk similarity threshold, only chunks with a similarity above this threshold</li> </ul> <p>The core idea is to retrieve pages using max context similarity as the initial scoring function, then re-rank the top-K pages using the ColBERT embeddings. This re-ranking is handled by the second phase of the Vespa ranking expression defined above, and is transparent to the retriever code below.</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#rag","title":"RAG\u00b6","text":""},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#interact-with-the-chain","title":"Interact with the chain\u00b6","text":"<p>Now, we can start asking questions using the <code>chain</code> define above.</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#summary","title":"Summary\u00b6","text":"<p>Vespa\u2019s streaming mode is a game-changer, enabling the creation of highly cost-effective RAG applications for naturally partitioned data. Now it is also possible to use ColBERT for re-ranking, without having to integrate any custom embedder or re-ranking code.</p> <p>In this notebook, we delved into the hands-on application of LangChain, leveraging document loaders and transformers. Finally, we showcased a custom LangChain retriever that connected all the functionality of LangChain with Vespa.</p> <p>For those interested in learning more about Vespa, join the Vespa community on Slack to exchange ideas, seek assistance, or stay in the loop on the latest Vespa developments.</p> <p>We can now delete the cloud instance:</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html","title":"Cohere binary vectors in vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa cohere==4.57 vespacli\n</pre> !pip3 install -U pyvespa cohere==4.57 vespacli In\u00a0[2]: Copied! <pre>import cohere\n\n# Make sure that the environment variable CO_API_KEY is set to your API key\nco = cohere.Client()\n</pre> import cohere  # Make sure that the environment variable CO_API_KEY is set to your API key co = cohere.Client() In\u00a0[3]: Copied! <pre>documents = [\n    \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",\n    \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\",\n    \"Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.\",\n    \"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\",\n]\n</pre> documents = [     \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",     \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\",     \"Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.\",     \"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\", ] <p>Notice that we ask for <code>embedding_types=[\"binary]</code></p> In\u00a0[4]: Copied! <pre># Compute the binary embeddings ofdocuments.\n# Set input_type to \"search_document\" and embedding_types to \"binary\"\n\ncohere_response = co.embed(\n    documents,\n    model=\"embed-english-v3.0\",\n    input_type=\"search_document\",\n    embedding_types=[\"binary\"],\n)\n</pre> # Compute the binary embeddings ofdocuments. # Set input_type to \"search_document\" and embedding_types to \"binary\"  cohere_response = co.embed(     documents,     model=\"embed-english-v3.0\",     input_type=\"search_document\",     embedding_types=[\"binary\"], ) In\u00a0[5]: Copied! <pre>print(cohere_response.embeddings.binary)\n</pre> print(cohere_response.embeddings.binary) <pre>[[-110, 121, 110, -50, 87, -59, 8, 35, 114, 30, -92, -112, -118, -16, 7, 96, 17, 51, 97, -9, -23, 25, -103, -35, -78, -47, 64, -123, -41, 67, 14, -31, -42, -126, 75, 111, 62, -64, 57, 64, -52, -66, -64, -12, 100, 99, 87, 61, -5, 5, 23, 34, -75, -66, -16, 91, 92, 121, 55, 117, 100, -112, -24, 84, 84, -65, 61, -31, -45, 7, 44, 8, -35, -125, 16, -50, -52, 11, -105, -32, 102, -62, -3, 86, -107, 21, 95, 15, 27, -79, -20, 114, 90, 125, 110, -97, -15, -98, 21, -102, -124, 112, -115, 26, -86, -55, 67, 7, 11, -127, 125, 103, -46, -55, 79, -31, 126, -32, 33, -128, -124, -80, 21, 27, -49, -9, 112, 101], [-110, -7, -24, 23, -33, 68, 24, 35, 22, -50, -32, 86, 74, -14, 71, 96, 81, -45, 105, -25, -73, 108, -99, 13, -76, 125, 73, -44, -34, -34, -105, 75, 86, -58, 85, -30, -92, -27, -39, 0, -75, -2, 30, -12, -116, 9, 81, 39, 76, 44, 87, 20, -43, 110, -75, 20, 108, 125, -75, 85, -28, -118, -24, 127, 78, -75, 108, -20, -48, 3, 12, 12, 71, -29, -98, -26, 68, 11, 0, -104, 96, 70, -3, 53, -98, -108, 127, -102, -17, -84, -88, 88, -54, -45, -11, -4, -4, 15, -67, 122, -108, 117, -51, 40, 98, -47, 102, -103, 3, -123, -85, 119, -48, -24, 95, -34, -26, -24, -31, -9, 99, 64, -128, -43, 74, -91, 80, -95], [64, -14, -4, 30, 118, 5, 8, 35, 51, 3, 72, -122, -70, -10, 2, -20, 17, 115, -67, -9, 115, 31, -103, -73, -78, 65, 64, -123, -41, 91, 14, -39, -41, -78, 73, -62, 60, -28, 89, 32, 33, -35, -62, 116, 102, -45, 83, 63, 73, 37, 23, 64, -43, -46, -106, 83, 109, 92, -87, -15, -60, -39, -23, 63, 84, 56, -6, -15, 20, 3, 76, 3, 104, -16, -79, 70, -123, 15, -125, -111, 109, -105, -99, 82, -19, -27, 95, -113, 94, -74, 57, 82, -102, -7, -95, -21, -3, -66, 73, 95, -124, 37, -115, -81, 107, -55, -25, 6, 19, -107, -120, 111, -110, -23, 79, -26, 106, -61, -96, -77, 9, 116, -115, -67, -63, -9, -43, 77], [-109, -7, -32, 19, 87, 116, 8, 35, 54, -102, -64, -106, -14, -10, 31, 78, -99, 59, -6, -45, 97, 96, -103, 37, 69, -35, -119, -59, 95, 27, 14, 73, 86, -9, -43, 110, -70, 96, 45, 32, -91, 62, -64, -12, 100, -55, 34, 62, 14, 5, 22, 67, -75, -17, -14, 81, 45, 125, -15, -11, -28, 75, -25, 20, 42, -78, -4, -67, -44, 11, 76, 3, 127, 40, 0, 103, 75, -62, -123, -111, 64, -13, -10, -5, -66, -89, 119, -70, -29, -95, -19, 82, 106, 127, -24, -11, -48, 15, -29, -102, -115, 107, -115, 55, -69, -61, 103, 11, 3, 25, -118, 63, -108, 11, 78, -28, 14, 124, 119, -61, 97, 84, 53, 69, 123, 89, -104, -127]]\n</pre> <p>As we can see from the above, we got an array of binary embeddings, using signed <code>int8</code> precision in the numeric range [-128 to 127]. Each embedding vector has 128 dimensions:</p> In\u00a0[6]: Copied! <pre>len(cohere_response.embeddings.binary[0])\n</pre> len(cohere_response.embeddings.binary[0]) Out[6]: <pre>128</pre> In\u00a0[20]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\nmy_schema = Schema(\n    name=\"doc\",\n    mode=\"index\",\n    document=Document(\n        fields=[\n            Field(\n                name=\"doc_id\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                match=[\"word\"],\n                rank=\"filter\",\n            ),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"binary_vector\",\n                type=\"tensor&lt;int8&gt;(x[128])\",\n                indexing=[\"attribute\", \"index\"],\n                attribute=[\"distance-metric: hamming\"],\n            ),\n        ]\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  my_schema = Schema(     name=\"doc\",     mode=\"index\",     document=Document(         fields=[             Field(                 name=\"doc_id\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 match=[\"word\"],                 rank=\"filter\",             ),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"binary_vector\",                 type=\"tensor(x[128])\",                 indexing=[\"attribute\", \"index\"],                 attribute=[\"distance-metric: hamming\"],             ),         ]     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])], ) <p>We must add the schema to a Vespa application package. This consists of configuration files, schemas, models, and possibly even custom code (plugins).</p> In\u00a0[21]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"cohere\"\nvespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema])\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"cohere\" vespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema]) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p><code>unpack_bits</code> unpacks the binary representation into a 1024-dimensional float vector doc.</p> <p>We define two tensor inputs, one compact binary representation that is used for the nearestNeighbor search and one full version that is used in ranking.</p> In\u00a0[22]: Copied! <pre>from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function\n\n\nrerank = RankProfile(\n    name=\"rerank\",\n    inputs=[\n        (\"query(q_binary)\", \"tensor&lt;int8&gt;(x[128])\"),\n        (\"query(q_full)\", \"tensor&lt;float&gt;(x[1024])\"),\n    ],\n    functions=[\n        Function(  # this returns a tensor&lt;float&gt;(x[1024]) with values -1 or 1\n            name=\"unpack_binary_representation\",\n            expression=\"2*unpack_bits(attribute(binary_vector)) -1\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(\n        expression=\"closeness(field, binary_vector)\"  # 1/(1 + hamming_distance). Calculated between the binary query and the binary_vector\n    ),\n    second_phase=SecondPhaseRanking(\n        expression=\"sum( query(q_full)* unpack_binary_representation )\",  # re-rank using the dot product between float query and the unpacked binary representation\n        rerank_count=100,\n    ),\n    match_features=[\n        \"distance(field, binary_vector)\",\n        \"closeness(field, binary_vector)\",\n    ],\n)\nmy_schema.add_rank_profile(rerank)\n</pre> from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function   rerank = RankProfile(     name=\"rerank\",     inputs=[         (\"query(q_binary)\", \"tensor(x[128])\"),         (\"query(q_full)\", \"tensor(x[1024])\"),     ],     functions=[         Function(  # this returns a tensor(x[1024]) with values -1 or 1             name=\"unpack_binary_representation\",             expression=\"2*unpack_bits(attribute(binary_vector)) -1\",         )     ],     first_phase=FirstPhaseRanking(         expression=\"closeness(field, binary_vector)\"  # 1/(1 + hamming_distance). Calculated between the binary query and the binary_vector     ),     second_phase=SecondPhaseRanking(         expression=\"sum( query(q_full)* unpack_binary_representation )\",  # re-rank using the dot product between float query and the unpacked binary representation         rerank_count=100,     ),     match_features=[         \"distance(field, binary_vector)\",         \"closeness(field, binary_vector)\",     ], ) my_schema.add_rank_profile(rerank) In\u00a0[26]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() In\u00a0[28]: Copied! <pre>from vespa.io import VespaResponse\n\nwith app.syncio(connections=12) as sync:\n    for i, doc in enumerate(documents):\n        response: VespaResponse = sync.feed_data_point(\n            schema=\"doc\",\n            data_id=str(i),\n            fields={\n                \"doc_id\": str(i),\n                \"text\": doc,\n                \"binary_vector\": cohere_response.embeddings.binary[i],\n            },\n        )\n        assert response.is_successful()\n</pre> from vespa.io import VespaResponse  with app.syncio(connections=12) as sync:     for i, doc in enumerate(documents):         response: VespaResponse = sync.feed_data_point(             schema=\"doc\",             data_id=str(i),             fields={                 \"doc_id\": str(i),                 \"text\": doc,                 \"binary_vector\": cohere_response.embeddings.binary[i],             },         )         assert response.is_successful() <p>For some cases where we have lots of vector data, we can use the hex format for binary indexed tensors.</p> In\u00a0[30]: Copied! <pre>from binascii import hexlify\nimport numpy as np\n\n\ndef to_hex_str(binary_vector):\n    return str(hexlify(np.array(binary_vector, dtype=np.int8)), \"utf-8\")\n</pre> from binascii import hexlify import numpy as np   def to_hex_str(binary_vector):     return str(hexlify(np.array(binary_vector, dtype=np.int8)), \"utf-8\") <p>Feed using hex format</p> In\u00a0[32]: Copied! <pre>with app.syncio() as sync:\n    for i, doc in enumerate(documents):\n        response: VespaResponse = sync.feed_data_point(\n            schema=\"doc\",\n            data_id=str(i),\n            fields={\n                \"doc_id\": str(i),\n                \"text\": doc,\n                \"binary_vector\": {\n                    \"values\": to_hex_str(cohere_response.embeddings.binary[i])\n                },\n            },\n        )\n        assert response.is_successful()\n</pre> with app.syncio() as sync:     for i, doc in enumerate(documents):         response: VespaResponse = sync.feed_data_point(             schema=\"doc\",             data_id=str(i),             fields={                 \"doc_id\": str(i),                 \"text\": doc,                 \"binary_vector\": {                     \"values\": to_hex_str(cohere_response.embeddings.binary[i])                 },             },         )         assert response.is_successful() In\u00a0[33]: Copied! <pre>query = \"Who discovered x-ray?\"\n\n# Make sure to set input_type=\"search_query\" when getting the embeddings for the query.\n# We ask for both float and binary query embeddings\ncohere_query_response = co.embed(\n    [query],\n    model=\"embed-english-v3.0\",\n    input_type=\"search_query\",\n    embedding_types=[\"float\", \"binary\"],\n)\n</pre> query = \"Who discovered x-ray?\"  # Make sure to set input_type=\"search_query\" when getting the embeddings for the query. # We ask for both float and binary query embeddings cohere_query_response = co.embed(     [query],     model=\"embed-english-v3.0\",     input_type=\"search_query\",     embedding_types=[\"float\", \"binary\"], ) <p>Now, we use nearestNeighbor search to retrieve 100 hits using hamming distance, these hits are then exposed to vespa ranking framework, where we re-rank using the dot product between the float tensor and the unpacked binary vector (the unpack returns a 1024 float version).</p> In\u00a0[35]: Copied! <pre>response = app.query(\n    yql=\"select * from doc where {targetHits:100}nearestNeighbor(binary_vector,q_binary)\",\n    ranking=\"rerank\",\n    body={\n        \"input.query(q_binary)\": to_hex_str(cohere_query_response.embeddings.binary[0]),\n        \"input.query(q_full)\": cohere_query_response.embeddings.float[0],\n    },\n)\nassert response.is_successful()\n</pre> response = app.query(     yql=\"select * from doc where {targetHits:100}nearestNeighbor(binary_vector,q_binary)\",     ranking=\"rerank\",     body={         \"input.query(q_binary)\": to_hex_str(cohere_query_response.embeddings.binary[0]),         \"input.query(q_full)\": cohere_query_response.embeddings.float[0],     }, ) assert response.is_successful() In\u00a0[36]: Copied! <pre>response.hits\n</pre> response.hits Out[36]: <pre>[{'id': 'id:doc:doc::3',\n  'relevance': 8.697503089904785,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.0029940119760479044,\n    'distance(field,binary_vector)': 333.0},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::3',\n   'doc_id': '3',\n   'text': 'Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity'}},\n {'id': 'id:doc:doc::1',\n  'relevance': 6.413589954376221,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.002551020408163265,\n    'distance(field,binary_vector)': 391.00000000000006},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::1',\n   'doc_id': '1',\n   'text': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.'}},\n {'id': 'id:doc:doc::2',\n  'relevance': 6.379772663116455,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.002652519893899204,\n    'distance(field,binary_vector)': 376.0},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::2',\n   'doc_id': '2',\n   'text': 'Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.'}},\n {'id': 'id:doc:doc::0',\n  'relevance': 4.5963287353515625,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.0024271844660194173,\n    'distance(field,binary_vector)': 411.00000000000006},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::0',\n   'doc_id': '0',\n   'text': 'Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.'}}]</pre> <p>Notice the returned hits. The <code>relevance</code> is the score assigned by the second-phase dot product between the full query version and the unpacked binary vector. Also, we see the match features and the hamming distances. Notice that the re-ranking step has re-ordered doc 1 and doc 2.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#using-cohere-binary-embeddings-in-vespa","title":"Using Cohere Binary Embeddings in Vespa\u00b6","text":"<p>Cohere just released a new embedding API supporting binary and <code>int8</code> vectors. Read the announcement in the blog post: Cohere int8 &amp; binary Embeddings - Scale Your Vector Database to Large Datasets.</p> <p>We are excited to announce that Cohere Embed is the first embedding model that natively supports int8 and binary embeddings.</p> <p>This is significant because:</p> <ul> <li>Binarization reduces the storage footprint from 1024 floats (4096 bytes) per vector to 128 int8 (128 bytes).</li> <li>32x less data to store</li> <li>Faster distance calculations using hamming distance, which Vespa natively supports for bits packed into int8 precision. More on hamming distance in Vespa.</li> </ul> <p>Vespa supports <code>hamming</code> distance with and without hnsw indexing.</p> <p>For those wanting to learn more about binary vectors, we recommend our 2021 blog series on Billion-scale vector search with Vespa and Billion-scale vector search with Vespa - part two.</p> <p></p> <p>This notebook demonstrates how to use the Cohere binary vectors with Vespa, including a re-ranking phase that uses the float query vector version for improved accuracy. From the Cohere blog announcement:</p> <p>To improve the search quality, the float query embedding can be compared with the binary document embeddings using dot-product. So we first retrieve 10*top_k results with the binary query embedding, and then rescore the binary document embeddings with the float query embedding. This pushes the search quality from 90% to 95%.</p> <p></p> <p>Install the dependencies:</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#examining-the-cohere-embeddings","title":"Examining the Cohere embeddings\u00b6","text":"<p>Let us check out the Cohere embedding API and how we can obtain binarized embeddings. See also the Cohere embed API doc.</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#some-sample-documents","title":"Some sample documents\u00b6","text":"<p>Define a few sample documents that we want to embed</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>First, we define a Vespa schema with the fields we want to store and their type.</p> <p>Notice the <code>binary_vector</code> field that defines an indexed (dense) Vespa tensor with the dimension name <code>x[128]</code>. Indexing specifies <code>index</code> which means that Vespa will use HNSW indexing for this field. Also notice the configuration of distance-metric where we specify <code>hamming</code>.</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#feed-our-sample-documents-and-their-binary-embedding-representation","title":"Feed our sample documents and their binary embedding representation\u00b6","text":"<p>With few documents, we use the synchronous API. Read more in reads and writes.</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> <li>Practical Nearest Neighbor Search Guide</li> </ul>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#conclusions","title":"Conclusions\u00b6","text":"<p>These new Cohere binary embeddings are a huge step forward for cost-efficient vector search at scale and integrates perfectly with the rich feature set in Vespa.</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#clean-up","title":"Clean up\u00b6","text":"<p>We can now delete the cloud instance:</p>"},{"location":"examples/colbert_standalone_Vespa-cloud.html","title":"colbert standalone Vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa colbert-ai numpy torch transformers&lt;=4.49.0\n</pre> !pip3 install -U pyvespa colbert-ai numpy torch transformers&lt;=4.49.0 <p>Load a checkpoint with colbert and obtain document and query embeddings</p> In\u00a0[\u00a0]: Copied! <pre>from colbert.modeling.checkpoint import Checkpoint\nfrom colbert.infra import ColBERTConfig\n\nckpt = Checkpoint(\n    \"colbert-ir/colbertv2.0\", colbert_config=ColBERTConfig(root=\"experiments\")\n)\n</pre> from colbert.modeling.checkpoint import Checkpoint from colbert.infra import ColBERTConfig  ckpt = Checkpoint(     \"colbert-ir/colbertv2.0\", colbert_config=ColBERTConfig(root=\"experiments\") ) In\u00a0[139]: Copied! <pre>passage = [\n    \"Alan Mathison Turing was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\"\n]\n</pre> passage = [     \"Alan Mathison Turing was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\" ] In\u00a0[\u00a0]: Copied! <pre>vectors = ckpt.docFromText(passage)[0]\n</pre> vectors = ckpt.docFromText(passage)[0] In\u00a0[129]: Copied! <pre>vectors.shape\n</pre> vectors.shape Out[129]: <pre>torch.Size([27, 128])</pre> <p>In this case, we got 27 token-level embeddings, each using 128 float dimensions. This includes CLS token and special tokens used to differentiate the query from the document encoding.</p> In\u00a0[130]: Copied! <pre>query_vectors = ckpt.queryFromText([\"Who was Alan Turing?\"])[0]\nquery_vectors.shape\n</pre> query_vectors = ckpt.queryFromText([\"Who was Alan Turing?\"])[0] query_vectors.shape Out[130]: <pre>torch.Size([32, 128])</pre> <p>Routines for binarization and output in Vespa tensor format that can be used in queries and in JSON feed.</p> In\u00a0[118]: Copied! <pre>import numpy as np\nimport torch\nfrom binascii import hexlify\nfrom typing import Dict, List\n\n\ndef binarize_token_vectors_hex(vectors: torch.Tensor) -&gt; Dict[str, str]:\n    binarized_token_vectors = np.packbits(np.where(vectors &gt; 0, 1, 0), axis=1).astype(\n        np.int8\n    )\n    vespa_token_feed = dict()\n    for index in range(0, len(binarized_token_vectors)):\n        vespa_token_feed[index] = str(\n            hexlify(binarized_token_vectors[index].tobytes()), \"utf-8\"\n        )\n    return vespa_token_feed\n\n\ndef float_query_token_vectors(vectors: torch.Tensor) -&gt; Dict[str, List[float]]:\n    vespa_token_feed = dict()\n    for index in range(0, len(vectors)):\n        vespa_token_feed[index] = vectors[index].tolist()\n    return vespa_token_feed\n</pre> import numpy as np import torch from binascii import hexlify from typing import Dict, List   def binarize_token_vectors_hex(vectors: torch.Tensor) -&gt; Dict[str, str]:     binarized_token_vectors = np.packbits(np.where(vectors &gt; 0, 1, 0), axis=1).astype(         np.int8     )     vespa_token_feed = dict()     for index in range(0, len(binarized_token_vectors)):         vespa_token_feed[index] = str(             hexlify(binarized_token_vectors[index].tobytes()), \"utf-8\"         )     return vespa_token_feed   def float_query_token_vectors(vectors: torch.Tensor) -&gt; Dict[str, List[float]]:     vespa_token_feed = dict()     for index in range(0, len(vectors)):         vespa_token_feed[index] = vectors[index].tolist()     return vespa_token_feed In\u00a0[\u00a0]: Copied! <pre>import json\n\nprint(json.dumps(binarize_token_vectors_hex(vectors)))\nprint(json.dumps(float_query_token_vectors(query_vectors)))\n</pre> import json  print(json.dumps(binarize_token_vectors_hex(vectors))) print(json.dumps(float_query_token_vectors(query_vectors))) In\u00a0[151]: Copied! <pre>from vespa.package import Schema, Document, Field\n\ncolbert_schema = Schema(\n    name=\"doc\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n            Field(name=\"passage\", type=\"string\", indexing=[\"index\", \"summary\"]),\n            Field(\n                name=\"colbert\",\n                type=\"tensor&lt;int8&gt;(token{}, v[16])\",\n                indexing=[\"attribute\", \"summary\", \"index\"],\n                attribute=[\"distance-metric:hamming\"],\n            ),\n        ]\n    ),\n)\n</pre> from vespa.package import Schema, Document, Field  colbert_schema = Schema(     name=\"doc\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),             Field(name=\"passage\", type=\"string\", indexing=[\"index\", \"summary\"]),             Field(                 name=\"colbert\",                 type=\"tensor(token{}, v[16])\",                 indexing=[\"attribute\", \"summary\", \"index\"],                 attribute=[\"distance-metric:hamming\"],             ),         ]     ), ) In\u00a0[152]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"colbert\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name, schema=[colbert_schema]\n)\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"colbert\" vespa_application_package = ApplicationPackage(     name=vespa_app_name, schema=[colbert_schema] ) <p>We need to define all the query input tensors. We are going to input up to 32 query tensors in binary form these are used for retrieval</p> In\u00a0[92]: Copied! <pre>query_binary_input_tensors = []\nfor index in range(0, 32):\n    query_binary_input_tensors.append(\n        (\"query(binary_vector_{})\".format(index), \"tensor&lt;int8&gt;(v[16])\")\n    )\n</pre> query_binary_input_tensors = [] for index in range(0, 32):     query_binary_input_tensors.append(         (\"query(binary_vector_{})\".format(index), \"tensor(v[16])\")     ) <p>Note that we just use max sim in the first phase ranking over all the hits that are retrieved by the query</p> In\u00a0[153]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking\n\ncolbert = RankProfile(\n    name=\"default\",\n    inputs=[\n        (\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\"),\n        *query_binary_input_tensors,\n    ],\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(colbert)) , v\n                        ),\n                        max, token\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim\"),\n)\ncolbert_schema.add_rank_profile(colbert)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking  colbert = RankProfile(     name=\"default\",     inputs=[         (\"query(qt)\", \"tensor(querytoken{}, v[128])\"),         *query_binary_input_tensors,     ],     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(colbert)) , v                         ),                         max, token                     ),                     querytoken                 )             \"\"\",         )     ],     first_phase=FirstPhaseRanking(expression=\"max_sim\"), ) colbert_schema.add_rank_profile(colbert) <p>Install the Vespa CLI.</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install vespacli\n</pre> !pip3 install vespacli <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial. Make note of the tenant name, it is used in the next steps.</p> In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"TENANT_NAME\"] = \"vespa-team\"  # Replace with your tenant name\n\nvespa_cli_command = (\n    f'vespa config set application {os.environ[\"TENANT_NAME\"]}.{vespa_app_name}'\n)\n\n!vespa config set target cloud\n!{vespa_cli_command}\n!vespa auth cert -N\n</pre> import os  os.environ[\"TENANT_NAME\"] = \"vespa-team\"  # Replace with your tenant name  vespa_cli_command = (     f'vespa config set application {os.environ[\"TENANT_NAME\"]}.{vespa_app_name}' )  !vespa config set target cloud !{vespa_cli_command} !vespa auth cert -N <p>Validate that we have the expected data-plane credential files:</p> In\u00a0[52]: Copied! <pre>from os.path import exists\nfrom pathlib import Path\n\ncert_path = (\n    Path.home()\n    / \".vespa\"\n    / f\"{os.environ['TENANT_NAME']}.{vespa_app_name}.default/data-plane-public-cert.pem\"\n)\nkey_path = (\n    Path.home()\n    / \".vespa\"\n    / f\"{os.environ['TENANT_NAME']}.{vespa_app_name}.default/data-plane-private-key.pem\"\n)\n\nif not exists(cert_path) or not exists(key_path):\n    print(\n        \"ERROR: set the correct paths to security credentials. Correct paths above and rerun until you do not see this error\"\n    )\n</pre> from os.path import exists from pathlib import Path  cert_path = (     Path.home()     / \".vespa\"     / f\"{os.environ['TENANT_NAME']}.{vespa_app_name}.default/data-plane-public-cert.pem\" ) key_path = (     Path.home()     / \".vespa\"     / f\"{os.environ['TENANT_NAME']}.{vespa_app_name}.default/data-plane-private-key.pem\" )  if not exists(cert_path) or not exists(key_path):     print(         \"ERROR: set the correct paths to security credentials. Correct paths above and rerun until you do not see this error\"     ) <p>Note that the subsequent Vespa Cloud deploy call below will add <code>data-plane-public-cert.pem</code> to the application before deploying it to Vespa Cloud, so that you have access to both the private key and the public certificate. At the same time, Vespa Cloud only knows the public certificate.</p> In\u00a0[\u00a0]: Copied! <pre>!vespa auth api-key\n\nfrom pathlib import Path\n\napi_key_path = Path.home() / \".vespa\" / f\"{os.environ['TENANT_NAME']}.api-key.pem\"\n</pre> !vespa auth api-key  from pathlib import Path  api_key_path = Path.home() / \".vespa\" / f\"{os.environ['TENANT_NAME']}.api-key.pem\" In\u00a0[154]: Copied! <pre>from vespa.deployment import VespaCloud\n\n\ndef read_secret():\n    \"\"\"Read the API key from the environment variable. This is\n    only used for CI/CD purposes.\"\"\"\n    t = os.getenv(\"VESPA_TEAM_API_KEY\")\n    if t:\n        return t.replace(r\"\\n\", \"\\n\")\n    else:\n        return t\n\n\nvespa_cloud = VespaCloud(\n    tenant=os.environ[\"TENANT_NAME\"],\n    application=vespa_app_name,\n    key_content=read_secret() if read_secret() else None,\n    key_location=api_key_path,\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud   def read_secret():     \"\"\"Read the API key from the environment variable. This is     only used for CI/CD purposes.\"\"\"     t = os.getenv(\"VESPA_TEAM_API_KEY\")     if t:         return t.replace(r\"\\n\", \"\\n\")     else:         return t   vespa_cloud = VespaCloud(     tenant=os.environ[\"TENANT_NAME\"],     application=vespa_app_name,     key_content=read_secret() if read_secret() else None,     key_location=api_key_path,     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() In\u00a0[156]: Copied! <pre>from vespa.io import VespaResponse\n\nvespa_feed_format = {\n    \"id\": \"1\",\n    \"passage\": passage[0],\n    \"colbert\": binarize_token_vectors_hex(vectors),\n}\nwith app.syncio() as sync:\n    response: VespaResponse = sync.feed_data_point(\n        data_id=1, fields=vespa_feed_format, schema=\"doc\"\n    )\n</pre> from vespa.io import VespaResponse  vespa_feed_format = {     \"id\": \"1\",     \"passage\": passage[0],     \"colbert\": binarize_token_vectors_hex(vectors), } with app.syncio() as sync:     response: VespaResponse = sync.feed_data_point(         data_id=1, fields=vespa_feed_format, schema=\"doc\"     ) In\u00a0[\u00a0]: Copied! <pre>query_vectors = ckpt.queryFromText([\"Who was Alan Turing?\"])[0]\nbinary_query_input_tensors = binarize_token_vectors_hex(query_vectors)\n</pre> query_vectors = ckpt.queryFromText([\"Who was Alan Turing?\"])[0] binary_query_input_tensors = binarize_token_vectors_hex(query_vectors) In\u00a0[158]: Copied! <pre>binary_query_vectors = dict()\nnn_operators = list()\nfor index in range(0, 32):\n    name = \"input.query(binary_vector_{})\".format(index)\n    nn_argument = \"binary_vector_{}\".format(index)\n    value = binary_query_input_tensors[index]\n    binary_query_vectors[name] = value\n    nn_operators.append(\"({targetHits:100}nearestNeighbor(colbert, %s))\" % nn_argument)\n</pre> binary_query_vectors = dict() nn_operators = list() for index in range(0, 32):     name = \"input.query(binary_vector_{})\".format(index)     nn_argument = \"binary_vector_{}\".format(index)     value = binary_query_input_tensors[index]     binary_query_vectors[name] = value     nn_operators.append(\"({targetHits:100}nearestNeighbor(colbert, %s))\" % nn_argument) In\u00a0[159]: Copied! <pre>nn_operators = \" OR \".join(nn_operators)\n</pre> nn_operators = \" OR \".join(nn_operators) Out[159]: <pre>'({targetHits:100}nearestNeighbor(colbert, binary_vector_0)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_1)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_2)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_3)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_4)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_5)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_6)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_7)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_8)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_9)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_10)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_11)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_12)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_13)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_14)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_15)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_16)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_17)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_18)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_19)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_20)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_21)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_22)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_23)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_24)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_25)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_26)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_27)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_28)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_29)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_30)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_31))'</pre> In\u00a0[161]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select * from doc where {}\".format(nn_operators),\n    ranking=\"default\",\n    body={\n        \"presentation.format.tensors\": \"short-value\",\n        \"input.query(qt)\": float_query_token_vectors(query_vectors),\n        **binary_query_vectors,\n    },\n)\nassert response.is_successful()\nprint(json.dumps(response.hits[0], indent=2))\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select * from doc where {}\".format(nn_operators),     ranking=\"default\",     body={         \"presentation.format.tensors\": \"short-value\",         \"input.query(qt)\": float_query_token_vectors(query_vectors),         **binary_query_vectors,     }, ) assert response.is_successful() print(json.dumps(response.hits[0], indent=2)) <pre>{\n  \"id\": \"id:doc:doc::1\",\n  \"relevance\": 100.57648777961731,\n  \"source\": \"colbert_content\",\n  \"fields\": {\n    \"sddocname\": \"doc\",\n    \"documentid\": \"id:doc:doc::1\",\n    \"id\": \"1\",\n    \"passage\": \"Alan Mathison Turing was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",\n    \"colbert\": {\n      \"0\": [\n        3,\n        120,\n        69,\n        0,\n        37,\n        -60,\n        -58,\n        -95,\n        -120,\n        32,\n        -127,\n        67,\n        -36,\n        68,\n        -106,\n        -12\n      ],\n      \"1\": [\n        -106,\n        40,\n        -119,\n        -128,\n        96,\n        -60,\n        -58,\n        33,\n        48,\n        96,\n        -127,\n        67,\n        -100,\n        96,\n        -106,\n        -12\n      ],\n      \"2\": [\n        -28,\n        -84,\n        73,\n        -18,\n        113,\n        -60,\n        -51,\n        40,\n        -96,\n        121,\n        4,\n        24,\n        -99,\n        68,\n        -47,\n        -60\n      ],\n      \"3\": [\n        -13,\n        40,\n        75,\n        -124,\n        65,\n        64,\n        -32,\n        -53,\n        12,\n        64,\n        125,\n        4,\n        24,\n        -64,\n        -69,\n        101\n      ],\n      \"4\": [\n        33,\n        -54,\n        113,\n        24,\n        77,\n        -36,\n        -44,\n        3,\n        -32,\n        -72,\n        40,\n        41,\n        -38,\n        102,\n        53,\n        -35\n      ],\n      \"5\": [\n        3,\n        -22,\n        73,\n        -95,\n        73,\n        -51,\n        85,\n        -128,\n        -121,\n        25,\n        17,\n        68,\n        90,\n        64,\n        -113,\n        -28\n      ],\n      \"6\": [\n        -109,\n        -72,\n        -114,\n        0,\n        97,\n        -58,\n        -57,\n        -95,\n        40,\n        -96,\n        -112,\n        67,\n        -97,\n        -85,\n        -42,\n        -12\n      ],\n      \"7\": [\n        -112,\n        56,\n        -114,\n        0,\n        97,\n        -58,\n        -57,\n        -83,\n        40,\n        -96,\n        -127,\n        67,\n        -97,\n        43,\n        -42,\n        -12\n      ],\n      \"8\": [\n        22,\n        -71,\n        65,\n        96,\n        0,\n        -60,\n        108,\n        37,\n        16,\n        106,\n        -55,\n        115,\n        -117,\n        -56,\n        -28,\n        -12\n      ],\n      \"9\": [\n        -106,\n        -72,\n        94,\n        30,\n        32,\n        -60,\n        -60,\n        -19,\n        24,\n        -56,\n        -47,\n        -63,\n        -40,\n        -53,\n        -103,\n        -11\n      ],\n      \"10\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"11\": [\n        -126,\n        121,\n        3,\n        -103,\n        32,\n        70,\n        103,\n        -23,\n        88,\n        -55,\n        -61,\n        71,\n        -101,\n        -106,\n        -8,\n        -68\n      ],\n      \"12\": [\n        18,\n        24,\n        -106,\n        30,\n        36,\n        -42,\n        -60,\n        104,\n        57,\n        -120,\n        -128,\n        -61,\n        -67,\n        -53,\n        -100,\n        -11\n      ],\n      \"13\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"14\": [\n        22,\n        49,\n        -38,\n        17,\n        36,\n        -42,\n        -25,\n        65,\n        25,\n        -56,\n        -45,\n        -59,\n        -102,\n        -2,\n        -65,\n        125\n      ],\n      \"15\": [\n        -105,\n        25,\n        -50,\n        16,\n        0,\n        -42,\n        -28,\n        45,\n        48,\n        -56,\n        -112,\n        -55,\n        -3,\n        -87,\n        -112,\n        -11\n      ],\n      \"16\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"17\": [\n        55,\n        43,\n        -62,\n        33,\n        -91,\n        68,\n        99,\n        32,\n        72,\n        10,\n        -41,\n        70,\n        -117,\n        -78,\n        -73,\n        -11\n      ],\n      \"18\": [\n        3,\n        53,\n        -117,\n        20,\n        36,\n        -42,\n        79,\n        33,\n        9,\n        -120,\n        -41,\n        69,\n        -36,\n        -69,\n        -111,\n        117\n      ],\n      \"19\": [\n        23,\n        16,\n        -42,\n        20,\n        44,\n        -42,\n        -26,\n        33,\n        57,\n        -120,\n        -112,\n        -63,\n        -3,\n        -24,\n        -108,\n        -11\n      ],\n      \"20\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"21\": [\n        -110,\n        53,\n        -106,\n        28,\n        32,\n        -42,\n        -58,\n        77,\n        61,\n        -56,\n        -42,\n        -15,\n        -68,\n        -5,\n        -110,\n        -11\n      ],\n      \"22\": [\n        -109,\n        56,\n        -114,\n        0,\n        96,\n        -42,\n        -58,\n        -83,\n        40,\n        -96,\n        -128,\n        -61,\n        -99,\n        -21,\n        -44,\n        -12\n      ],\n      \"23\": [\n        18,\n        57,\n        -50,\n        30,\n        36,\n        86,\n        -60,\n        69,\n        9,\n        -120,\n        -48,\n        -63,\n        -75,\n        -22,\n        -98,\n        -11\n      ],\n      \"24\": [\n        30,\n        -71,\n        -106,\n        26,\n        32,\n        -42,\n        -50,\n        104,\n        56,\n        64,\n        -48,\n        -61,\n        -4,\n        -8,\n        -104,\n        -12\n      ],\n      \"25\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"26\": [\n        7,\n        56,\n        70,\n        0,\n        36,\n        -58,\n        -42,\n        33,\n        -104,\n        34,\n        -127,\n        67,\n        -99,\n        96,\n        -105,\n        -12\n      ]\n    }\n  }\n}\n</pre> <p>Another example where we brute-force \"true\" search without a retrieval step using nearestNeighbor or other filters.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select * from doc where true\",\n    ranking=\"default\",\n    body={\n        \"presentation.format.tensors\": \"short-value\",\n        \"input.query(qt)\": float_query_token_vectors(query_vectors),\n    },\n)\nassert response.is_successful()\nprint(json.dumps(response.hits[0], indent=2))\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select * from doc where true\",     ranking=\"default\",     body={         \"presentation.format.tensors\": \"short-value\",         \"input.query(qt)\": float_query_token_vectors(query_vectors),     }, ) assert response.is_successful() print(json.dumps(response.hits[0], indent=2)) In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/colbert_standalone_Vespa-cloud.html#standalone-colbert-with-vespa-for-end-to-end-retrieval-and-ranking","title":"Standalone ColBERT with Vespa for end-to-end retrieval and ranking\u00b6","text":"<p>This notebook illustrates using ColBERT package to produce token vectors, instead of using the native Vespa colbert embedder.</p> <p>This guide illustrates how to feed and query using a single passage representation</p> <ul> <li>Compress token vectors using binarization compatible with Vespa unpackbits used in ranking. This implements the binarization of token-level vectors using <code>numpy</code>.</li> <li>Use Vespa hex feed format for binary vectors doc.</li> <li>Query examples.</li> </ul> <p>As a bonus, this also demonstrates how to use ColBERT end-to-end with Vespa for both retrieval and ranking. The retrieval step searches the binary token-level representations using hamming distance. This uses 32 nearestNeighbor operators in the same query, each finding 100 nearest hits in hamming space. Then the results are re-ranked using the full-blown MaxSim calculation.</p> <p>See Announcing the Vespa ColBERT embedder for details on ColBERT and the binary quantization used to compress ColBERT's token-level vectors.</p>"},{"location":"examples/colbert_standalone_Vespa-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type.</p> <p>We use HNSW with hamming distance for retrieval</p>"},{"location":"examples/colbert_standalone_Vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud. It is also possible to deploy the app using docker; see the Hybrid Search - Quickstart guide for an example of deploying it to a local docker container.</p>"},{"location":"examples/colbert_standalone_Vespa-cloud.html#configure-vespa-cloud-date-plane-security","title":"Configure Vespa Cloud date-plane security\u00b6","text":"<p>Create Vespa Cloud data-plane mTLS cert/key-pair. The mutual certificate pair is used to talk to your Vespa cloud endpoints. See Vespa Cloud Security Guide for details.</p> <p>We save the paths to the credentials for later data-plane access without using pyvespa APIs.</p>"},{"location":"examples/colbert_standalone_Vespa-cloud.html#configure-vespa-cloud-control-plane-security","title":"Configure Vespa Cloud control-plane security\u00b6","text":"<p>Authenticate to generate a tenant level control plane API key for deploying the applications to Vespa Cloud, and save the path to it.</p> <p>The generated tenant api key must be added in the Vespa Console before attempting to deploy the application.</p> <pre><code>To use this key in Vespa Cloud click 'Add custom key' at\nhttps://console.vespa-cloud.com/tenant/TENANT_NAME/account/keys\nand paste the entire public key including the BEGIN and END lines.\n</code></pre>"},{"location":"examples/colbert_standalone_Vespa-cloud.html#deploy-to-vespa-cloud","title":"Deploy to Vespa Cloud\u00b6","text":"<p>Now that we have data-plane and control-plane credentials ready, we can deploy our application to Vespa Cloud!</p> <p><code>PyVespa</code> supports deploying apps to the development zone.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/colbert_standalone_Vespa-cloud.html#querying","title":"Querying\u00b6","text":"<p>Now we create all the query token vectors in binary form and use 32 nearestNeighbor query operators that are combined with OR. These hits are then exposed to ranking where the final MaxSim is performed using the unpacked binary representations.</p>"},{"location":"examples/colbert_standalone_long_context_Vespa-cloud.html","title":"colbert standalone long context Vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa colbert-ai numpy torch vespacli transformers&lt;=4.49.0\n</pre> !pip3 install -U pyvespa colbert-ai numpy torch vespacli transformers&lt;=4.49.0 <p>Load a checkpoint with ColBERT and obtain document and query embeddings</p> In\u00a0[\u00a0]: Copied! <pre>from colbert.modeling.checkpoint import Checkpoint\nfrom colbert.infra import ColBERTConfig\n\nckpt = Checkpoint(\n    \"colbert-ir/colbertv2.0\", colbert_config=ColBERTConfig(root=\"experiments\")\n)\n</pre> from colbert.modeling.checkpoint import Checkpoint from colbert.infra import ColBERTConfig  ckpt = Checkpoint(     \"colbert-ir/colbertv2.0\", colbert_config=ColBERTConfig(root=\"experiments\") ) <p>A few sample documents:</p> In\u00a0[50]: Copied! <pre>document_passages = [\n    \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",\n    \"Born in Maida Vale, London, Turing was raised in southern England. He graduated from King's College, Cambridge, with a degree in mathematics.\",\n    \"After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer.\",\n    \"Turing has an extensive legacy with statues of him and many things named after him, including an annual award for computer science innovations.\",\n]\n</pre> document_passages = [     \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",     \"Born in Maida Vale, London, Turing was raised in southern England. He graduated from King's College, Cambridge, with a degree in mathematics.\",     \"After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer.\",     \"Turing has an extensive legacy with statues of him and many things named after him, including an annual award for computer science innovations.\", ] In\u00a0[\u00a0]: Copied! <pre>document_token_vectors = ckpt.docFromText(document_passages)\n</pre> document_token_vectors = ckpt.docFromText(document_passages) <p>See the shape of the ColBERT document embeddings:</p> In\u00a0[52]: Copied! <pre>document_token_vectors.shape\n</pre> document_token_vectors.shape Out[52]: <pre>torch.Size([4, 35, 128])</pre> In\u00a0[53]: Copied! <pre>query_vectors = ckpt.queryFromText([\"Who was Alan Turing?\"])[0]\nquery_vectors.shape\n</pre> query_vectors = ckpt.queryFromText([\"Who was Alan Turing?\"])[0] query_vectors.shape Out[53]: <pre>torch.Size([32, 128])</pre> <p>The query is always padded to 32 so in the above we have 32 query token vectors.</p> <p>Routines for binarization and output in Vespa tensor format that can be used in queries and JSON feed.</p> In\u00a0[67]: Copied! <pre>import numpy as np\nimport torch\nfrom binascii import hexlify\nfrom typing import List, Dict\n\n\ndef binarize_token_vectors_hex(vectors: torch.Tensor) -&gt; Dict[str, str]:\n    # Notice axix=2 to pack the bits in the last dimension, which is the token level vectors\n    binarized_token_vectors = np.packbits(np.where(vectors &gt; 0, 1, 0), axis=2).astype(\n        np.int8\n    )\n    vespa_tensor = list()\n    for chunk_index in range(0, len(binarized_token_vectors)):\n        token_vectors = binarized_token_vectors[chunk_index]\n        for token_index in range(0, len(token_vectors)):\n            values = str(hexlify(token_vectors[token_index].tobytes()), \"utf-8\")\n            if (\n                values == \"00000000000000000000000000000000\"\n            ):  # skip empty vectors due to padding with batch of passages\n                continue\n            vespa_tensor_cell = {\n                \"address\": {\"context\": chunk_index, \"token\": token_index},\n                \"values\": values,\n            }\n            vespa_tensor.append(vespa_tensor_cell)\n\n    return vespa_tensor\n\n\ndef float_query_token_vectors(vectors: torch.Tensor) -&gt; Dict[str, List[float]]:\n    vespa_token_feed = dict()\n    for index in range(0, len(vectors)):\n        vespa_token_feed[index] = vectors[index].tolist()\n    return vespa_token_feed\n</pre> import numpy as np import torch from binascii import hexlify from typing import List, Dict   def binarize_token_vectors_hex(vectors: torch.Tensor) -&gt; Dict[str, str]:     # Notice axix=2 to pack the bits in the last dimension, which is the token level vectors     binarized_token_vectors = np.packbits(np.where(vectors &gt; 0, 1, 0), axis=2).astype(         np.int8     )     vespa_tensor = list()     for chunk_index in range(0, len(binarized_token_vectors)):         token_vectors = binarized_token_vectors[chunk_index]         for token_index in range(0, len(token_vectors)):             values = str(hexlify(token_vectors[token_index].tobytes()), \"utf-8\")             if (                 values == \"00000000000000000000000000000000\"             ):  # skip empty vectors due to padding with batch of passages                 continue             vespa_tensor_cell = {                 \"address\": {\"context\": chunk_index, \"token\": token_index},                 \"values\": values,             }             vespa_tensor.append(vespa_tensor_cell)      return vespa_tensor   def float_query_token_vectors(vectors: torch.Tensor) -&gt; Dict[str, List[float]]:     vespa_token_feed = dict()     for index in range(0, len(vectors)):         vespa_token_feed[index] = vectors[index].tolist()     return vespa_token_feed In\u00a0[\u00a0]: Copied! <pre>import json\n\nprint(json.dumps(binarize_token_vectors_hex(document_token_vectors)))\nprint(json.dumps(float_query_token_vectors(query_vectors)))\n</pre> import json  print(json.dumps(binarize_token_vectors_hex(document_token_vectors))) print(json.dumps(float_query_token_vectors(query_vectors))) In\u00a0[60]: Copied! <pre>from vespa.package import Schema, Document, Field\n\ncolbert_schema = Schema(\n    name=\"doc\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"passages\",\n                type=\"array&lt;string&gt;\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"colbert\",\n                type=\"tensor&lt;int8&gt;(context{}, token{}, v[16])\",\n                indexing=[\"attribute\", \"summary\"],\n            ),\n        ]\n    ),\n)\n</pre> from vespa.package import Schema, Document, Field  colbert_schema = Schema(     name=\"doc\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"passages\",                 type=\"array\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"colbert\",                 type=\"tensor(context{}, token{}, v[16])\",                 indexing=[\"attribute\", \"summary\"],             ),         ]     ), ) In\u00a0[61]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"colbertlong\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name, schema=[colbert_schema]\n)\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"colbertlong\" vespa_application_package = ApplicationPackage(     name=vespa_app_name, schema=[colbert_schema] ) <p>Note that we use max sim in the first phase ranking over all the hits that are retrieved by the query logic. Also note that asymmetric MaxSim where we use <code>unpack_bits</code> to obtain a 128-d float vector representation from the binary vector representation.</p> In\u00a0[62]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking\n\ncolbert_profile = RankProfile(\n    name=\"default\",\n    inputs=[(\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\")],\n    functions=[\n        Function(\n            name=\"max_sim_per_context\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(colbert)) , v\n                        ),\n                        max, token\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(\n            name=\"max_sim\", expression=\"reduce(max_sim_per_context, max, context)\"\n        ),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim\"),\n    match_features=[\"max_sim_per_context\"],\n)\ncolbert_schema.add_rank_profile(colbert_profile)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking  colbert_profile = RankProfile(     name=\"default\",     inputs=[(\"query(qt)\", \"tensor(querytoken{}, v[128])\")],     functions=[         Function(             name=\"max_sim_per_context\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(colbert)) , v                         ),                         max, token                     ),                     querytoken                 )             \"\"\",         ),         Function(             name=\"max_sim\", expression=\"reduce(max_sim_per_context, max, context)\"         ),     ],     first_phase=FirstPhaseRanking(expression=\"max_sim\"),     match_features=[\"max_sim_per_context\"], ) colbert_schema.add_rank_profile(colbert_profile) In\u00a0[63]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <p>Use Vespa tensor <code>blocks</code> format for mixed tensors (two mapped dimensions with one dense) doc.</p> In\u00a0[65]: Copied! <pre>from vespa.io import VespaResponse\n\nvespa_feed_format = {\n    \"id\": \"1\",\n    \"passages\": document_passages,\n    \"colbert\": {\"blocks\": binarize_token_vectors_hex(document_token_vectors)},\n}\n# synchrounous feed (this is blocking and slow, but few docs..)\nwith app.syncio() as sync:\n    response: VespaResponse = sync.feed_data_point(\n        data_id=1, fields=vespa_feed_format, schema=\"doc\"\n    )\n</pre> from vespa.io import VespaResponse  vespa_feed_format = {     \"id\": \"1\",     \"passages\": document_passages,     \"colbert\": {\"blocks\": binarize_token_vectors_hex(document_token_vectors)}, } # synchrounous feed (this is blocking and slow, but few docs..) with app.syncio() as sync:     response: VespaResponse = sync.feed_data_point(         data_id=1, fields=vespa_feed_format, schema=\"doc\"     ) <p>This example uses brute-force \"true\" search without a retrieval step using nearestNeighbor or keywords.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select * from doc where true\",\n    ranking=\"default\",\n    body={\n        \"presentation.format.tensors\": \"short-value\",\n        \"input.query(qt)\": float_query_token_vectors(query_vectors),\n    },\n)\nassert response.is_successful()\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select * from doc where true\",     ranking=\"default\",     body={         \"presentation.format.tensors\": \"short-value\",         \"input.query(qt)\": float_query_token_vectors(query_vectors),     }, ) assert response.is_successful() <p>You should see output similar to this:</p> <pre>{\n  \"id\": \"id:doc:doc::1\",\n  \"relevance\": 100.0651626586914,\n  \"source\": \"colbertlong_content\",\n  \"fields\": {\n    \"matchfeatures\": {\n      \"max_sim_per_context\": {\n        \"0\": 100.0651626586914,\n        \"1\": 62.7861328125,\n        \"2\": 67.44772338867188,\n        \"3\": 60.133323669433594\n      }\n    },\n    \"sddocname\": \"doc\",\n    \"documentid\": \"id:doc:doc::1\",\n    \"id\": \"1\",\n    \"passages\": [\n      \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",\n      \"Born in Maida Vale, London, Turing was raised in southern England. He graduated from King's College, Cambridge, with a degree in mathematics.\",\n      \"After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer.\",\n      \"Turing has an extensive legacy with statues of him and many things named after him, including an annual award for computer science innovations.\"\n    ],\n    \"colbert\": [\n      {\n        \"address\": {\n          \"context\": \"0\",\n          \"token\": \"0\"\n        },\n        \"values\": [\n          1,\n          120,\n          69,\n          0,\n          33,\n          -60,\n          -58,\n          -95,\n          -120,\n          32,\n          -127,\n          67,\n          -51,\n          68,\n          -106,\n          -12\n        ]\n      },\n      {\n        \"address\": {\n          \"context\": \"0\",\n          \"token\": \"1\"\n        },\n        \"values\": [\n          -122,\n          60,\n          9,\n          -128,\n          97,\n          -60,\n          -58,\n          -95,\n          -80,\n          112,\n          -127,\n          67,\n          -99,\n          68,\n          -106,\n          -28\n        ]\n      },\n      \"...\"\n    ],\n\n  }\n}\n</pre> <p>As can be seen from the matchfeatures, the first context (index 0) scored the highest and this is the score that is used to score the entire document.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/colbert_standalone_long_context_Vespa-cloud.html#standalone-colbert-vespa-for-long-context-ranking","title":"Standalone ColBERT + Vespa for long-context ranking\u00b6","text":"<p>This is a guide on how to use the ColBERT package to produce token-level vectors. This as an alternative for using the native Vespa colbert embedder.</p> <p>This guide illustrates how to feed multiple passages per Vespa document (long-context)</p> <ul> <li>Compress token vectors using binarization compatible with Vespa <code>unpack_bits</code></li> <li>Use Vespa hex feed format for binary vectors with mixed vespa tensors</li> <li>How to query Vespa with the ColBERT query tensor representation</li> </ul> <p>Read more about Vespa Long-Context ColBERT.</p> <p></p>"},{"location":"examples/colbert_standalone_long_context_Vespa-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/colbert_standalone_long_context_Vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/colbert_standalone_long_context_Vespa-cloud.html#querying-vespa-with-colbert-tensors","title":"Querying Vespa with ColBERT tensors\u00b6","text":""},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html","title":"colpali benchmark vqa vlm Vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install colpali-engine==0.2.2  pyvespa vespacli requests numpy scipy ir_measures pillow datasets\n</pre> !pip3 install colpali-engine==0.2.2  pyvespa vespacli requests numpy scipy ir_measures pillow datasets In\u00a0[2]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoProcessor\nfrom PIL import Image\n\n\nfrom colpali_engine.models.paligemma_colbert_architecture import ColPali\nfrom colpali_engine.utils.colpali_processing_utils import (\n    process_images,\n    process_queries,\n)\nfrom torch.amp import autocast\n</pre> import torch from torch.utils.data import DataLoader from tqdm import tqdm from transformers import AutoProcessor from PIL import Image   from colpali_engine.models.paligemma_colbert_architecture import ColPali from colpali_engine.utils.colpali_processing_utils import (     process_images,     process_queries, ) from torch.amp import autocast <p>Choose the right device to run the model on.</p> In\u00a0[3]: Copied! <pre>if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    dtype = torch.bfloat16\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    dtype = torch.float32\nelse:\n    device = torch.device(\"cpu\")\n    dtype = torch.float32\n</pre> if torch.cuda.is_available():     device = torch.device(\"cuda\")     dtype = torch.bfloat16 elif torch.backends.mps.is_available():     device = torch.device(\"mps\")     dtype = torch.float32 else:     device = torch.device(\"cpu\")     dtype = torch.float32 <p>Load the base model and the adapter.</p> In\u00a0[\u00a0]: Copied! <pre>model_name = \"vidore/colpali-v1.2\"\nmodel = ColPali.from_pretrained(\n    \"vidore/colpaligemma-3b-pt-448-base\", torch_dtype=dtype\n).eval()\nmodel.load_adapter(model_name)\nmodel = model.eval()\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_name)\n</pre> model_name = \"vidore/colpali-v1.2\" model = ColPali.from_pretrained(     \"vidore/colpaligemma-3b-pt-448-base\", torch_dtype=dtype ).eval() model.load_adapter(model_name) model = model.eval() model.to(device) processor = AutoProcessor.from_pretrained(model_name) In\u00a0[5]: Copied! <pre>from datasets import load_dataset\n\nds = load_dataset(\"vidore/docvqa_test_subsampled\", split=\"test\")\n</pre> from datasets import load_dataset  ds = load_dataset(\"vidore/docvqa_test_subsampled\", split=\"test\") <p>Now we use the ColPali model to generate embeddings for the images in the dataset. We use a dataloader to process each image and store the embeddings in a list.</p> <p>Batch size 4 requires a GPU with 16GB of memory and fits into a T4 GPU. If you have a smaller GPU, you can reduce the batch size to 2.</p> In\u00a0[6]: Copied! <pre>dataloader = DataLoader(\n    ds[\"image\"],\n    batch_size=4,\n    shuffle=False,\n    collate_fn=lambda x: process_images(processor, x),\n)\nembeddings = []\nfor batch_doc in tqdm(dataloader):\n    with torch.no_grad():\n        with autocast(device_type=device.type):\n            batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\n            embeddings_doc = model(**batch_doc)\n            embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n</pre> dataloader = DataLoader(     ds[\"image\"],     batch_size=4,     shuffle=False,     collate_fn=lambda x: process_images(processor, x), ) embeddings = [] for batch_doc in tqdm(dataloader):     with torch.no_grad():         with autocast(device_type=device.type):             batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}             embeddings_doc = model(**batch_doc)             embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\")))) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [29:29&lt;00:00, 14.16s/it]\n</pre> <p>Generate embeddings for the queries in the dataset.</p> In\u00a0[7]: Copied! <pre>dummy_image = Image.new(\"RGB\", (448, 448), (255, 255, 255))\ndataloader = DataLoader(\n    ds[\"query\"],\n    batch_size=1,\n    shuffle=False,\n    collate_fn=lambda x: process_queries(processor, x, dummy_image),\n)\nquery_embeddings = []\nfor batch_query in tqdm(dataloader):\n    with torch.no_grad():\n        with autocast(device_type=device.type):\n            batch_query = {k: v.to(model.device) for k, v in batch_query.items()}\n            embeddings_query = model(**batch_query)\n            query_embeddings.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n</pre> dummy_image = Image.new(\"RGB\", (448, 448), (255, 255, 255)) dataloader = DataLoader(     ds[\"query\"],     batch_size=1,     shuffle=False,     collate_fn=lambda x: process_queries(processor, x, dummy_image), ) query_embeddings = [] for batch_query in tqdm(dataloader):     with torch.no_grad():         with autocast(device_type=device.type):             batch_query = {k: v.to(model.device) for k, v in batch_query.items()}             embeddings_query = model(**batch_query)             query_embeddings.extend(list(torch.unbind(embeddings_query.to(\"cpu\")))) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [01:45&lt;00:00,  4.72it/s]\n</pre> <p>Now we have all the embeddings. We'll define two helper functions to perform binarization (BQ) and also packing float values to shorter hex representation in JSON. Both save bandwidth and improve feed performance.</p> In\u00a0[8]: Copied! <pre>import struct\nimport numpy as np\n\n\ndef binarize_tensor(tensor: torch.Tensor) -&gt; str:\n    \"\"\"\n    Binarize a floating-point 1-d tensor by thresholding at zero\n    and packing the bits into bytes. Returns the hex str representation of the bytes.\n    \"\"\"\n    if not tensor.is_floating_point():\n        raise ValueError(\"Input tensor must be of floating-point type.\")\n    return (\n        np.packbits(np.where(tensor &gt; 0, 1, 0), axis=0).astype(np.int8).tobytes().hex()\n    )\n</pre> import struct import numpy as np   def binarize_tensor(tensor: torch.Tensor) -&gt; str:     \"\"\"     Binarize a floating-point 1-d tensor by thresholding at zero     and packing the bits into bytes. Returns the hex str representation of the bytes.     \"\"\"     if not tensor.is_floating_point():         raise ValueError(\"Input tensor must be of floating-point type.\")     return (         np.packbits(np.where(tensor &gt; 0, 1, 0), axis=0).astype(np.int8).tobytes().hex()     ) In\u00a0[9]: Copied! <pre>def tensor_to_hex_bfloat16(tensor: torch.Tensor) -&gt; str:\n    if not tensor.is_floating_point():\n        raise ValueError(\"Input tensor must be of float32 type.\")\n\n    def float_to_bfloat16_hex(f: float) -&gt; str:\n        packed_float = struct.pack(\"=f\", f)\n        bfloat16_bits = struct.unpack(\"=H\", packed_float[2:])[0]\n        return format(bfloat16_bits, \"04X\")\n\n    hex_list = [float_to_bfloat16_hex(float(val)) for val in tensor.flatten()]\n    return \"\".join(hex_list)\n</pre> def tensor_to_hex_bfloat16(tensor: torch.Tensor) -&gt; str:     if not tensor.is_floating_point():         raise ValueError(\"Input tensor must be of float32 type.\")      def float_to_bfloat16_hex(f: float) -&gt; str:         packed_float = struct.pack(\"=f\", f)         bfloat16_bits = struct.unpack(\"=H\", packed_float[2:])[0]         return format(bfloat16_bits, \"04X\")      hex_list = [float_to_bfloat16_hex(float(val)) for val in tensor.flatten()]     return \"\".join(hex_list) In\u00a0[\u00a0]: Copied! <pre>from scipy.cluster.hierarchy import fcluster, linkage\nfrom typing import Dict, List\n\n\ndef pool_embeddings(embeddings: torch.Tensor, pool_factor=3) -&gt; torch.Tensor:\n    \"\"\"\n    pool embeddings using hierarchical clustering to reduce the number of patch embeddings.\n    Adapted from https://github.com/illuin-tech/vidore-benchmark/blob/e3b4f456d50271c69bce3d2c23131f5245d0c270/src/vidore_benchmark/compression/token_pooling.py#L32\n    Inspired by https://www.answer.ai/posts/colbert-pooling.html\n    \"\"\"\n\n    pooled_embeddings = []\n    token_length = embeddings.size(0)\n\n    if token_length == 1:\n        raise ValueError(\"The input tensor must have more than one token.\")\n    embeddings.to(device)\n\n    similarities = torch.mm(embeddings, embeddings.t())\n    if similarities.dtype == torch.bfloat16:\n        similarities = similarities.to(torch.float16)\n    similarities = 1 - similarities.cpu().numpy()\n\n    Z = linkage(similarities, metric=\"euclidean\", method=\"ward\")  # noqa: N806\n    max_clusters = max(token_length // pool_factor, 1)\n    cluster_labels = fcluster(Z, t=max_clusters, criterion=\"maxclust\")\n\n    cluster_id_to_indices: Dict[int, torch.Tensor] = {}\n\n    with torch.no_grad():\n        for cluster_id in range(1, max_clusters + 1):\n            cluster_indices = torch.where(torch.tensor(cluster_labels == cluster_id))[0]\n            cluster_id_to_indices[cluster_id] = cluster_indices\n\n            if cluster_indices.numel() &gt; 0:\n                pooled_embedding = embeddings[cluster_indices].mean(dim=0)\n                pooled_embedding = torch.nn.functional.normalize(\n                    pooled_embedding, p=2, dim=-1\n                )\n                pooled_embeddings.append(pooled_embedding)\n\n        pooled_embeddings = torch.stack(pooled_embeddings, dim=0)\n\n    return pooled_embeddings\n</pre> from scipy.cluster.hierarchy import fcluster, linkage from typing import Dict, List   def pool_embeddings(embeddings: torch.Tensor, pool_factor=3) -&gt; torch.Tensor:     \"\"\"     pool embeddings using hierarchical clustering to reduce the number of patch embeddings.     Adapted from https://github.com/illuin-tech/vidore-benchmark/blob/e3b4f456d50271c69bce3d2c23131f5245d0c270/src/vidore_benchmark/compression/token_pooling.py#L32     Inspired by https://www.answer.ai/posts/colbert-pooling.html     \"\"\"      pooled_embeddings = []     token_length = embeddings.size(0)      if token_length == 1:         raise ValueError(\"The input tensor must have more than one token.\")     embeddings.to(device)      similarities = torch.mm(embeddings, embeddings.t())     if similarities.dtype == torch.bfloat16:         similarities = similarities.to(torch.float16)     similarities = 1 - similarities.cpu().numpy()      Z = linkage(similarities, metric=\"euclidean\", method=\"ward\")  # noqa: N806     max_clusters = max(token_length // pool_factor, 1)     cluster_labels = fcluster(Z, t=max_clusters, criterion=\"maxclust\")      cluster_id_to_indices: Dict[int, torch.Tensor] = {}      with torch.no_grad():         for cluster_id in range(1, max_clusters + 1):             cluster_indices = torch.where(torch.tensor(cluster_labels == cluster_id))[0]             cluster_id_to_indices[cluster_id] = cluster_indices              if cluster_indices.numel() &gt; 0:                 pooled_embedding = embeddings[cluster_indices].mean(dim=0)                 pooled_embedding = torch.nn.functional.normalize(                     pooled_embedding, p=2, dim=-1                 )                 pooled_embeddings.append(pooled_embedding)          pooled_embeddings = torch.stack(pooled_embeddings, dim=0)      return pooled_embeddings <p>Create the Vespa feed format. We use hex formats for mixed tensors doc.</p> In\u00a0[12]: Copied! <pre>vespa_docs = []\n\nfor row, embedding in zip(ds, embeddings):\n    embedding_full = dict()\n    embedding_binary = dict()\n    # You can experiment with pooling if you want to reduce the number of embeddings\n    # pooled_embedding = pool_embeddings(embedding, pool_factor=2) # reduce the number of embeddings by a factor of 2\n    for j, emb in enumerate(embedding):\n        embedding_full[j] = tensor_to_hex_bfloat16(emb)\n        embedding_binary[j] = binarize_tensor(emb)\n    vespa_doc = {\n        \"id\": row[\"docId\"],\n        \"embedding\": embedding_full,\n        \"binary_embedding\": embedding_binary,\n    }\n    vespa_docs.append(vespa_doc)\n</pre> vespa_docs = []  for row, embedding in zip(ds, embeddings):     embedding_full = dict()     embedding_binary = dict()     # You can experiment with pooling if you want to reduce the number of embeddings     # pooled_embedding = pool_embeddings(embedding, pool_factor=2) # reduce the number of embeddings by a factor of 2     for j, emb in enumerate(embedding):         embedding_full[j] = tensor_to_hex_bfloat16(emb)         embedding_binary[j] = binarize_tensor(emb)     vespa_doc = {         \"id\": row[\"docId\"],         \"embedding\": embedding_full,         \"binary_embedding\": embedding_binary,     }     vespa_docs.append(vespa_doc) In\u00a0[14]: Copied! <pre>from vespa.package import Schema, Document, Field\n\ncolpali_schema = Schema(\n    name=\"pdf_page\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;bfloat16&gt;(patch{}, v[128])\",\n                indexing=[\"attribute\"],\n            ),\n            Field(\n                name=\"binary_embedding\",\n                type=\"tensor&lt;int8&gt;(patch{}, v[16])\",\n                indexing=[\"attribute\"],\n            ),\n        ]\n    ),\n)\n</pre> from vespa.package import Schema, Document, Field  colpali_schema = Schema(     name=\"pdf_page\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),             Field(                 name=\"embedding\",                 type=\"tensor(patch{}, v[128])\",                 indexing=[\"attribute\"],             ),             Field(                 name=\"binary_embedding\",                 type=\"tensor(patch{}, v[16])\",                 indexing=[\"attribute\"],             ),         ]     ), ) In\u00a0[15]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"visionragtest\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name, schema=[colpali_schema]\n)\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"visionragtest\" vespa_application_package = ApplicationPackage(     name=vespa_app_name, schema=[colpali_schema] ) <p>Now we define how we want to rank the pages. We have 4 ranking models that we want to evaluate. These are all MaxSim variants but with various precision trade-offs.</p> <ol> <li>float-float A regular MaxSim implementation that uses the float representation of both query and page embeddings.</li> <li>float-binary Use the binarized representation of the page embeddings and where we unpack it into float representation. The query representation is still float.</li> <li>binary-binary Use the binarized representation of the doc embeddings and the query embeddings and replaces the dot product with inverted hamming distance.</li> <li>phased This uses the binary-binary in a first-phase, and then re-ranks using the float-binary representation. Only top 20 pages are re-ranked (This can be overriden in the query request as well).</li> </ol> In\u00a0[17]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\ncolpali_profile = RankProfile(\n    name=\"float-float\",\n    # We define both the float and binary query inputs here; the rest of the profiles inherit these inputs\n    inputs=[\n        (\"query(qtb)\", \"tensor&lt;int8&gt;(querytoken{}, v[16])\"),\n        (\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\"),\n    ],\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * cell_cast(attribute(embedding), float), v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim\"),\n)\n\ncolpali_binary_profile = RankProfile(\n    name=\"float-binary\",\n    inherits=\"float-float\",\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(binary_embedding)), v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim\"),\n)\n\ncolpali_hamming_profile = RankProfile(\n    name=\"binary-binary\",\n    inherits=\"float-float\",\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        1/(1+ sum(\n                            hamming(query(qtb), attribute(binary_embedding)),v\n                        )),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim\"),\n)\n\ncolpali__phased_hamming_profile = RankProfile(\n    name=\"phased\",\n    inherits=\"float-float\",\n    functions=[\n        Function(\n            name=\"max_sim_hamming\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        1/(1+ sum(\n                            hamming(query(qtb), attribute(binary_embedding)),v\n                        )),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(binary_embedding)), v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim_hamming\"),\n    second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=20),\n)\n\n\ncolpali_schema.add_rank_profile(colpali_profile)\ncolpali_schema.add_rank_profile(colpali_binary_profile)\ncolpali_schema.add_rank_profile(colpali_hamming_profile)\ncolpali_schema.add_rank_profile(colpali__phased_hamming_profile)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  colpali_profile = RankProfile(     name=\"float-float\",     # We define both the float and binary query inputs here; the rest of the profiles inherit these inputs     inputs=[         (\"query(qtb)\", \"tensor(querytoken{}, v[16])\"),         (\"query(qt)\", \"tensor(querytoken{}, v[128])\"),     ],     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * cell_cast(attribute(embedding), float), v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         )     ],     first_phase=FirstPhaseRanking(expression=\"max_sim\"), )  colpali_binary_profile = RankProfile(     name=\"float-binary\",     inherits=\"float-float\",     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(binary_embedding)), v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         )     ],     first_phase=FirstPhaseRanking(expression=\"max_sim\"), )  colpali_hamming_profile = RankProfile(     name=\"binary-binary\",     inherits=\"float-float\",     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         1/(1+ sum(                             hamming(query(qtb), attribute(binary_embedding)),v                         )),                         max, patch                     ),                     querytoken                 )             \"\"\",         )     ],     first_phase=FirstPhaseRanking(expression=\"max_sim\"), )  colpali__phased_hamming_profile = RankProfile(     name=\"phased\",     inherits=\"float-float\",     functions=[         Function(             name=\"max_sim_hamming\",             expression=\"\"\"                 sum(                     reduce(                         1/(1+ sum(                             hamming(query(qtb), attribute(binary_embedding)),v                         )),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(binary_embedding)), v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),     ],     first_phase=FirstPhaseRanking(expression=\"max_sim_hamming\"),     second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=20), )   colpali_schema.add_rank_profile(colpali_profile) colpali_schema.add_rank_profile(colpali_binary_profile) colpali_schema.add_rank_profile(colpali_hamming_profile) colpali_schema.add_rank_profile(colpali__phased_hamming_profile) <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial. Make note of the tenant name, it is used in the next steps.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <p>This example uses the asynchronous feed method and feeds one document at a time.</p> In\u00a0[23]: Copied! <pre>from vespa.io import VespaResponse\n\nasync with app.asyncio(connections=1, timeout=180) as session:\n    for doc in tqdm(vespa_docs):\n        response: VespaResponse = await session.feed_data_point(\n            data_id=doc[\"id\"], fields=doc, schema=\"pdf_page\"\n        )\n        if not response.is_successful():\n            print(response.json())\n</pre> from vespa.io import VespaResponse  async with app.asyncio(connections=1, timeout=180) as session:     for doc in tqdm(vespa_docs):         response: VespaResponse = await session.feed_data_point(             data_id=doc[\"id\"], fields=doc, schema=\"pdf_page\"         )         if not response.is_successful():             print(response.json()) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [01:13&lt;00:00,  6.77it/s]\n</pre> <p>We use ir_measures to evaluate the effectiveness of the retrieval model.</p> In\u00a0[24]: Copied! <pre>from ir_measures import calc_aggregate, nDCG, ScoredDoc, Qrel\n</pre> from ir_measures import calc_aggregate, nDCG, ScoredDoc, Qrel <p>A simple routine for querying Vespa. Note that we send both vector representations in the query independently of the ranking method used, this for simplicity. Not all the ranking models we evaluate need both representations.</p> In\u00a0[32]: Copied! <pre>from vespa.io import VespaQueryResponse\nfrom vespa.application import VespaAsync\n\n\nasync def get_vespa_response(\n    embedding: torch.Tensor,\n    qid: str,\n    session: VespaAsync,\n    depth=20,\n    profile=\"float-float\",\n) -&gt; List[ScoredDoc]:\n    # The query tensor api does not support hex formats yet\n    float_embedding = {index: vector.tolist() for index, vector in enumerate(embedding)}\n    binary_embedding = {\n        index: np.packbits(np.where(vector &gt; 0, 1, 0), axis=0).astype(np.int8).tolist()\n        for index, vector in enumerate(embedding)\n    }\n    response: VespaQueryResponse = await session.query(\n        yql=\"select id from pdf_page where true\",  # brute force search, rank all pages\n        ranking=profile,\n        hits=5,\n        timeout=10,\n        body={\n            \"input.query(qt)\": float_embedding,\n            \"input.query(qtb)\": binary_embedding,\n            \"ranking.rerankCount\": depth,\n        },\n    )\n    assert response.is_successful()\n    scored_docs = []\n    for hit in response.hits:\n        doc_id = hit[\"fields\"][\"id\"]\n        score = hit[\"relevance\"]\n        scored_docs.append(ScoredDoc(qid, doc_id, score))\n    return scored_docs\n</pre> from vespa.io import VespaQueryResponse from vespa.application import VespaAsync   async def get_vespa_response(     embedding: torch.Tensor,     qid: str,     session: VespaAsync,     depth=20,     profile=\"float-float\", ) -&gt; List[ScoredDoc]:     # The query tensor api does not support hex formats yet     float_embedding = {index: vector.tolist() for index, vector in enumerate(embedding)}     binary_embedding = {         index: np.packbits(np.where(vector &gt; 0, 1, 0), axis=0).astype(np.int8).tolist()         for index, vector in enumerate(embedding)     }     response: VespaQueryResponse = await session.query(         yql=\"select id from pdf_page where true\",  # brute force search, rank all pages         ranking=profile,         hits=5,         timeout=10,         body={             \"input.query(qt)\": float_embedding,             \"input.query(qtb)\": binary_embedding,             \"ranking.rerankCount\": depth,         },     )     assert response.is_successful()     scored_docs = []     for hit in response.hits:         doc_id = hit[\"fields\"][\"id\"]         score = hit[\"relevance\"]         scored_docs.append(ScoredDoc(qid, doc_id, score))     return scored_docs <p>Run a test query first..</p> In\u00a0[28]: Copied! <pre>async with app.asyncio() as session:\n    for profile in [\"float-float\", \"float-binary\", \"binary-binary\", \"phased\"]:\n        print(\n            await get_vespa_response(\n                query_embeddings[0], profile, session, profile=profile\n            )\n        )\n</pre> async with app.asyncio() as session:     for profile in [\"float-float\", \"float-binary\", \"binary-binary\", \"phased\"]:         print(             await get_vespa_response(                 query_embeddings[0], profile, session, profile=profile             )         ) <pre>[ScoredDoc(query_id='float-float', doc_id='4720', score=16.292504370212555), ScoredDoc(query_id='float-float', doc_id='4858', score=13.315170526504517), ScoredDoc(query_id='float-float', doc_id='14686', score=12.212152108550072), ScoredDoc(query_id='float-float', doc_id='4846', score=12.002869427204132), ScoredDoc(query_id='float-float', doc_id='864', score=11.308563649654388)]\n[ScoredDoc(query_id='float-binary', doc_id='4720', score=82.99432492256165), ScoredDoc(query_id='float-binary', doc_id='4858', score=71.45464742183685), ScoredDoc(query_id='float-binary', doc_id='14686', score=68.46699643135071), ScoredDoc(query_id='float-binary', doc_id='4846', score=64.85357594490051), ScoredDoc(query_id='float-binary', doc_id='2161', score=63.85516130924225)]\n[ScoredDoc(query_id='binary-binary', doc_id='4720', score=0.771387243643403), ScoredDoc(query_id='binary-binary', doc_id='4858', score=0.7132036704570055), ScoredDoc(query_id='binary-binary', doc_id='14686', score=0.6979007869958878), ScoredDoc(query_id='binary-binary', doc_id='6087', score=0.6534321829676628), ScoredDoc(query_id='binary-binary', doc_id='2161', score=0.6525899451225996)]\n[ScoredDoc(query_id='phased', doc_id='4720', score=82.99432492256165), ScoredDoc(query_id='phased', doc_id='4858', score=71.45464742183685), ScoredDoc(query_id='phased', doc_id='14686', score=68.46699643135071), ScoredDoc(query_id='phased', doc_id='4846', score=64.85357594490051), ScoredDoc(query_id='phased', doc_id='2161', score=63.85516130924225)]\n</pre> <p>Now, run through all of the test queries for each of the ranking models.</p> In\u00a0[29]: Copied! <pre>qrels = []\nprofiles = [\"float-float\", \"float-binary\", \"binary-binary\", \"phased\"]\nresults = {profile: [] for profile in profiles}\nasync with app.asyncio(connections=3) as session:\n    for row, embedding in zip(tqdm(ds), query_embeddings):\n        qrels.append(Qrel(row[\"questionId\"], str(row[\"docId\"]), 1))\n        for profile in profiles:\n            scored_docs = await get_vespa_response(\n                embedding, row[\"questionId\"], session, profile=profile\n            )\n            results[profile].extend(scored_docs)\n</pre> qrels = [] profiles = [\"float-float\", \"float-binary\", \"binary-binary\", \"phased\"] results = {profile: [] for profile in profiles} async with app.asyncio(connections=3) as session:     for row, embedding in zip(tqdm(ds), query_embeddings):         qrels.append(Qrel(row[\"questionId\"], str(row[\"docId\"]), 1))         for profile in profiles:             scored_docs = await get_vespa_response(                 embedding, row[\"questionId\"], session, profile=profile             )             results[profile].extend(scored_docs) <pre>500it [11:32,  1.39s/it]\n</pre> <p>Calculate the effectiveness of the 4 different models</p> In\u00a0[30]: Copied! <pre>for profile in profiles:\n    score = calc_aggregate([nDCG @ 5], qrels, results[profile])[nDCG @ 5]\n    print(f\"nDCG@5 for {profile}: {100*score:.2f}\")\n</pre> for profile in profiles:     score = calc_aggregate([nDCG @ 5], qrels, results[profile])[nDCG @ 5]     print(f\"nDCG@5 for {profile}: {100*score:.2f}\") <pre>nDCG@5 for float-float: 52.37\nnDCG@5 for float-binary: 51.64\nnDCG@5 for binary-binary: 49.48\nnDCG@5 for phased: 51.70\n</pre> <p>This is encouraging as the binary-binary representation is 4x faster than the float-float representation and saves 32x space. We can also largely retain the effectiveness of the float-binary representation by using the phased approach, where we re-rank the top 20 pages from the hamming (binary-binary) version using the float-binary representation. Now we can explore the ranking depth and see how the phased approach performs with different ranking depths.</p> In\u00a0[35]: Copied! <pre>results = {\n    profile: []\n    for profile in [\n        \"phased-rerank-count=5\",\n        \"phased-rerank-count=10\",\n        \"phased-rerank-count=20\",\n        \"phased-rerank-count=40\",\n    ]\n}\nasync with app.asyncio(connections=3) as session:\n    for row, embedding in zip(tqdm(ds), query_embeddings):\n        qrels.append(Qrel(row[\"questionId\"], str(row[\"docId\"]), 1))\n        for count in [5, 10, 20, 40]:\n            scored_docs = await get_vespa_response(\n                embedding, row[\"questionId\"], session, profile=\"phased\", depth=count\n            )\n            results[\"phased-rerank-count=\" + str(count)].extend(scored_docs)\n</pre> results = {     profile: []     for profile in [         \"phased-rerank-count=5\",         \"phased-rerank-count=10\",         \"phased-rerank-count=20\",         \"phased-rerank-count=40\",     ] } async with app.asyncio(connections=3) as session:     for row, embedding in zip(tqdm(ds), query_embeddings):         qrels.append(Qrel(row[\"questionId\"], str(row[\"docId\"]), 1))         for count in [5, 10, 20, 40]:             scored_docs = await get_vespa_response(                 embedding, row[\"questionId\"], session, profile=\"phased\", depth=count             )             results[\"phased-rerank-count=\" + str(count)].extend(scored_docs) <pre>500it [08:18,  1.00it/s]\n</pre> In\u00a0[36]: Copied! <pre>for profile in results.keys():\n    score = calc_aggregate([nDCG @ 5], qrels, results[profile])[nDCG @ 5]\n    print(f\"nDCG@5 for {profile}: {100*score:.2f}\")\n</pre> for profile in results.keys():     score = calc_aggregate([nDCG @ 5], qrels, results[profile])[nDCG @ 5]     print(f\"nDCG@5 for {profile}: {100*score:.2f}\") <pre>nDCG@5 for phased-rerank-count=5: 50.77\nnDCG@5 for phased-rerank-count=10: 51.58\nnDCG@5 for phased-rerank-count=20: 51.70\nnDCG@5 for phased-rerank-count=40: 51.64\n</pre>"},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#colpali-ranking-experiments-on-docvqa","title":"ColPali Ranking Experiments on DocVQA\u00b6","text":"<p>This notebook demonstrates how to reproduce the ColPali results on DocVQA with Vespa. The dataset consists of PDF documents with questions and answers.</p> <p>We demonstrate how we can binarize the patch embeddings and replace the float MaxSim scoring with a <code>hamming</code> based MaxSim without much loss in ranking accuracy but with a significant speedup (close to 4x) and reducing the memory (and storage) requirements by 32x.</p> <p>In this notebook, we represent one PDF page as one vespa document. See other notebooks for more information about using ColPali with Vespa:</p> <ul> <li>Scaling ColPALI (VLM) Retrieval</li> <li>Vespa \ud83e\udd1d ColPali: Efficient Document Retrieval with Vision Language Models</li> </ul> <p></p> <p>Install dependencies:</p>"},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#load-the-model","title":"Load the model\u00b6","text":"<p>Load the model, also choose the correct device and model weights.</p>"},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#the-vidore-benchmark","title":"The ViDoRe Benchmark\u00b6","text":"<p>We load the DocVQA test set, a subset of the ViDoRe dataset It has 500 pages and a question per page. The task is retrieve the page across the 500 indexed pages.</p>"},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#patch-vector-pooling","title":"Patch Vector pooling\u00b6","text":"<p>This reduces the number of patch embeddings by a factor of 3, meaning that we go from 1030 patch vectors to 343 patch vectors. This reduces both the memory and the number of dotproducts we need to calculate. This function is not in use in this notebook, but it is included for reference.</p>"},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#configure-vespa","title":"Configure Vespa\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type. This is a simple schema, which is all we need to evaluate the effectiveness of the model.</p>"},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#deploy-to-vespa-cloud","title":"Deploy to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p><code>PyVespa</code> supports deploying apps to the development zone.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#run-queries-and-evaluate-effectiveness","title":"Run queries and evaluate effectiveness\u00b6","text":""},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#conclusion","title":"Conclusion\u00b6","text":"<p>The binary representation of the patch embeddings reduces the storage by 32x, and using hamming distance instead of dotproduct saves us about 4x in computation compared to the float-float model or the float-binary model (which only saves storage). Using a re-ranking step with only depth 10, we can improve the effectiveness of the binary-binary model to almost match the float-float MaxSim model. The additional re-ranking step only requires that we pass also the float query embedding version without any additional storage overhead.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html","title":"Colpali document retrieval vision language models cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y\n</pre> !sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y <p>Install python packages</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install colpali-engine==0.3.1 vidore_benchmark==4.0.0 pdf2image google-generativeai pypdf==5.0.1 pyvespa vespacli requests\n</pre> !pip3 install colpali-engine==0.3.1 vidore_benchmark==4.0.0 pdf2image google-generativeai pypdf==5.0.1 pyvespa vespacli requests In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom colpali_engine.models import ColPali, ColPaliProcessor\nfrom colpali_engine.utils.torch_utils import get_torch_device\nfrom vidore_benchmark.utils.image_utils import scale_image, get_base64_image\n</pre> import torch from torch.utils.data import DataLoader from tqdm import tqdm from PIL import Image from io import BytesIO  from colpali_engine.models import ColPali, ColPaliProcessor from colpali_engine.utils.torch_utils import get_torch_device from vidore_benchmark.utils.image_utils import scale_image, get_base64_image <p>Choose the right device to run the model.</p> In\u00a0[\u00a0]: Copied! <pre>device = get_torch_device(\"auto\")\nif device == \"cuda\":\n    dtype = torch.bfloat16\nelse:\n    dtype = torch.float32\n</pre> device = get_torch_device(\"auto\") if device == \"cuda\":     dtype = torch.bfloat16 else:     dtype = torch.float32 In\u00a0[\u00a0]: Copied! <pre>model_name = \"vidore/colpali-v1.2\"\nmodel = ColPali.from_pretrained(model_name, torch_dtype=dtype, device_map=device).eval()\nprocessor = ColPaliProcessor.from_pretrained(model_name)\n</pre> model_name = \"vidore/colpali-v1.2\" model = ColPali.from_pretrained(model_name, torch_dtype=dtype, device_map=device).eval() processor = ColPaliProcessor.from_pretrained(model_name) In\u00a0[\u00a0]: Copied! <pre>import requests\nfrom pdf2image import convert_from_path\nfrom pypdf import PdfReader\n\n\ndef download_pdf(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return BytesIO(response.content)\n    else:\n        raise Exception(f\"Failed to download PDF: Status code {response.status_code}\")\n\n\ndef get_pdf_images(pdf_url):\n    # Download the PDF\n    pdf_file = download_pdf(pdf_url)\n    # Save the PDF temporarily to disk (pdf2image requires a file path)\n    with open(\"temp.pdf\", \"wb\") as f:\n        f.write(pdf_file.read())\n    reader = PdfReader(\"temp.pdf\")\n    page_texts = []\n    for page_number in range(len(reader.pages)):\n        page = reader.pages[page_number]\n        text = page.extract_text()\n        page_texts.append(text)\n    images = convert_from_path(\"temp.pdf\")\n    assert len(images) == len(page_texts)\n    return (images, page_texts)\n</pre> import requests from pdf2image import convert_from_path from pypdf import PdfReader   def download_pdf(url):     response = requests.get(url)     if response.status_code == 200:         return BytesIO(response.content)     else:         raise Exception(f\"Failed to download PDF: Status code {response.status_code}\")   def get_pdf_images(pdf_url):     # Download the PDF     pdf_file = download_pdf(pdf_url)     # Save the PDF temporarily to disk (pdf2image requires a file path)     with open(\"temp.pdf\", \"wb\") as f:         f.write(pdf_file.read())     reader = PdfReader(\"temp.pdf\")     page_texts = []     for page_number in range(len(reader.pages)):         page = reader.pages[page_number]         text = page.extract_text()         page_texts.append(text)     images = convert_from_path(\"temp.pdf\")     assert len(images) == len(page_texts)     return (images, page_texts) <p>We define a few sample PDFs to work with.</p> In\u00a0[\u00a0]: Copied! <pre>sample_pdfs = [\n    {\n        \"title\": \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\",\n        \"url\": \"https://arxiv.org/pdf/2112.01488.pdf\",\n        \"authors\": \"Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia\",\n    },\n    {\n        \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",\n        \"url\": \"https://arxiv.org/pdf/2004.12832.pdf\",\n        \"authors\": \"Omar Khattab, Matei Zaharia\",\n    },\n]\n</pre> sample_pdfs = [     {         \"title\": \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\",         \"url\": \"https://arxiv.org/pdf/2112.01488.pdf\",         \"authors\": \"Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia\",     },     {         \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",         \"url\": \"https://arxiv.org/pdf/2004.12832.pdf\",         \"authors\": \"Omar Khattab, Matei Zaharia\",     }, ] <p>Now we can convert the PDFs to images and also extract the text content.</p> In\u00a0[\u00a0]: Copied! <pre>for pdf in sample_pdfs:\n    page_images, page_texts = get_pdf_images(pdf[\"url\"])\n    pdf[\"images\"] = page_images\n    pdf[\"texts\"] = page_texts\n</pre> for pdf in sample_pdfs:     page_images, page_texts = get_pdf_images(pdf[\"url\"])     pdf[\"images\"] = page_images     pdf[\"texts\"] = page_texts <p>Let us look at the extracted image of the first PDF page. This is the input to ColPali.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display\n\ndisplay(scale_image(sample_pdfs[0][\"images\"][0], 720))\n</pre> from IPython.display import display  display(scale_image(sample_pdfs[0][\"images\"][0], 720)) <p>Now we use the ColPali model to generate embeddings for the images.</p> In\u00a0[\u00a0]: Copied! <pre>for pdf in sample_pdfs:\n    page_embeddings = []\n    dataloader = DataLoader(\n        pdf[\"images\"],\n        batch_size=2,\n        shuffle=False,\n        collate_fn=lambda x: processor.process_images(x),\n    )\n    for batch_doc in tqdm(dataloader):\n        with torch.no_grad():\n            batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\n            embeddings_doc = model(**batch_doc)\n            # also handle if running on GPU\n            if model.device == \"cuda\":\n                embeddings_doc = embeddings_doc.float()\n            page_embeddings.extend(list(torch.unbind(embeddings_doc.cpu())))\n    pdf[\"embeddings\"] = page_embeddings\n</pre> for pdf in sample_pdfs:     page_embeddings = []     dataloader = DataLoader(         pdf[\"images\"],         batch_size=2,         shuffle=False,         collate_fn=lambda x: processor.process_images(x),     )     for batch_doc in tqdm(dataloader):         with torch.no_grad():             batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}             embeddings_doc = model(**batch_doc)             # also handle if running on GPU             if model.device == \"cuda\":                 embeddings_doc = embeddings_doc.float()             page_embeddings.extend(list(torch.unbind(embeddings_doc.cpu())))     pdf[\"embeddings\"] = page_embeddings <p>Now we are done with the document side embeddings, we now convert the custom dict to Vespa JSON feed format.</p> <p>We use binarization of the vector embeddings to reduce their size. Read more about binarization of multi-vector representations in the colbert blog post. This maps 128 dimensional floats to 128 bits, or 16 bytes per vector. Reducing the size by 32x.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom typing import Dict, List\nfrom binascii import hexlify\n\n\ndef binarize_token_vectors_hex(vectors: List[torch.Tensor]) -&gt; Dict[str, str]:\n    vespa_tensor = list()\n    for page_id in range(0, len(vectors)):\n        page_vector = vectors[page_id]\n        binarized_token_vectors = np.packbits(\n            np.where(page_vector &gt; 0, 1, 0), axis=1\n        ).astype(np.int8)\n        for patch_index in range(0, len(page_vector)):\n            values = str(\n                hexlify(binarized_token_vectors[patch_index].tobytes()), \"utf-8\"\n            )\n            if (\n                values == \"00000000000000000000000000000000\"\n            ):  # skip empty vectors due to padding of batch\n                continue\n            vespa_tensor_cell = {\n                \"address\": {\"page\": page_id, \"patch\": patch_index},\n                \"values\": values,\n            }\n            vespa_tensor.append(vespa_tensor_cell)\n\n    return vespa_tensor\n</pre> import numpy as np from typing import Dict, List from binascii import hexlify   def binarize_token_vectors_hex(vectors: List[torch.Tensor]) -&gt; Dict[str, str]:     vespa_tensor = list()     for page_id in range(0, len(vectors)):         page_vector = vectors[page_id]         binarized_token_vectors = np.packbits(             np.where(page_vector &gt; 0, 1, 0), axis=1         ).astype(np.int8)         for patch_index in range(0, len(page_vector)):             values = str(                 hexlify(binarized_token_vectors[patch_index].tobytes()), \"utf-8\"             )             if (                 values == \"00000000000000000000000000000000\"             ):  # skip empty vectors due to padding of batch                 continue             vespa_tensor_cell = {                 \"address\": {\"page\": page_id, \"patch\": patch_index},                 \"values\": values,             }             vespa_tensor.append(vespa_tensor_cell)      return vespa_tensor <p>Iterate over the sample and create the Vespa JSON feed format, including the base64 encoded page images.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_feed = []\nfor idx, pdf in enumerate(sample_pdfs):\n    images_base_64 = []\n    for image in pdf[\"images\"]:\n        images_base_64.append(get_base64_image(image, add_url_prefix=False))\n    pdf[\"images_base_64\"] = images_base_64\n    doc = {\n        \"fields\": {\n            \"url\": pdf[\"url\"],\n            \"title\": pdf[\"title\"],\n            \"images\": pdf[\"images_base_64\"],\n            \"texts\": pdf[\"texts\"],  # Array of text per page\n            \"colbert\": {  # Colbert embeddings per page\n                \"blocks\": binarize_token_vectors_hex(pdf[\"embeddings\"])\n            },\n        }\n    }\n    vespa_feed.append(doc)\n</pre> vespa_feed = [] for idx, pdf in enumerate(sample_pdfs):     images_base_64 = []     for image in pdf[\"images\"]:         images_base_64.append(get_base64_image(image, add_url_prefix=False))     pdf[\"images_base_64\"] = images_base_64     doc = {         \"fields\": {             \"url\": pdf[\"url\"],             \"title\": pdf[\"title\"],             \"images\": pdf[\"images_base_64\"],             \"texts\": pdf[\"texts\"],  # Array of text per page             \"colbert\": {  # Colbert embeddings per page                 \"blocks\": binarize_token_vectors_hex(pdf[\"embeddings\"])             },         }     }     vespa_feed.append(doc) In\u00a0[\u00a0]: Copied! <pre>vespa_feed[0][\"fields\"][\"colbert\"][\"blocks\"][0:5]\n</pre> vespa_feed[0][\"fields\"][\"colbert\"][\"blocks\"][0:5] <p>Above is the feed format for mixed tensors with more than one mapped dimension, see details. We have the <code>page</code> and <code>patch</code> dimensions and for each combination with have a binary representation of the 128 dimensional embeddings, packed into 16 bytes.</p> <p>For each page image, we have 1030 patches, each with a 128 dimensional embedding.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\ncolbert_schema = Schema(\n    name=\"doc\",\n    document=Document(\n        fields=[\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"texts\",\n                type=\"array&lt;string&gt;\",\n                indexing=[\"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"images\",\n                type=\"array&lt;string&gt;\",\n                indexing=[\"summary\"],\n            ),\n            Field(\n                name=\"colbert\",\n                type=\"tensor&lt;int8&gt;(page{}, patch{}, v[16])\",\n                indexing=[\"attribute\"],\n            ),\n        ]\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"texts\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  colbert_schema = Schema(     name=\"doc\",     document=Document(         fields=[             Field(name=\"url\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"texts\",                 type=\"array\",                 indexing=[\"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"images\",                 type=\"array\",                 indexing=[\"summary\"],             ),             Field(                 name=\"colbert\",                 type=\"tensor(page{}, patch{}, v[16])\",                 indexing=[\"attribute\"],             ),         ]     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"texts\"])], ) <p>Notice the <code>colbert</code> field is a tensor field with the type <code>tensor(page{}, patch{}, v[128])</code>. This is the field that will store the embeddings generated by ColPali. This is an example of a mixed tensor where we combine two mapped (sparse) dimensions with one dense.</p> <p>Read more in Tensor guide. We also enable BM25 for the <code>title</code> and <code>texts</code>\u00a0fields.</p> <p>Create the Vespa application package:</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"visionrag\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name, schema=[colbert_schema]\n)\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"visionrag\" vespa_application_package = ApplicationPackage(     name=vespa_app_name, schema=[colbert_schema] ) <p>Now we define how we want to rank the pages. We use BM25 for the text and late interaction with Max Sim for the image embeddings. This means that we retrieve using the text representations to find relevant PDF documents, then we use the ColPALI embeddings to rerank the pages within the document using the max of the page scores.</p> <p>We also return all the page level scores using <code>match-features</code>,  so that we can render multiple scoring pages in the search result.</p> <p>As LLMs gets longer context windows, we can input more than a single page per PDF.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\ncolbert_profile = RankProfile(\n    name=\"default\",\n    inputs=[(\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\")],\n    functions=[\n        Function(\n            name=\"max_sim_per_page\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(colbert)) , v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(name=\"max_sim\", expression=\"reduce(max_sim_per_page, max, page)\"),\n        Function(name=\"bm25_score\", expression=\"bm25(title) + bm25(texts)\"),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"bm25_score\"),\n    second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10),\n    match_features=[\"max_sim_per_page\", \"bm25_score\"],\n)\ncolbert_schema.add_rank_profile(colbert_profile)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  colbert_profile = RankProfile(     name=\"default\",     inputs=[(\"query(qt)\", \"tensor(querytoken{}, v[128])\")],     functions=[         Function(             name=\"max_sim_per_page\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(colbert)) , v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(name=\"max_sim\", expression=\"reduce(max_sim_per_page, max, page)\"),         Function(name=\"bm25_score\", expression=\"bm25(title) + bm25(texts)\"),     ],     first_phase=FirstPhaseRanking(expression=\"bm25_score\"),     second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10),     match_features=[\"max_sim_per_page\", \"bm25_score\"], ) colbert_schema.add_rank_profile(colbert_profile) <p>Validate that certificates are ok and deploy the application to Vespa Cloud.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <p>This example uses the synchronous feed method and feeds one document at a time. For larger datasets, consider using the asynchronous feed method.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaResponse\n\nwith app.syncio() as sync:\n    for operation in vespa_feed:\n        fields = operation[\"fields\"]\n        response: VespaResponse = sync.feed_data_point(\n            data_id=fields[\"url\"], fields=fields, schema=\"doc\"\n        )\n        if not response.is_successful():\n            print(response.json())\n</pre> from vespa.io import VespaResponse  with app.syncio() as sync:     for operation in vespa_feed:         fields = operation[\"fields\"]         response: VespaResponse = sync.feed_data_point(             data_id=fields[\"url\"], fields=fields, schema=\"doc\"         )         if not response.is_successful():             print(response.json()) <p>Our demo query:</p> <p>Composition of the Lotte Benchmark</p> In\u00a0[\u00a0]: Copied! <pre>queries = [\"Composition of the LoTTE benchmark\"]\n</pre> queries = [\"Composition of the LoTTE benchmark\"] <p>Obtain the query embeddings using the ColPali model</p> In\u00a0[\u00a0]: Copied! <pre>dataloader = DataLoader(\n    queries,\n    batch_size=1,\n    shuffle=False,\n    collate_fn=lambda x: processor.process_queries(x),\n)\nqs = []\nfor batch_query in dataloader:\n    with torch.no_grad():\n        batch_query = {k: v.to(model.device) for k, v in batch_query.items()}\n        embeddings_query = model(**batch_query)\n        # also handle if running on GPU\n        if model.device == \"cuda\":\n            embeddings_query = embeddings_query.float()\n        qs.extend(list(torch.unbind(embeddings_query.cpu())))\n</pre> dataloader = DataLoader(     queries,     batch_size=1,     shuffle=False,     collate_fn=lambda x: processor.process_queries(x), ) qs = [] for batch_query in dataloader:     with torch.no_grad():         batch_query = {k: v.to(model.device) for k, v in batch_query.items()}         embeddings_query = model(**batch_query)         # also handle if running on GPU         if model.device == \"cuda\":             embeddings_query = embeddings_query.float()         qs.extend(list(torch.unbind(embeddings_query.cpu()))) <p>A simple routine to format the ColPali multi-vector emebeddings to a format that can be used in Vespa. See querying with tensors for more details.</p> In\u00a0[\u00a0]: Copied! <pre>def float_query_token_vectors(vectors: torch.Tensor) -&gt; Dict[str, List[float]]:\n    vespa_token_dict = dict()\n    for index in range(0, len(vectors)):\n        vespa_token_dict[index] = vectors[index].tolist()\n    return vespa_token_dict\n</pre> def float_query_token_vectors(vectors: torch.Tensor) -&gt; Dict[str, List[float]]:     vespa_token_dict = dict()     for index in range(0, len(vectors)):         vespa_token_dict[index] = vectors[index].tolist()     return vespa_token_dict <p>We create a simple routine to display the results.</p> <p>Notice that each hit is a PDF document. Within a PDF document we have multiple pages and we have the MaxSim score for each page.</p> <p>The PDF documents are ranked by the maximum page score. But, we have access to all the page level scores and below we display the top 2-pages for each PDF document. We convert the base64 encoded image to a PIL image for rendering. We could also render the extracted text, but we skip that for now.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display, HTML\nimport base64\n\n\ndef display_query_results(query, response):\n    \"\"\"\n    Displays the query result, including the two best matching pages per matched pdf.\n    \"\"\"\n    html_content = f\"&lt;h3&gt;Query text: {query}&lt;/h3&gt;\"\n\n    for i, hit in enumerate(response.hits[:2]):  # Adjust to show more hits if needed\n        title = hit[\"fields\"][\"title\"]\n        url = hit[\"fields\"][\"url\"]\n        match_scores = hit[\"fields\"][\"matchfeatures\"][\"max_sim_per_page\"]\n        images = hit[\"fields\"][\"images\"]\n\n        html_content += f\"&lt;h3&gt;PDF Result {i + 1}&lt;/h3&gt;\"\n        html_content += f'&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; &lt;a href=\"{url}\"&gt;{title}&lt;/a&gt;&lt;/p&gt;'\n\n        # Find the two best matching pages\n        sorted_pages = sorted(match_scores.items(), key=lambda x: x[1], reverse=True)\n        best_pages = sorted_pages[:2]\n\n        for page, score in best_pages:\n            page = int(page)\n            image_data = base64.b64decode(images[page])\n            image = Image.open(BytesIO(image_data))\n            scaled_image = scale_image(image, 648)\n\n            buffered = BytesIO()\n            scaled_image.save(buffered, format=\"PNG\")\n            img_str = base64.b64encode(buffered.getvalue()).decode()\n\n            html_content += f\"&lt;p&gt;&lt;strong&gt;Best Matching Page {page+1} for PDF document:&lt;/strong&gt; with MaxSim score {score:.2f}&lt;/p&gt;\"\n            html_content += (\n                f'&lt;img src=\"data:image/png;base64,{img_str}\" style=\"max-width:100%;\"&gt;'\n            )\n\n    display(HTML(html_content))\n</pre> from IPython.display import display, HTML import base64   def display_query_results(query, response):     \"\"\"     Displays the query result, including the two best matching pages per matched pdf.     \"\"\"     html_content = f\"Query text: {query}\"      for i, hit in enumerate(response.hits[:2]):  # Adjust to show more hits if needed         title = hit[\"fields\"][\"title\"]         url = hit[\"fields\"][\"url\"]         match_scores = hit[\"fields\"][\"matchfeatures\"][\"max_sim_per_page\"]         images = hit[\"fields\"][\"images\"]          html_content += f\"PDF Result {i + 1}\"         html_content += f'<p>Title: {title}</p>'          # Find the two best matching pages         sorted_pages = sorted(match_scores.items(), key=lambda x: x[1], reverse=True)         best_pages = sorted_pages[:2]          for page, score in best_pages:             page = int(page)             image_data = base64.b64decode(images[page])             image = Image.open(BytesIO(image_data))             scaled_image = scale_image(image, 648)              buffered = BytesIO()             scaled_image.save(buffered, format=\"PNG\")             img_str = base64.b64encode(buffered.getvalue()).decode()              html_content += f\"<p>Best Matching Page {page+1} for PDF document: with MaxSim score {score:.2f}</p>\"             html_content += (                 f''             )      display(HTML(html_content)) <p>Query Vespa with a text query and display the results.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\n\nfor idx, query in enumerate(queries):\n    response: VespaQueryResponse = app.query(\n        yql=\"select title,url,images from doc where userInput(@userQuery)\",\n        ranking=\"default\",\n        userQuery=query,\n        timeout=2,\n        hits=3,\n        body={\n            \"presentation.format.tensors\": \"short-value\",\n            \"input.query(qt)\": float_query_token_vectors(qs[idx]),\n        },\n    )\n    assert response.is_successful()\n    display_query_results(query, response)\n</pre> from vespa.io import VespaQueryResponse  for idx, query in enumerate(queries):     response: VespaQueryResponse = app.query(         yql=\"select title,url,images from doc where userInput(@userQuery)\",         ranking=\"default\",         userQuery=query,         timeout=2,         hits=3,         body={             \"presentation.format.tensors\": \"short-value\",             \"input.query(qt)\": float_query_token_vectors(qs[idx]),         },     )     assert response.is_successful()     display_query_results(query, response) In\u00a0[\u00a0]: Copied! <pre>import google.generativeai as genai\n\ngenai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n</pre> import google.generativeai as genai  genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"]) <p>Just extract the best page image from the first hit to demonstrate how to use the image with Gemini Flash to answer the question.</p> In\u00a0[\u00a0]: Copied! <pre>best_hit = response.hits[0]\npdf_url = best_hit[\"fields\"][\"url\"]\npdf_title = best_hit[\"fields\"][\"title\"]\nmatch_scores = best_hit[\"fields\"][\"matchfeatures\"][\"max_sim_per_page\"]\nimages = best_hit[\"fields\"][\"images\"]\nsorted_pages = sorted(match_scores.items(), key=lambda x: x[1], reverse=True)\nbest_page, score = sorted_pages[0]\nbest_page = int(best_page)\nimage_data = base64.b64decode(images[best_page])\nimage = Image.open(BytesIO(image_data))\nscaled_image = scale_image(image, 720)\ndisplay(scaled_image)\n</pre> best_hit = response.hits[0] pdf_url = best_hit[\"fields\"][\"url\"] pdf_title = best_hit[\"fields\"][\"title\"] match_scores = best_hit[\"fields\"][\"matchfeatures\"][\"max_sim_per_page\"] images = best_hit[\"fields\"][\"images\"] sorted_pages = sorted(match_scores.items(), key=lambda x: x[1], reverse=True) best_page, score = sorted_pages[0] best_page = int(best_page) image_data = base64.b64decode(images[best_page]) image = Image.open(BytesIO(image_data)) scaled_image = scale_image(image, 720) display(scaled_image) <p>Initialize the Gemini Flash model and answer the question.</p> In\u00a0[\u00a0]: Copied! <pre>model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\nresponse = model.generate_content([queries[0], image])\n</pre> model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\") response = model.generate_content([queries[0], image]) <p>Some formatting of the response from Gemini Flash.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Markdown, display\n\nmarkdown_text = response.candidates[0].content.parts[0].text\ndisplay(Markdown(markdown_text))\n</pre> from IPython.display import Markdown, display  markdown_text = response.candidates[0].content.parts[0].text display(Markdown(markdown_text)) In\u00a0[\u00a0]: Copied! <pre>if os.getenv(\"CI\", \"false\") == \"true\":\n    vespa_cloud.delete()\n</pre> if os.getenv(\"CI\", \"false\") == \"true\":     vespa_cloud.delete()"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#vespa-colpali-efficient-document-retrieval-with-vision-language-models","title":"Vespa \ud83e\udd1d ColPali: Efficient Document Retrieval with Vision Language Models\u00b6","text":"<p>For a simpler example of using ColPali, where we use one Vespa document = One PDF page, see simplified-retrieval-with-colpali.</p> <p>This notebook demonstrates how to represent ColPali in Vespa. ColPali is a powerful visual language model that can generate embeddings for images and text. In this notebook, we will use ColPali to generate embeddings for images of PDF pages and store them in Vespa. We will also store the base64 encoded image of the PDF page and some meta data like title and url. We will then demonstrate how to retrieve the pdf pages using the embeddings generated by ColPali.</p> <p>ColPali: Efficient Document Retrieval with Vision Language Models Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, C\u00e9line Hudelot, Pierre Colombo</p> <p>ColPali is a combination of ColBERT and PaliGemma:</p> <p>ColPali is enabled by the latest advances in Vision Language Models, notably the PaliGemma model from the Google Z\u00fcrich team, and leverages multi-vector retrieval through late interaction mechanisms as proposed in ColBERT by Omar Khattab.</p> <p>Quote from ColPali: Efficient Document Retrieval with Vision Language Models \ud83d\udc40</p> <p></p> <p>The ColPali model achieves remarkable retrieval performance on the ViDoRe (Visual Document Retrieval) Benchmark. Beating complex pipelines with a single model.</p> <p></p> <p>The TLDR of this notebook:</p> <ul> <li>Generate an image per PDF page using pdf2image and also extract the text using pypdf.</li> <li>For each page image, use ColPali to obtain the visual multi-vector embeddings</li> </ul> <p>Then we store colbert embeddings in Vespa and use the long-context variant where we represent the colbert embeddings per document with the tensor <code>tensor(page{}, patch{}, v[128])</code>. This enables us to use the PDF as the document (retrievable unit), storing the page embeddings in the same document.</p> <p>The upside of this is that we do not need to duplicate document level meta data like title, url, etc. But, the downside is that we cannot retrieve using the ColPali embeddings directly, but need to use the extracted text for retrieval. The ColPali embeddings are only used for reranking the results.</p> <p>For a simpler example where we use one vespa document = One PDF page, see simplified-retrieval-with-colpali.</p> <p>Consider following the ColQWen2 notebook instead as it use a better model with improved performance (Both accuracy and speed).</p> <p>We also store the base64 encoded image, and page meta data like title and url so that we can display it in the result page, but also use it for RAG with powerful LLMs with vision capabilities.</p> <p>At query time, we retrieve using BM25 over all the text from all pages, then use the ColPali embeddings to rerank the results using the max page score.</p> <p>Let us get started.</p> <p></p> <p>Install dependencies:</p> <p>Note that the python pdf2image package requires poppler-utils, see other installation options here.</p> <p>Install dependencies:</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#load-the-model","title":"Load the model\u00b6","text":"<p>This requires that the HF_TOKEN environment variable is set as the underlaying PaliGemma model is hosted on Hugging Face and has a restricive licence that requires authentication.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#working-with-pdfs","title":"Working with pdfs\u00b6","text":"<p>We need to convert a PDF to an array of images. One image per page. We will use pdf2image for this. Secondary, we also extract the text content of the pdf using pypdf.</p> <p>NOTE: This step requires that you have <code>poppler</code> installed on your system. Read more in pdf2image docs.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#configure-vespa","title":"Configure Vespa\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#deploy-to-vespa-cloud","title":"Deploy to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p><code>PyVespa</code> supports deploying apps to the development zone.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#querying-vespa","title":"Querying Vespa\u00b6","text":"<p>Ok, so now we have indexed the PDF pages in Vespa. Let us now obtain ColPali embeddings for a text query and use it to match against the indexed PDF pages.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#rag-with-llms-with-vision-capabilities","title":"RAG with LLMs with vision capabilities.\u00b6","text":"<p>Now we can use the top k documents to answer the question using a LLM with vision capabilities.</p> <p>This then becomes an end-to-end pipeline using vision capable language models, where we use ColPali visual embeddings for retrieval and Gemini Flash to read the retrieved PDF pages and answer the question with that context.</p> <p>We will use the Gemini Flash model for reading and answering.</p> <p>In the following, we input the best matching PDF page image and the question.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#summary","title":"Summary\u00b6","text":"<p>In this notebook, we have demonstrated how to represent ColPali in Vespa. We have used ColPali to generate embeddings for images of pdf pages and stored them in Vespa. We have also stored the base64 encoded image of the pdf page and some meta data like title and url. We have then demonstrated how to retrieve the pdf pages using the embeddings generated by ColPali. We have also demonstrated how to use the top k documents to answer a question using a LLM with vision capabilities.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#cleanup","title":"Cleanup\u00b6","text":"<p>When this notebook is running in CI, we want to delete the application.</p>"},{"location":"examples/cross-encoders-for-global-reranking.html","title":"Cross encoders for global reranking","text":"In\u00a0[1]: Copied! <pre>import requests\nfrom pathlib import Path\n\nurl = \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/resolve/main/onnx/model_quantized.onnx\"\nlocal_model_path = \"model/model_quantized.onnx\"\n\nr = requests.get(url)\n# Create path if it doesn't exist\nPath(local_model_path).parent.mkdir(parents=True, exist_ok=True)\nwith open(local_model_path, \"wb\") as f:\n    f.write(r.content)\n    print(f\"Downloaded model to {local_model_path}\")\n</pre> import requests from pathlib import Path  url = \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/resolve/main/onnx/model_quantized.onnx\" local_model_path = \"model/model_quantized.onnx\"  r = requests.get(url) # Create path if it doesn't exist Path(local_model_path).parent.mkdir(parents=True, exist_ok=True) with open(local_model_path, \"wb\") as f:     f.write(r.content)     print(f\"Downloaded model to {local_model_path}\") <pre>Downloaded model to model/model_quantized.onnx\n</pre> <p>We can see that the input pairs (query, document) are prefixed with a special <code>[CLS]</code> token, and separated by a <code>[SEP]</code> token.</p> <p>In Vespa, we want to tokenize the document body at indexing time, and the query at query time, and then combine them in the same way as the cross-encoder does, during ranking.</p> <p>Let us see how we can achieve this in Vespa.</p> In\u00a0[2]: Copied! <pre>from vespa.package import (\n    Component,\n    Document,\n    Field,\n    FieldSet,\n    Function,\n    GlobalPhaseRanking,\n    OnnxModel,\n    Parameter,\n    RankProfile,\n    Schema,\n)\n\nschema = Schema(\n    name=\"doc\",\n    mode=\"index\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"index\", \"summary\"],\n                index=\"enable-bm25\",\n            ),\n            # Let\u00b4s add a synthetic field (see https://docs.vespa.ai/en/schemas.html#field)\n            # to define how the tokens are derived from the text field\n            Field(\n                name=\"body_tokens\",\n                type=\"tensor&lt;float&gt;(d0[512])\",\n                # The tokenizer will be defined in the next cell\n                indexing=[\"input text\", \"embed tokenizer\", \"attribute\", \"summary\"],\n                is_document_field=False,  # Indicates a synthetic field\n            ),\n        ],\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n    models=[\n        OnnxModel(\n            model_name=\"crossencoder\",\n            model_file_path=f\"{local_model_path}\",\n            inputs={\n                \"input_ids\": \"input_ids\",\n                \"attention_mask\": \"attention_mask\",\n            },\n            outputs={\"logits\": \"logits\"},\n        )\n    ],\n    rank_profiles=[\n        RankProfile(name=\"bm25\", first_phase=\"bm25(text)\"),\n        RankProfile(\n            name=\"reranking\",\n            inherits=\"default\",\n            # We truncate the query to 64 tokens, meaning we have 512-64=448 tokens left for the document.\n            inputs=[(\"query(q)\", \"tensor&lt;float&gt;(d0[64])\")],\n            # See https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/blob/main/tokenizer_config.json\n            functions=[\n                Function(\n                    name=\"input_ids\",\n                    # See https://docs.vespa.ai/en/cross-encoders.html#roberta-based-model and https://docs.vespa.ai/en/reference/rank-features.html\n                    expression=\"customTokenInputIds(1, 2, 512, query(q), attribute(body_tokens))\",\n                ),\n                Function(\n                    name=\"attention_mask\",\n                    expression=\"tokenAttentionMask(512, query(q), attribute(body_tokens))\",\n                ),\n            ],\n            first_phase=\"bm25(text)\",\n            global_phase=GlobalPhaseRanking(\n                rerank_count=10,\n                # We use the sigmoid function to force the output to be between 0 and 1, converting logits to probabilities.\n                expression=\"sigmoid(onnx(crossencoder).logits{d0:0,d1:0})\",\n            ),\n            summary_features=[\n                \"query(q)\",\n                \"input_ids\",\n                \"attention_mask\",\n                \"onnx(crossencoder).logits\",\n            ],\n        ),\n    ],\n)\n</pre> from vespa.package import (     Component,     Document,     Field,     FieldSet,     Function,     GlobalPhaseRanking,     OnnxModel,     Parameter,     RankProfile,     Schema, )  schema = Schema(     name=\"doc\",     mode=\"index\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"index\", \"summary\"],                 index=\"enable-bm25\",             ),             # Let\u00b4s add a synthetic field (see https://docs.vespa.ai/en/schemas.html#field)             # to define how the tokens are derived from the text field             Field(                 name=\"body_tokens\",                 type=\"tensor(d0[512])\",                 # The tokenizer will be defined in the next cell                 indexing=[\"input text\", \"embed tokenizer\", \"attribute\", \"summary\"],                 is_document_field=False,  # Indicates a synthetic field             ),         ],     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],     models=[         OnnxModel(             model_name=\"crossencoder\",             model_file_path=f\"{local_model_path}\",             inputs={                 \"input_ids\": \"input_ids\",                 \"attention_mask\": \"attention_mask\",             },             outputs={\"logits\": \"logits\"},         )     ],     rank_profiles=[         RankProfile(name=\"bm25\", first_phase=\"bm25(text)\"),         RankProfile(             name=\"reranking\",             inherits=\"default\",             # We truncate the query to 64 tokens, meaning we have 512-64=448 tokens left for the document.             inputs=[(\"query(q)\", \"tensor(d0[64])\")],             # See https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/blob/main/tokenizer_config.json             functions=[                 Function(                     name=\"input_ids\",                     # See https://docs.vespa.ai/en/cross-encoders.html#roberta-based-model and https://docs.vespa.ai/en/reference/rank-features.html                     expression=\"customTokenInputIds(1, 2, 512, query(q), attribute(body_tokens))\",                 ),                 Function(                     name=\"attention_mask\",                     expression=\"tokenAttentionMask(512, query(q), attribute(body_tokens))\",                 ),             ],             first_phase=\"bm25(text)\",             global_phase=GlobalPhaseRanking(                 rerank_count=10,                 # We use the sigmoid function to force the output to be between 0 and 1, converting logits to probabilities.                 expression=\"sigmoid(onnx(crossencoder).logits{d0:0,d1:0})\",             ),             summary_features=[                 \"query(q)\",                 \"input_ids\",                 \"attention_mask\",                 \"onnx(crossencoder).logits\",             ],         ),     ], ) In\u00a0[3]: Copied! <pre>from vespa.package import ApplicationPackage\n\napp_package = ApplicationPackage(\n    name=\"reranking\",\n    schema=[schema],\n    components=[\n        Component(\n            # See https://docs.vespa.ai/en/reference/embedding-reference.html#huggingface-tokenizer-embedder\n            id=\"tokenizer\",\n            type=\"hugging-face-tokenizer\",\n            parameters=[\n                Parameter(\n                    \"model\",\n                    {\n                        \"url\": \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/raw/main/tokenizer.json\"\n                    },\n                ),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import ApplicationPackage  app_package = ApplicationPackage(     name=\"reranking\",     schema=[schema],     components=[         Component(             # See https://docs.vespa.ai/en/reference/embedding-reference.html#huggingface-tokenizer-embedder             id=\"tokenizer\",             type=\"hugging-face-tokenizer\",             parameters=[                 Parameter(                     \"model\",                     {                         \"url\": \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/raw/main/tokenizer.json\"                     },                 ),             ],         )     ], ) <p>It is useful to inspect the schema-file (see https://docs.vespa.ai/en/reference/schema-reference.html) before deploying the application.</p> In\u00a0[4]: Copied! <pre>print(schema.schema_to_text)\n</pre> print(schema.schema_to_text) <pre>schema doc {\n    document doc {\n        field id type string {\n            indexing: summary | attribute\n        }\n        field text type string {\n            indexing: index | summary\n            index: enable-bm25\n        }\n    }\n    field body_tokens type tensor&lt;float&gt;(d0[512]) {\n        indexing: input text | embed tokenizer | attribute | summary\n    }\n    fieldset default {\n        fields: text\n    }\n    onnx-model crossencoder {\n        file: files/crossencoder.onnx\n        input input_ids: input_ids\n        input attention_mask: attention_mask\n        output logits: logits\n    }\n    rank-profile bm25 {\n        first-phase {\n            expression {\n                bm25(text)\n            }\n        }\n    }\n    rank-profile reranking inherits default {\n        inputs {\n            query(q) tensor&lt;float&gt;(d0[64])             \n        \n        }\n        function input_ids() {\n            expression {\n                customTokenInputIds(1, 2, 512, query(q), attribute(body_tokens))\n            }\n        }\n        function attention_mask() {\n            expression {\n                tokenAttentionMask(512, query(q), attribute(body_tokens))\n            }\n        }\n        first-phase {\n            expression {\n                bm25(text)\n            }\n        }\n        global-phase {\n            rerank-count: 10\n            expression {\n                sigmoid(onnx(crossencoder).logits{d0:0,d1:0})\n            }\n        }\n        summary-features {\n            query(q)\n            input_ids\n            attention_mask\n            onnx(crossencoder).logits\n        }\n    }\n}\n</pre> <p>It looks fine. Now, let's just save the application package first, so that we also have more insight into the other files that are part of the application package.</p> In\u00a0[5]: Copied! <pre># Optionally, we can also write the application package to disk before deploying it.\napp_package.to_files(\"crossencoder-demo\")\n</pre> # Optionally, we can also write the application package to disk before deploying it. app_package.to_files(\"crossencoder-demo\") In\u00a0[6]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker(port=8080)\n\napp = vespa_docker.deploy(application_package=app_package)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker(port=8080)  app = vespa_docker.deploy(application_package=app_package) <pre>Waiting for configuration server, 0/60 seconds...\nUsing plain http against endpoint http://localhost:8089/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8089/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8089/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8089/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[7]: Copied! <pre>from docker.models.containers import Container\n\n\ndef download_and_analyze_model(url: str, container: Container) -&gt; None:\n    \"\"\"\n    Downloads an ONNX model from a specified URL and analyzes it within a Docker container.\n\n    Parameters:\n    url (str): The URL from where the ONNX model should be downloaded.\n    container (Container): The Docker container in which the command will be executed.\n\n    Raises:\n    Exception: Raises an exception if the command execution fails or if there are issues in streaming the output.\n\n    Note:\n    This function assumes that 'curl' and 'vespa-analyze-onnx-model' are available in the container environment.\n    \"\"\"\n\n    # Define the path inside the container where the model will be stored.\n    model_path = \"/opt/vespa/var/model.onnx\"\n\n    # Construct the command to download and analyze the model inside the container.\n    command = f\"bash -c 'curl -Lo {model_path} {url} &amp;&amp; vespa-analyze-onnx-model {model_path}'\"\n\n    # Command to delete the model after analysis.\n    delete_command = f\"rm {model_path}\"\n\n    # Execute the command in the container and handle potential errors.\n    try:\n        exit_code, output = container.exec_run(command, stream=True)\n        # Print the output from the command.\n        for line in output:\n            print(line.decode(), end=\"\")\n        # Remove the model after analysis.\n        container.exec_run(delete_command)\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        raise\n\n\nurl = \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/resolve/main/onnx/model.onnx\"\n# Example usage:\n# download_and_analyze_model(url, vespa_docker.container)\n</pre> from docker.models.containers import Container   def download_and_analyze_model(url: str, container: Container) -&gt; None:     \"\"\"     Downloads an ONNX model from a specified URL and analyzes it within a Docker container.      Parameters:     url (str): The URL from where the ONNX model should be downloaded.     container (Container): The Docker container in which the command will be executed.      Raises:     Exception: Raises an exception if the command execution fails or if there are issues in streaming the output.      Note:     This function assumes that 'curl' and 'vespa-analyze-onnx-model' are available in the container environment.     \"\"\"      # Define the path inside the container where the model will be stored.     model_path = \"/opt/vespa/var/model.onnx\"      # Construct the command to download and analyze the model inside the container.     command = f\"bash -c 'curl -Lo {model_path} {url} &amp;&amp; vespa-analyze-onnx-model {model_path}'\"      # Command to delete the model after analysis.     delete_command = f\"rm {model_path}\"      # Execute the command in the container and handle potential errors.     try:         exit_code, output = container.exec_run(command, stream=True)         # Print the output from the command.         for line in output:             print(line.decode(), end=\"\")         # Remove the model after analysis.         container.exec_run(delete_command)      except Exception as e:         print(f\"An error occurred: {e}\")         raise   url = \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/resolve/main/onnx/model.onnx\" # Example usage: # download_and_analyze_model(url, vespa_docker.container) <pre>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  1126  100  1126    0     0   5715      0 --:--:-- --:--:-- --:--:--  5686\n100  271M  100  271M    0     0  15.8M      0  0:00:17  0:00:17 --:--:-- 16.3M\nunspecified option[0](optimize model), fallback: true\nvm_size: 166648 kB, vm_rss: 46700 kB, malloc_peak: 0 kb, malloc_curr: 1100 (before loading model)\nvm_size: 517176 kB, vm_rss: 405592 kB, malloc_peak: 0 kb, malloc_curr: 351628 (after loading model)\nmodel meta-data:\n  input[0]: 'input_ids' long[batch_size][sequence_length]\n  input[1]: 'attention_mask' long[batch_size][sequence_length]\n  output[0]: 'logits' float[batch_size][1]\nunspecified option[1](symbolic size 'batch_size'), fallback: 1\nunspecified option[2](symbolic size 'sequence_length'), fallback: 1\n1717140328.769314\tlocalhost\t1305/26134\t-\t.eval.onnx_wrapper\twarning\tinput 'input_ids' with element type 'long' is bound to vespa value with cell type 'double'; adding explicit conversion step (this conversion might be lossy)\n1717140328.769336\tlocalhost\t1305/26134\t-\t.eval.onnx_wrapper\twarning\tinput 'attention_mask' with element type 'long' is bound to vespa value with cell type 'double'; adding explicit conversion step (this conversion might be lossy)\ntest setup:\n  input[0]: tensor(d0[1],d1[1]) -&gt; long[1][1]\n  input[1]: tensor(d0[1],d1[1]) -&gt; long[1][1]\n  output[0]: float[1][1] -&gt; tensor&lt;float&gt;(d0[1],d1[1])\nunspecified option[3](max concurrent evaluations), fallback: 1\nvm_size: 517176 kB, vm_rss: 405592 kB, malloc_peak: 0 kb, malloc_curr: 351628 (no evaluations yet)\nvm_size: 517176 kB, vm_rss: 405856 kB, malloc_peak: 0 kb, malloc_curr: 351628 (concurrent evaluations: 1)\nestimated model evaluation time: 3.77819 ms\n</pre> <p>By doing this with the different size models and their quantized versions, we get this table.</p> Model Model File Inference Time (ms) Size N docs in 200ms mixedbread-ai/mxbai-rerank-xsmall-v1 model_quantized.onnx 2.4 87MB 82 mixedbread-ai/mxbai-rerank-xsmall-v1 model.onnx 3.8 284MB 52 mixedbread-ai/mxbai-rerank-base-v1 model_quantized.onnx 5.4 244MB 37 mixedbread-ai/mxbai-rerank-base-v1 model.onnx 10.3 739MB 19 mixedbread-ai/mxbai-rerank-large-v1 model_quantized.onnx 16.0 643MB 12 mixedbread-ai/mxbai-rerank-large-v1 model.onnx 35.6 1.74GB 5 <p>With a time budget of 200ms for reranking, we can add a column indicating the number of documents we are able to rerank within the budget time.</p> In\u00a0[8]: Copied! <pre># Feed a few sample documents to the application\nsample_docs = [\n    {\"id\": i, \"fields\": {\"text\": text}}\n    for i, text in enumerate(\n        [\n            \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n            \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n            \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n            \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n            \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n            \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\",\n        ]\n    )\n]\n</pre> # Feed a few sample documents to the application sample_docs = [     {\"id\": i, \"fields\": {\"text\": text}}     for i, text in enumerate(         [             \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",             \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",             \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",             \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",             \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",             \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\",         ]     ) ] In\u00a0[9]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n\n\napp.feed_iterable(sample_docs, schema=\"doc\", callback=callback)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         )   app.feed_iterable(sample_docs, schema=\"doc\", callback=callback) In\u00a0[10]: Copied! <pre>from pprint import pprint\n\nwith app.syncio(connections=1) as sync_app:\n    query = sync_app.query(\n        body={\n            \"yql\": \"select * from sources * where userQuery();\",\n            \"query\": \"who wrote to kill a mockingbird?\",\n            \"input.query(q)\": \"embed(tokenizer, @query)\",\n            \"ranking.profile\": \"reranking\",\n            \"ranking.listFeatures\": \"true\",\n            \"presentation.timing\": \"true\",\n        }\n    )\n    for hit in query.hits:\n        pprint(hit[\"fields\"][\"text\"])\n        pprint(hit[\"relevance\"])\n</pre> from pprint import pprint  with app.syncio(connections=1) as sync_app:     query = sync_app.query(         body={             \"yql\": \"select * from sources * where userQuery();\",             \"query\": \"who wrote to kill a mockingbird?\",             \"input.query(q)\": \"embed(tokenizer, @query)\",             \"ranking.profile\": \"reranking\",             \"ranking.listFeatures\": \"true\",             \"presentation.timing\": \"true\",         }     )     for hit in query.hits:         pprint(hit[\"fields\"][\"text\"])         pprint(hit[\"relevance\"]) <pre>(\"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was \"\n 'immediately successful, winning the Pulitzer Prize, and has become a classic '\n 'of modern American literature.')\n0.9634037778787636\n(\"Harper Lee, an American novelist widely known for her novel 'To Kill a \"\n \"Mockingbird', was born in 1926 in Monroeville, Alabama. She received the \"\n 'Pulitzer Prize for Fiction in 1961.')\n0.8672221280618897\n(\"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, \"\n 'was published in 1925. The story is set in the Jazz Age and follows the life '\n 'of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.')\n0.09325768904619067\n(\"The novel 'Moby-Dick' was written by Herman Melville and first published in \"\n '1851. It is considered a masterpiece of American literature and deals with '\n 'complex themes of obsession, revenge, and the conflict between good and '\n 'evil.')\n0.010269765303083865\n</pre> <p>It will of course be necessary to evaluate the performance of the cross-encoder in your specific use-case, but this notebook should give you a good starting point.</p> In\u00a0[11]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"examples/cross-encoders-for-global-reranking.html#using-mixedbreadai-cross-encoder-for-reranking-in-vespaai","title":"Using Mixedbread.ai cross-encoder for reranking in Vespa.ai\u00b6","text":"<p>First, let us recap what cross-encoders are and where they might fit in a Vespa application.</p> <p>In contrast to bi-encoders, it is important to know that cross-encoders do NOT produce an embedding. Instead, a cross-encoder acts on pairs of input sequences and produces a single scalar score between 0 and 1, indicating the similarity or relevance between the two sentences.</p> <p>The cross-encoder model is a transformer-based model with a classification head on top of the Transformer CLS token (classification token).</p> <p>The model has been fine-tuned using the MS Marco passage training set and is a binary classifier which classifies if a query,document pair is relevant or not.</p> <p>The quote is from this blog post from 2021 that explains cross-encoders more in-depth. Note that the reference to the MS Marco dataset is for the model used in the blog post, and not the model we will use in this notebook.</p>"},{"location":"examples/cross-encoders-for-global-reranking.html#properties-of-cross-encoders-and-where-they-fit-in-vespa","title":"Properties of cross-encoders and where they fit in Vespa\u00b6","text":"<p>Cross-encoders are great at comparing a query and a document, but the time complexity increases linearly with the number of documents a query is compared to.</p> <p>This is why cross-encoders are often part of solutions at the top of leaderboards for ranking performance, such as MS MARCO Passage Ranking leaderboard.</p> <p>However, this leaderboard does not evaluate a solution's latency, and for production systems, doing cross-encoder inference for all documents in a corpus become prohibitively expensive.</p> <p>With Vespa's phased ranking capabilities, doing cross-encoder inference for a subset of documents at a later stage in the ranking pipeline can be a good trade-off between ranking performance and latency. For the remainder of this notebook, we will look at using a cross-encoder in global-phase reranking, introduced in this blog post.</p> <p></p> <p>In this notebook, we will show how to use the Mixedbread.ai cross-encoder for global-phase reranking in Vespa.</p> <p>The inference can also be run on GPU in Vespa Cloud, to accelerate inference even further.</p>"},{"location":"examples/cross-encoders-for-global-reranking.html#exploring-the-mixedbreadai-cross-encoder","title":"Exploring the Mixedbread.ai cross-encoder\u00b6","text":"<p>mixedbread.ai has done an amazing job of releasing both (binary) embedding-models and rerankers on huggingface \ud83e\udd17 the last weeks.</p> <p>Check out our previous notebook on using binary embeddings from mixedbread.ai in Vespa Cloud here</p> <p>For this demo, we will use mixedbread-ai/mxbai-rerank-xsmall-v1, but you can experiment with the larger models, depending on how you want to balance speed, accuracy, and cost (if you want to use GPU).</p> <p>This model is really powerful despite its small size, and provides a good trade-off between speed and accuracy.</p> <p>Table of accuracy on a BEIR (11 datasets):</p> Model Accuracy Lexical Search 66.4 bge-reranker-base 66.9 bge-reranker-large 70.6 cohere-embed-v3 70.9 mxbai-rerank-xsmall-v1 70.0 mxbai-rerank-base-v1 72.3 mxbai-rerank-large-v1 74.9 <p>(Table from mixedbread.ai's introductory blog post.)</p> <p>As we can see, the <code>mxbai-rerank-xsmall-v1</code> model is almost on par with much larger models while being much faster and cheaper to run.</p>"},{"location":"examples/cross-encoders-for-global-reranking.html#downloading-the-model","title":"Downloading the model\u00b6","text":"<p>We will use the quantized version of <code>mxbai-rerank-xsmall-v1</code> for this demo, as it is faster and cheaper to run, but feel free to change to the model of your choice.</p>"},{"location":"examples/cross-encoders-for-global-reranking.html#inspecting-the-model","title":"Inspecting the model\u00b6","text":"<p>It is useful to inspect the expected inputs and outputs, along with their shapes, before integrating the model into Vespa.</p> <p>This can either be done by, for instance, by using the <code>sentence_transformers</code> and <code>onnxruntime</code> libraries.</p> <p>One-off tasks like this are well suited for a Colab notebook. One example of how to do this in Colab can be found here:</p> <p></p>"},{"location":"examples/cross-encoders-for-global-reranking.html#what-does-a-crossencoder-do","title":"What does a crossencoder do?\u00b6","text":"<p>Below, we have tried to visualize what is going on in a cross-encoder, which helps us understand how we can use it in Vespa.</p> <p></p>"},{"location":"examples/cross-encoders-for-global-reranking.html#defining-our-vespa-application","title":"Defining our Vespa application\u00b6","text":""},{"location":"examples/cross-encoders-for-global-reranking.html#next-steps","title":"Next steps\u00b6","text":"<p>Try to use hybrid search for the first phase, and rerank with a cross-encoder.</p>"},{"location":"examples/cross-encoders-for-global-reranking.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"examples/evaluating-with-snowflake-arctic-embed.html","title":"Evaluating with snowflake arctic embed","text":"In\u00a0[1]: Copied! <pre>from vespa.package import (\n    HNSW,\n    ApplicationPackage,\n    Component,\n    Field,\n    Parameter,\n    Function,\n)\n\napp_name = \"snowflake\"\n\napp_package = ApplicationPackage(\n    name=app_name,\n    components=[\n        Component(\n            id=\"snow\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    \"transformer-model\",\n                    {\n                        \"url\": \"https://huggingface.co/Snowflake/snowflake-arctic-embed-s/resolve/main/onnx/model_int8.onnx\"\n                    },\n                ),\n                Parameter(\n                    \"tokenizer-model\",\n                    {\n                        \"url\": \"https://huggingface.co/Snowflake/snowflake-arctic-embed-s/raw/main/tokenizer.json\"\n                    },\n                ),\n                Parameter(\n                    \"normalize\",\n                    {},\n                    \"true\",\n                ),\n                Parameter(\n                    \"pooling-strategy\",\n                    {},\n                    \"cls\",\n                ),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import (     HNSW,     ApplicationPackage,     Component,     Field,     Parameter,     Function, )  app_name = \"snowflake\"  app_package = ApplicationPackage(     name=app_name,     components=[         Component(             id=\"snow\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(                     \"transformer-model\",                     {                         \"url\": \"https://huggingface.co/Snowflake/snowflake-arctic-embed-s/resolve/main/onnx/model_int8.onnx\"                     },                 ),                 Parameter(                     \"tokenizer-model\",                     {                         \"url\": \"https://huggingface.co/Snowflake/snowflake-arctic-embed-s/raw/main/tokenizer.json\"                     },                 ),                 Parameter(                     \"normalize\",                     {},                     \"true\",                 ),                 Parameter(                     \"pooling-strategy\",                     {},                     \"cls\",                 ),             ],         )     ], ) In\u00a0[2]: Copied! <pre>app_package.schema.add_fields(\n    Field(name=\"id\", type=\"int\", indexing=[\"attribute\", \"summary\"]),\n    Field(\n        name=\"doc\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n    ),\n    Field(\n        name=\"doc_embeddings\",\n        type=\"tensor&lt;float&gt;(x[384])\",\n        indexing=[\"input doc\", \"embed\", \"index\", \"attribute\"],\n        ann=HNSW(distance_metric=\"prenormalized-angular\"),\n        is_document_field=False,\n    ),\n)\n</pre> app_package.schema.add_fields(     Field(name=\"id\", type=\"int\", indexing=[\"attribute\", \"summary\"]),     Field(         name=\"doc\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"     ),     Field(         name=\"doc_embeddings\",         type=\"tensor(x[384])\",         indexing=[\"input doc\", \"embed\", \"index\", \"attribute\"],         ann=HNSW(distance_metric=\"prenormalized-angular\"),         is_document_field=False,     ), ) In\u00a0[3]: Copied! <pre>from vespa.package import (\n    DocumentSummary,\n    FieldSet,\n    FirstPhaseRanking,\n    RankProfile,\n    SecondPhaseRanking,\n    Summary,\n)\n\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"semantic\",\n        inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n        inherits=\"default\",\n        first_phase=\"closeness(field, doc_embeddings)\",\n        match_features=[\"closeness(field, doc_embeddings)\"],\n    )\n)\n\napp_package.schema.add_rank_profile(RankProfile(name=\"bm25\", first_phase=\"bm25(doc)\"))\n</pre> from vespa.package import (     DocumentSummary,     FieldSet,     FirstPhaseRanking,     RankProfile,     SecondPhaseRanking,     Summary, )  app_package.schema.add_rank_profile(     RankProfile(         name=\"semantic\",         inputs=[(\"query(q)\", \"tensor(x[384])\")],         inherits=\"default\",         first_phase=\"closeness(field, doc_embeddings)\",         match_features=[\"closeness(field, doc_embeddings)\"],     ) )  app_package.schema.add_rank_profile(RankProfile(name=\"bm25\", first_phase=\"bm25(doc)\")) In\u00a0[4]: Copied! <pre>app_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"hybrid\",\n        inherits=\"semantic\",\n        # Guard against no keword match -&gt; bm25 = 0 -&gt; log10(0) = undefined\n        functions=[\n            Function(\n                name=\"log_guard\", expression=\"if (bm25(doc) &gt; 0, log10(bm25(doc)), 0)\"\n            )\n        ],\n        first_phase=FirstPhaseRanking(expression=\"closeness(field, doc_embeddings)\"),\n        # Notice that we use log10 here, as the bm25 values with the natural logarithm tends to dominate the closeness values for these documents.\n        second_phase=SecondPhaseRanking(expression=\"firstPhase + log_guard\"),\n        match_features=[\n            \"firstPhase\",\n            \"bm25(doc)\",\n        ],\n    )\n)\n</pre> app_package.schema.add_rank_profile(     RankProfile(         name=\"hybrid\",         inherits=\"semantic\",         # Guard against no keword match -&gt; bm25 = 0 -&gt; log10(0) = undefined         functions=[             Function(                 name=\"log_guard\", expression=\"if (bm25(doc) &gt; 0, log10(bm25(doc)), 0)\"             )         ],         first_phase=FirstPhaseRanking(expression=\"closeness(field, doc_embeddings)\"),         # Notice that we use log10 here, as the bm25 values with the natural logarithm tends to dominate the closeness values for these documents.         second_phase=SecondPhaseRanking(expression=\"firstPhase + log_guard\"),         match_features=[             \"firstPhase\",             \"bm25(doc)\",         ],     ) ) In\u00a0[5]: Copied! <pre>app_package.schema.add_field_set(FieldSet(name=\"default\", fields=[\"doc\"]))\n</pre> app_package.schema.add_field_set(FieldSet(name=\"default\", fields=[\"doc\"])) In\u00a0[6]: Copied! <pre>app_package.schema.add_document_summary(\n    DocumentSummary(\n        name=\"minimal\",\n        summary_fields=[Summary(\"id\", \"int\"), Summary(\"doc\", \"string\")],\n    )\n)\n</pre> app_package.schema.add_document_summary(     DocumentSummary(         name=\"minimal\",         summary_fields=[Summary(\"id\", \"int\"), Summary(\"doc\", \"string\")],     ) ) <p>Create some sample documents that will help us see where the different ranking strategies have their strengths and weaknesses.</p> <p>These sample documents were created with a little help from ChatGPT.</p> <p>Looking through the documents, we can see that a ranking of the documents in the order they are presented seem quite reasonable.</p> In\u00a0[7]: Copied! <pre># Query that the user is searching for\nquery = \"How does Vespa handle real-time indexing and search?\"\n\ndocuments = [\n    \"Vespa excels in real-time data indexing and its ability to search large datasets quickly.\",\n    \"Instant data availability and maintaining query performance while simultaneously indexing are key features of the Vespa search engine.\",\n    \"With our search solution, real-time updates are seamlessly integrated into the search index, enhancing responsiveness.\",\n    \"While not as robust as Vespa, our vector database strives to meet your search needs, despite certain, shall we say, 'flexible' features.\",\n    \"Search engines like ours utilize complex algorithms to handle immediate data querying and indexing.\",\n    \"Modern search platforms emphasize quick data retrieval from continuously updated indexes.\",\n    \"Discover the history and cultural impact of the classic Italian Vespa scooter brand.\",\n    \"Tips for maintaining your Vespa to ensure optimal performance and longevity of your scooter.\",\n    \"Review of different scooter brands including Vespa, highlighting how they handle features like speed, cost, and aesthetics, and how consumers search for the best options.\",\n    \"Vespa scooter safety regulations and best practices for urban commuting.\",\n]\n</pre> # Query that the user is searching for query = \"How does Vespa handle real-time indexing and search?\"  documents = [     \"Vespa excels in real-time data indexing and its ability to search large datasets quickly.\",     \"Instant data availability and maintaining query performance while simultaneously indexing are key features of the Vespa search engine.\",     \"With our search solution, real-time updates are seamlessly integrated into the search index, enhancing responsiveness.\",     \"While not as robust as Vespa, our vector database strives to meet your search needs, despite certain, shall we say, 'flexible' features.\",     \"Search engines like ours utilize complex algorithms to handle immediate data querying and indexing.\",     \"Modern search platforms emphasize quick data retrieval from continuously updated indexes.\",     \"Discover the history and cultural impact of the classic Italian Vespa scooter brand.\",     \"Tips for maintaining your Vespa to ensure optimal performance and longevity of your scooter.\",     \"Review of different scooter brands including Vespa, highlighting how they handle features like speed, cost, and aesthetics, and how consumers search for the best options.\",     \"Vespa scooter safety regulations and best practices for urban commuting.\", ] In\u00a0[8]: Copied! <pre>app_package.to_files(\"snowflake\")\n</pre> app_package.to_files(\"snowflake\") In\u00a0[9]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(app_package)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy(app_package) <pre>Waiting for configuration server, 0/60 seconds...\nWaiting for configuration server, 5/60 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 15/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 20/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 25/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[10]: Copied! <pre>feed_docs = [\n    {\n        \"id\": str(i),\n        \"fields\": {\n            \"doc\": doc,\n        },\n    }\n    for i, doc in enumerate(documents)\n]\n</pre> feed_docs = [     {         \"id\": str(i),         \"fields\": {             \"doc\": doc,         },     }     for i, doc in enumerate(documents) ] In\u00a0[11]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         ) In\u00a0[12]: Copied! <pre>app.feed_iterable(feed_docs, schema=app_package.schema.name, callback=callback)\n</pre> app.feed_iterable(feed_docs, schema=app_package.schema.name, callback=callback) In\u00a0[13]: Copied! <pre>import math\nfrom typing import List\n\n\ndef ndcg_at_k(rank_order: List[int], ideal_order: List[int], k: int) -&gt; float:\n    \"\"\"\n    Calculate the normalized Discounted Cumulative Gain (nDCG) at position k.\n\n    Parameters:\n        rank_order (List[int]): The list of document indices as ranked by the search system.\n        ideal_order (List[int]): The list of document indices in the ideal order.\n        k (int): The position up to which to calculate nDCG.\n\n    Returns:\n        float: The nDCG value at position k.\n    \"\"\"\n    dcg = 0.0\n    idcg = 0.0\n\n    # Calculate DCG based on the ranked order up to k\n    for i in range(min(k, len(rank_order))):\n        rank_index = rank_order[i]\n        # Find the rank index in the ideal order to assign relevance\n        if rank_index in ideal_order:\n            relevance = len(ideal_order) - ideal_order.index(rank_index)\n        else:\n            relevance = 0\n        dcg += relevance / math.log2(i + 2)\n\n    # Calculate IDCG based on the ideal order up to k\n    for i in range(min(k, len(ideal_order))):\n        relevance = len(ideal_order) - i\n        idcg += relevance / math.log2(i + 2)\n\n    # Handle the case where IDCG is zero to avoid division by zero\n    if idcg == 0:\n        return 0.0\n    return dcg / idcg\n\n\n# Example usage\nrank_order = [5, 6, 1]  # Example ranked order indices\nideal_result_order = [0, 1, 2, 4, 5, 3, 6, 7, 8, 9]  # Example ideal order indices\n\n# Calculate nDCG@3\nresult = ndcg_at_k(rank_order, ideal_result_order, 3)\nprint(f\"nDCG@3: {result:.4f}\")\n\nassert ndcg_at_k([0, 1, 2], ideal_result_order, 3) == 1.0\n</pre> import math from typing import List   def ndcg_at_k(rank_order: List[int], ideal_order: List[int], k: int) -&gt; float:     \"\"\"     Calculate the normalized Discounted Cumulative Gain (nDCG) at position k.      Parameters:         rank_order (List[int]): The list of document indices as ranked by the search system.         ideal_order (List[int]): The list of document indices in the ideal order.         k (int): The position up to which to calculate nDCG.      Returns:         float: The nDCG value at position k.     \"\"\"     dcg = 0.0     idcg = 0.0      # Calculate DCG based on the ranked order up to k     for i in range(min(k, len(rank_order))):         rank_index = rank_order[i]         # Find the rank index in the ideal order to assign relevance         if rank_index in ideal_order:             relevance = len(ideal_order) - ideal_order.index(rank_index)         else:             relevance = 0         dcg += relevance / math.log2(i + 2)      # Calculate IDCG based on the ideal order up to k     for i in range(min(k, len(ideal_order))):         relevance = len(ideal_order) - i         idcg += relevance / math.log2(i + 2)      # Handle the case where IDCG is zero to avoid division by zero     if idcg == 0:         return 0.0     return dcg / idcg   # Example usage rank_order = [5, 6, 1]  # Example ranked order indices ideal_result_order = [0, 1, 2, 4, 5, 3, 6, 7, 8, 9]  # Example ideal order indices  # Calculate nDCG@3 result = ndcg_at_k(rank_order, ideal_result_order, 3) print(f\"nDCG@3: {result:.4f}\")  assert ndcg_at_k([0, 1, 2], ideal_result_order, 3) == 1.0 <pre>nDCG@3: 0.6618\n</pre> In\u00a0[14]: Copied! <pre># Define the different rank profiles to evaluate\n\nrank_profiles = {\n    \"unranked\": {\n        \"yql\": f\"select * from {app_name} where true\",\n        \"ranking.profile\": \"unranked\",\n    },\n    \"bm25\": {\n        \"yql\": f\"select * from {app_name} where userQuery()\",\n        \"ranking.profile\": \"bm25\",\n    },\n    \"semantic\": {\n        \"yql\": f\"select * from {app_name} where {{targetHits:5}}nearestNeighbor(doc_embeddings,q)\",\n        \"ranking.profile\": \"semantic\",\n        \"input.query(q)\": f\"embed({query})\",\n    },\n    \"hybrid\": {\n        \"yql\": f\"select * from {app_name} where userQuery() or ({{targetHits:5}}nearestNeighbor(doc_embeddings,q))\",\n        \"ranking.profile\": \"hybrid\",\n        \"input.query(q)\": f\"embed({query})\",\n    },\n    \"hybrid_filtered\": {\n        # Here, we will add an heuristic, filtering out documents that contain the word \"scooter\"\n        \"yql\": f'select * from {app_name} where !(doc contains \"scooter\") and userQuery() or ({{targetHits:5}}nearestNeighbor(doc_embeddings,q))',\n        \"ranking.profile\": \"hybrid\",\n        \"input.query(q)\": f\"embed({query})\",\n    },\n}\n\n# Define some common params that will be used for all queries\n\ncommon_params = {\n    \"query\": query,\n    \"hits\": 3,\n}\n</pre> # Define the different rank profiles to evaluate  rank_profiles = {     \"unranked\": {         \"yql\": f\"select * from {app_name} where true\",         \"ranking.profile\": \"unranked\",     },     \"bm25\": {         \"yql\": f\"select * from {app_name} where userQuery()\",         \"ranking.profile\": \"bm25\",     },     \"semantic\": {         \"yql\": f\"select * from {app_name} where {{targetHits:5}}nearestNeighbor(doc_embeddings,q)\",         \"ranking.profile\": \"semantic\",         \"input.query(q)\": f\"embed({query})\",     },     \"hybrid\": {         \"yql\": f\"select * from {app_name} where userQuery() or ({{targetHits:5}}nearestNeighbor(doc_embeddings,q))\",         \"ranking.profile\": \"hybrid\",         \"input.query(q)\": f\"embed({query})\",     },     \"hybrid_filtered\": {         # Here, we will add an heuristic, filtering out documents that contain the word \"scooter\"         \"yql\": f'select * from {app_name} where !(doc contains \"scooter\") and userQuery() or ({{targetHits:5}}nearestNeighbor(doc_embeddings,q))',         \"ranking.profile\": \"hybrid\",         \"input.query(q)\": f\"embed({query})\",     }, }  # Define some common params that will be used for all queries  common_params = {     \"query\": query,     \"hits\": 3, } In\u00a0[15]: Copied! <pre>from typing import List, Tuple\n\nfrom vespa.application import Vespa\nfrom vespa.io import VespaQueryResponse\n\n\ndef evaluate_rank_profile(\n    app: Vespa, rank_profile: str, params: dict, k: int\n) -&gt; Tuple[float, List[str]]:\n    \"\"\"\n    Run a query against a Vespa application using a specific rank profile and parameters.\n    Evaluate the nDCG@3 of the search results based on the ideal order.\n\n    Parameters:\n        app (Vespa): The Vespa application to query.\n        rank_profile (str): The name of the rank profile to use.\n        params (dict): The common parameters to use in addition to the rank profile specific parameters.\n        k (int): The position up to which to calculate nDCG.\n\n    Returns:\n        List[str]: The search results\n    \"\"\"\n    body_params = {\n        **rank_profiles[rank_profile],\n        **params,\n    }\n    response: VespaQueryResponse = app.query(body_params)\n    rankings = [int(hit[\"id\"][-1]) for hit in response.hits]\n    docs = [hit[\"fields\"][\"doc\"] for hit in response.hits]\n    ndcg = ndcg_at_k(rankings, ideal_order=ideal_result_order, k=3)\n    return ndcg, docs\n</pre> from typing import List, Tuple  from vespa.application import Vespa from vespa.io import VespaQueryResponse   def evaluate_rank_profile(     app: Vespa, rank_profile: str, params: dict, k: int ) -&gt; Tuple[float, List[str]]:     \"\"\"     Run a query against a Vespa application using a specific rank profile and parameters.     Evaluate the nDCG@3 of the search results based on the ideal order.      Parameters:         app (Vespa): The Vespa application to query.         rank_profile (str): The name of the rank profile to use.         params (dict): The common parameters to use in addition to the rank profile specific parameters.         k (int): The position up to which to calculate nDCG.      Returns:         List[str]: The search results     \"\"\"     body_params = {         **rank_profiles[rank_profile],         **params,     }     response: VespaQueryResponse = app.query(body_params)     rankings = [int(hit[\"id\"][-1]) for hit in response.hits]     docs = [hit[\"fields\"][\"doc\"] for hit in response.hits]     ndcg = ndcg_at_k(rankings, ideal_order=ideal_result_order, k=3)     return ndcg, docs In\u00a0[16]: Copied! <pre>import json\n\nrank_results = {}\n\nfor rank_profile, params in rank_profiles.items():\n    ndcg, docs = evaluate_rank_profile(\n        app, rank_profile=rank_profile, params=common_params, k=3\n    )\n    rank_results[rank_profile] = ndcg\n    print(f\"Rank profile: {rank_profile}, nDCG@3: {ndcg:.2f}\")\n    print(json.dumps(docs, indent=2))\n</pre> import json  rank_results = {}  for rank_profile, params in rank_profiles.items():     ndcg, docs = evaluate_rank_profile(         app, rank_profile=rank_profile, params=common_params, k=3     )     rank_results[rank_profile] = ndcg     print(f\"Rank profile: {rank_profile}, nDCG@3: {ndcg:.2f}\")     print(json.dumps(docs, indent=2)) <pre>Rank profile: unranked, nDCG@3: 0.68\n[\n  \"With our search solution, real-time updates are seamlessly integrated into the search index, enhancing responsiveness.\",\n  \"Tips for maintaining your Vespa to ensure optimal performance and longevity of your scooter.\",\n  \"Search engines like ours utilize complex algorithms to handle immediate data querying and indexing.\"\n]\nRank profile: bm25, nDCG@3: 0.78\n[\n  \"Vespa excels in real-time data indexing and its ability to search large datasets quickly.\",\n  \"Review of different scooter brands including Vespa, highlighting how they handle features like speed, cost, and aesthetics, and how consumers search for the best options.\",\n  \"With our search solution, real-time updates are seamlessly integrated into the search index, enhancing responsiveness.\"\n]\nRank profile: semantic, nDCG@3: 0.94\n[\n  \"Vespa excels in real-time data indexing and its ability to search large datasets quickly.\",\n  \"With our search solution, real-time updates are seamlessly integrated into the search index, enhancing responsiveness.\",\n  \"Search engines like ours utilize complex algorithms to handle immediate data querying and indexing.\"\n]\nRank profile: hybrid, nDCG@3: 0.82\n[\n  \"Vespa excels in real-time data indexing and its ability to search large datasets quickly.\",\n  \"With our search solution, real-time updates are seamlessly integrated into the search index, enhancing responsiveness.\",\n  \"Review of different scooter brands including Vespa, highlighting how they handle features like speed, cost, and aesthetics, and how consumers search for the best options.\"\n]\nRank profile: hybrid_filtered, nDCG@3: 0.94\n[\n  \"Vespa excels in real-time data indexing and its ability to search large datasets quickly.\",\n  \"With our search solution, real-time updates are seamlessly integrated into the search index, enhancing responsiveness.\",\n  \"Search engines like ours utilize complex algorithms to handle immediate data querying and indexing.\"\n]\n</pre> <p>Uncomment the cell below to install dependencies needed to generate the plot.</p> In\u00a0[17]: Copied! <pre>#!pip3 install pandas plotly\n</pre> #!pip3 install pandas plotly In\u00a0[20]: Copied! <pre>import pandas as pd\nimport plotly.express as px\n\n\ndef plot_rank_profiles(rank_profiles):\n    # Convert dictionary to DataFrame for easier manipulation\n    data = pd.DataFrame(list(rank_profiles.items()), columns=[\"Rank Profile\", \"nDCG@3\"])\n\n    colors = {\n        \"unranked\": \"#e74c3c\",  # Red\n        \"bm25\": \"#2ecc71\",  # Green\n        \"semantic\": \"#9b59b6\",  # Purple\n        \"hybrid\": \"#3498db\",  # Blue\n        \"hybrid_filtered\": \"#2980b9\",  # Darker Blue\n    }\n\n    # Map the colors to the dataframe\n    data[\"Color\"] = data[\"Rank Profile\"].map(colors)\n\n    # Create a bar chart using Plotly\n    fig = px.bar(\n        data,\n        x=\"Rank Profile\",\n        y=\"nDCG@3\",\n        title=\"Rank Profile Performance - snowflake-arctic-embed-s\",\n        labels={\"nDCG@3\": \"nDCG@3 Score\"},\n        text=\"nDCG@3\",\n        template=\"simple_white\",\n        color=\"Color\",\n        color_discrete_map=\"identity\",\n    )\n\n    # Set bar width and update traces for individual colors\n    fig.update_traces(\n        marker_line_color=\"black\", marker_line_width=1.5, width=0.4\n    )  # Less wide bars\n\n    # Enhance chart design adhering to Tufte's principles\n    fig.update_traces(texttemplate=\"%{text:.2f}\", textposition=\"outside\")\n    fig.update_layout(\n        plot_bgcolor=\"white\",\n        xaxis=dict(\n            title=\"Rank Profile\",\n            showline=True,\n            linewidth=2,\n            linecolor=\"black\",\n            mirror=True,\n        ),\n        yaxis=dict(\n            title=\"nDCG@3 Score\",\n            range=[0, 1.1],\n            showline=True,\n            linewidth=2,\n            linecolor=\"black\",\n            mirror=True,\n        ),\n        title_font=dict(size=24),\n        font=dict(family=\"Arial, sans-serif\", size=18, color=\"black\"),\n        margin=dict(l=40, r=40, t=40, b=40),\n        width=800,  # Set the width of the plot\n    )\n\n    # Show the plot\n    fig.show()\n\n\nplot_rank_profiles(rank_profiles=rank_results)\n</pre> import pandas as pd import plotly.express as px   def plot_rank_profiles(rank_profiles):     # Convert dictionary to DataFrame for easier manipulation     data = pd.DataFrame(list(rank_profiles.items()), columns=[\"Rank Profile\", \"nDCG@3\"])      colors = {         \"unranked\": \"#e74c3c\",  # Red         \"bm25\": \"#2ecc71\",  # Green         \"semantic\": \"#9b59b6\",  # Purple         \"hybrid\": \"#3498db\",  # Blue         \"hybrid_filtered\": \"#2980b9\",  # Darker Blue     }      # Map the colors to the dataframe     data[\"Color\"] = data[\"Rank Profile\"].map(colors)      # Create a bar chart using Plotly     fig = px.bar(         data,         x=\"Rank Profile\",         y=\"nDCG@3\",         title=\"Rank Profile Performance - snowflake-arctic-embed-s\",         labels={\"nDCG@3\": \"nDCG@3 Score\"},         text=\"nDCG@3\",         template=\"simple_white\",         color=\"Color\",         color_discrete_map=\"identity\",     )      # Set bar width and update traces for individual colors     fig.update_traces(         marker_line_color=\"black\", marker_line_width=1.5, width=0.4     )  # Less wide bars      # Enhance chart design adhering to Tufte's principles     fig.update_traces(texttemplate=\"%{text:.2f}\", textposition=\"outside\")     fig.update_layout(         plot_bgcolor=\"white\",         xaxis=dict(             title=\"Rank Profile\",             showline=True,             linewidth=2,             linecolor=\"black\",             mirror=True,         ),         yaxis=dict(             title=\"nDCG@3 Score\",             range=[0, 1.1],             showline=True,             linewidth=2,             linecolor=\"black\",             mirror=True,         ),         title_font=dict(size=24),         font=dict(family=\"Arial, sans-serif\", size=18, color=\"black\"),         margin=dict(l=40, r=40, t=40, b=40),         width=800,  # Set the width of the plot     )      # Show the plot     fig.show()   plot_rank_profiles(rank_profiles=rank_results) <p>For this particular synthetic small dataset, we can see that using the <code>snowflake-arctic-embed</code>-model improved the results significantly compared to keyword search only. Still, our experience with real-world data is that hybrid search is often the way to go.</p> <p>We also provided a little taste of how one can evaluate different ranking profiles if you have a ground truth dataset available, (or can create a synthetic one).</p> In\u00a0[19]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"examples/evaluating-with-snowflake-arctic-embed.html#evaluating-retrieval-with-snowflake-arctic-embed","title":"Evaluating retrieval with Snowflake arctic embed\u00b6","text":"<p>This notebook will demonstrate how different rank profiles in Vespa can be set up and evaluated. For the rank profiles that use semantic search, we will use the small version of Snowflake's arctic embed model series for generating embeddings.</p> <p>Feel free to experiment with different sizes based on your need and compute/latency constraints.</p> <p>The snowflake-arctic-embedding models achieve state-of-the-art performance on the MTEB/BEIR leaderboard for each of their size variants.</p> <p>We will set up and compare the following rank profiles:</p> <ul> <li>unranked: No ranking at all, for a dummy baseline.</li> <li>bm25: The classic BM25 ranker.</li> <li>semantic: Using <code>closeness(query_embedding, document_embedding)</code> only.</li> <li>hybrid: Combining BM25 and semantic search - <code>closeness(query_embedding, document_embedding) + log10( bm25(doc) )</code>.</li> <li>hybrid_filter: Same as the previous, but with a filter to exclude hits based on some heuristics.</li> </ul>"},{"location":"examples/evaluating-with-snowflake-arctic-embed.html#dumping-the-application-package-to-files","title":"Dumping the application package to files\u00b6","text":"<p>This is a good practice to inspect and understand the structure of the application package and schema files, generated by pyvespa.</p>"},{"location":"examples/evaluating-with-snowflake-arctic-embed.html#choosing-a-metric","title":"Choosing a metric\u00b6","text":"<p>Check out this wikipedia-article for an overview of evaluation measures in information retrieval.</p> <p>In our case, we have a query and ranked documents as the ground truth.</p> <p>When evaluating a ranking against our ground truth ranking, the Normalized Discounted Cumulative Gain (NDCG) metric is a good choice.</p> <p>The NDCG is a measure of ranking quality. It is calculated as the sum of the discounted gain of the relevant documents(DCG), divided by the ideal DCG. The ideal DCG is the DCG of the perfect ranking, where the documents are ordered by relevance.</p> <p>If you are already familiar with NDCG, feel free to skip this part. There is also an implementation in scikit-learn that you can use.</p> <p>The formula for NDCG is:</p> <p>$$ NDCG = \\frac{DCG}{IDCG} $$</p> <p>where:</p> <p>$$ DCG = \\sum_{i=1}^{n} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)} $$</p> <p>Let us create a function to calculate the NDCG for a given ranking.</p>"},{"location":"examples/evaluating-with-snowflake-arctic-embed.html#next-steps","title":"Next steps\u00b6","text":"<p>Check out global reranking strategies, and try to introduce a global_phase reranking strategy.</p>"},{"location":"examples/evaluating-with-snowflake-arctic-embed.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"examples/feed_performance.html","title":"Feed performance","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Install Vespa CLI. The <code>vespacli</code> python package is just a thin wrapper, allowing for installation through pypi.</p> <p>Do NOT install if you already have the Vespa CLI installed.</p> In\u00a0[\u00a0]: Copied! <pre>#!pip3 install vespacli\n</pre> #!pip3 install vespacli <p>Install pyvespa, other dependencies, and start the Docker Daemon.</p> In\u00a0[1]: Copied! <pre>#!pip3 install pyvespa datasets plotly&gt;=5.20\n#!docker info\n</pre> #!pip3 install pyvespa datasets plotly&gt;=5.20 #!docker info In\u00a0[2]: Copied! <pre>from vespa.package import (\n    ApplicationPackage,\n    Field,\n    Schema,\n    Document,\n    FieldSet,\n)\n\npackage = ApplicationPackage(\n    name=\"pyvespafeed\",\n    schema=[\n        Schema(\n            name=\"doc\",\n            document=Document(\n                fields=[\n                    Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n                    Field(name=\"text\", type=\"string\", indexing=[\"summary\"]),\n                ]\n            ),\n            fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n        )\n    ],\n)\n</pre> from vespa.package import (     ApplicationPackage,     Field,     Schema,     Document,     FieldSet, )  package = ApplicationPackage(     name=\"pyvespafeed\",     schema=[         Schema(             name=\"doc\",             document=Document(                 fields=[                     Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),                     Field(name=\"text\", type=\"string\", indexing=[\"summary\"]),                 ]             ),             fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],         )     ], ) <p>Note that the <code>ApplicationPackage</code> name cannot have <code>-</code> or <code>_</code>.</p> In\u00a0[3]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=package)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy(application_package=package) <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 15/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 20/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 25/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p><code>app</code> now holds a reference to a Vespa instance.</p> In\u00a0[4]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\n    \"Cohere/wikipedia-2023-11-embed-multilingual-v3\",\n    \"simple\",\n    split=\"train\",\n    streaming=True,\n)\n</pre> from datasets import load_dataset  dataset = load_dataset(     \"Cohere/wikipedia-2023-11-embed-multilingual-v3\",     \"simple\",     split=\"train\",     streaming=True, ) In\u00a0[5]: Copied! <pre>def get_dataset(n_docs: int = 1000):\n    return (\n        dataset.map(lambda x: {\"id\": x[\"_id\"] + \"-iter\", \"fields\": {\"text\": x[\"text\"]}})\n        .select_columns([\"id\", \"fields\"])\n        .take(n_docs)\n    )\n</pre> def get_dataset(n_docs: int = 1000):     return (         dataset.map(lambda x: {\"id\": x[\"_id\"] + \"-iter\", \"fields\": {\"text\": x[\"text\"]}})         .select_columns([\"id\", \"fields\"])         .take(n_docs)     ) In\u00a0[6]: Copied! <pre>from dataclasses import dataclass\nfrom typing import Callable, Optional, Iterable, Dict\n\n\n@dataclass\nclass FeedParams:\n    name: str\n    num_docs: int\n    max_connections: int\n    function_name: str\n    max_workers: Optional[int] = None\n    max_queue_size: Optional[int] = None\n    num_concurrent_requests: Optional[int] = None\n\n\n@dataclass\nclass FeedResult(FeedParams):\n    feed_time: Optional[float] = None\n</pre> from dataclasses import dataclass from typing import Callable, Optional, Iterable, Dict   @dataclass class FeedParams:     name: str     num_docs: int     max_connections: int     function_name: str     max_workers: Optional[int] = None     max_queue_size: Optional[int] = None     num_concurrent_requests: Optional[int] = None   @dataclass class FeedResult(FeedParams):     feed_time: Optional[float] = None In\u00a0[7]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         ) In\u00a0[8]: Copied! <pre>import time\nimport asyncio\n\n\ndef feed_sync(params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:\n    start_time = time.time()\n    with app.syncio(connections=params.max_connections):\n        for doc in data:\n            app.feed_data_point(\n                data_id=doc[\"id\"],\n                fields=doc[\"fields\"],\n                schema=\"doc\",\n                namespace=\"pyvespa-feed\",\n                callback=callback,\n            )\n    end_time = time.time()\n    return FeedResult(\n        **params.__dict__,\n        feed_time=end_time - start_time,\n    )\n\n\nasync def feed_async(params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:\n    start_time = time.time()\n    tasks = []\n    # We use a semaphore to limit the number of concurrent requests, this is useful to avoid\n    # running into memory issues when feeding a large number of documents\n    semaphore = asyncio.Semaphore(params.num_concurrent_requests)\n\n    async with app.asyncio(\n        connections=params.max_connections, timeout=params.num_docs // 10\n    ) as async_app:\n        for doc in data:\n            async with semaphore:\n                task = asyncio.create_task(\n                    async_app.feed_data_point(\n                        data_id=doc[\"id\"],\n                        fields=doc[\"fields\"],\n                        schema=\"doc\",\n                        namespace=\"pyvespa-feed\",\n                        timeout=10,\n                    )\n                )\n                tasks.append(task)\n\n        await asyncio.wait(tasks, return_when=asyncio.ALL_COMPLETED)\n\n    end_time = time.time()\n    return FeedResult(\n        **params.__dict__,\n        feed_time=end_time - start_time,\n    )\n\n\ndef feed_iterable(params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:\n    start = time.time()\n    app.feed_iterable(\n        data,\n        schema=\"doc\",\n        namespace=\"pyvespa-feed\",\n        operation_type=\"feed\",\n        max_queue_size=params.max_queue_size,\n        max_workers=params.max_workers,\n        max_connections=params.max_connections,\n        callback=callback,\n    )\n    end = time.time()\n    sync_feed_time = end - start\n    return FeedResult(\n        **params.__dict__,\n        feed_time=sync_feed_time,\n    )\n</pre> import time import asyncio   def feed_sync(params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:     start_time = time.time()     with app.syncio(connections=params.max_connections):         for doc in data:             app.feed_data_point(                 data_id=doc[\"id\"],                 fields=doc[\"fields\"],                 schema=\"doc\",                 namespace=\"pyvespa-feed\",                 callback=callback,             )     end_time = time.time()     return FeedResult(         **params.__dict__,         feed_time=end_time - start_time,     )   async def feed_async(params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:     start_time = time.time()     tasks = []     # We use a semaphore to limit the number of concurrent requests, this is useful to avoid     # running into memory issues when feeding a large number of documents     semaphore = asyncio.Semaphore(params.num_concurrent_requests)      async with app.asyncio(         connections=params.max_connections, timeout=params.num_docs // 10     ) as async_app:         for doc in data:             async with semaphore:                 task = asyncio.create_task(                     async_app.feed_data_point(                         data_id=doc[\"id\"],                         fields=doc[\"fields\"],                         schema=\"doc\",                         namespace=\"pyvespa-feed\",                         timeout=10,                     )                 )                 tasks.append(task)          await asyncio.wait(tasks, return_when=asyncio.ALL_COMPLETED)      end_time = time.time()     return FeedResult(         **params.__dict__,         feed_time=end_time - start_time,     )   def feed_iterable(params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:     start = time.time()     app.feed_iterable(         data,         schema=\"doc\",         namespace=\"pyvespa-feed\",         operation_type=\"feed\",         max_queue_size=params.max_queue_size,         max_workers=params.max_workers,         max_connections=params.max_connections,         callback=callback,     )     end = time.time()     sync_feed_time = end - start     return FeedResult(         **params.__dict__,         feed_time=sync_feed_time,     ) In\u00a0[9]: Copied! <pre>from itertools import product\n\n# We will only run for 1000 documents here as notebook is run as part of CI.\n# But you will see some plots when run with 100k documents as well.\n\nnum_docs = [1000]\n\nparams_by_function = {\n    \"feed_sync\": {\n        \"num_docs\": num_docs,\n        \"max_connections\": [16, 64],\n    },\n    \"feed_async\": {\n        \"num_docs\": num_docs,\n        \"max_connections\": [16, 64],\n        \"num_concurrent_requests\": [1000, 10_000],\n    },\n    \"feed_iterable\": {\n        \"num_docs\": num_docs,\n        \"max_connections\": [64, 128],\n        \"max_workers\": [16, 64],\n        \"max_queue_size\": [1000, 10000],\n    },\n}\n\nfeed_params = []\n# Create one FeedParams instance of each permutation\nfor func, parameters in params_by_function.items():\n    print(f\"Function: {func}\")\n    keys, values = zip(*parameters.items())\n    for combination in product(*values):\n        settings = dict(zip(keys, combination))\n        print(settings)\n        feed_params.append(\n            FeedParams(\n                name=f\"{settings['num_docs']}_{settings['max_connections']}_{settings.get('max_workers', 0)}_{func}\",\n                function_name=func,\n                **settings,\n            )\n        )\n    print(\"\\n\")  # Just to add space between different functions\n</pre> from itertools import product  # We will only run for 1000 documents here as notebook is run as part of CI. # But you will see some plots when run with 100k documents as well.  num_docs = [1000]  params_by_function = {     \"feed_sync\": {         \"num_docs\": num_docs,         \"max_connections\": [16, 64],     },     \"feed_async\": {         \"num_docs\": num_docs,         \"max_connections\": [16, 64],         \"num_concurrent_requests\": [1000, 10_000],     },     \"feed_iterable\": {         \"num_docs\": num_docs,         \"max_connections\": [64, 128],         \"max_workers\": [16, 64],         \"max_queue_size\": [1000, 10000],     }, }  feed_params = [] # Create one FeedParams instance of each permutation for func, parameters in params_by_function.items():     print(f\"Function: {func}\")     keys, values = zip(*parameters.items())     for combination in product(*values):         settings = dict(zip(keys, combination))         print(settings)         feed_params.append(             FeedParams(                 name=f\"{settings['num_docs']}_{settings['max_connections']}_{settings.get('max_workers', 0)}_{func}\",                 function_name=func,                 **settings,             )         )     print(\"\\n\")  # Just to add space between different functions <pre>Function: feed_sync\n{'num_docs': 1000, 'max_connections': 16}\n{'num_docs': 1000, 'max_connections': 64}\n\n\nFunction: feed_async\n{'num_docs': 1000, 'max_connections': 16, 'num_concurrent_requests': 1000}\n{'num_docs': 1000, 'max_connections': 16, 'num_concurrent_requests': 10000}\n{'num_docs': 1000, 'max_connections': 64, 'num_concurrent_requests': 1000}\n{'num_docs': 1000, 'max_connections': 64, 'num_concurrent_requests': 10000}\n\n\nFunction: feed_iterable\n{'num_docs': 1000, 'max_connections': 64, 'max_workers': 16, 'max_queue_size': 1000}\n{'num_docs': 1000, 'max_connections': 64, 'max_workers': 16, 'max_queue_size': 10000}\n{'num_docs': 1000, 'max_connections': 64, 'max_workers': 64, 'max_queue_size': 1000}\n{'num_docs': 1000, 'max_connections': 64, 'max_workers': 64, 'max_queue_size': 10000}\n{'num_docs': 1000, 'max_connections': 128, 'max_workers': 16, 'max_queue_size': 1000}\n{'num_docs': 1000, 'max_connections': 128, 'max_workers': 16, 'max_queue_size': 10000}\n{'num_docs': 1000, 'max_connections': 128, 'max_workers': 64, 'max_queue_size': 1000}\n{'num_docs': 1000, 'max_connections': 128, 'max_workers': 64, 'max_queue_size': 10000}\n\n\n</pre> In\u00a0[10]: Copied! <pre>print(f\"Total number of feed_params: {len(feed_params)}\")\n</pre> print(f\"Total number of feed_params: {len(feed_params)}\") <pre>Total number of feed_params: 14\n</pre> <p>Now, we will need a way to retrieve the callable function from the function name.</p> In\u00a0[11]: Copied! <pre># Get reference to function from string name\ndef get_func_from_str(func_name: str) -&gt; Callable:\n    return globals()[func_name]\n</pre> # Get reference to function from string name def get_func_from_str(func_name: str) -&gt; Callable:     return globals()[func_name] In\u00a0[12]: Copied! <pre>from typing import Iterable, Dict\n\n\ndef delete_data(data: Iterable[Dict]):\n    app.feed_iterable(\n        iter=data,\n        schema=\"doc\",\n        namespace=\"pyvespa-feed\",\n        operation_type=\"delete\",\n        callback=callback,\n    )\n</pre> from typing import Iterable, Dict   def delete_data(data: Iterable[Dict]):     app.feed_iterable(         iter=data,         schema=\"doc\",         namespace=\"pyvespa-feed\",         operation_type=\"delete\",         callback=callback,     ) <p>The line below is used to make the code run in Jupyter, as it is already running an event loop</p> In\u00a0[13]: Copied! <pre>import nest_asyncio\n\nnest_asyncio.apply()\n</pre> import nest_asyncio  nest_asyncio.apply() In\u00a0[14]: Copied! <pre>results = []\nfor params in feed_params:\n    print(\"-\" * 50)\n    print(\"Starting feed with params:\")\n    print(params)\n    data = get_dataset(params.num_docs)\n    if \"async\" not in params.function_name:\n        feed_result = get_func_from_str(params.function_name)(params=params, data=data)\n    else:\n        feed_result = asyncio.run(\n            get_func_from_str(params.function_name)(params=params, data=data)\n        )\n    print(feed_result.feed_time)\n    results.append(feed_result)\n    print(\"Deleting data\")\n    delete_data(data)\n</pre> results = [] for params in feed_params:     print(\"-\" * 50)     print(\"Starting feed with params:\")     print(params)     data = get_dataset(params.num_docs)     if \"async\" not in params.function_name:         feed_result = get_func_from_str(params.function_name)(params=params, data=data)     else:         feed_result = asyncio.run(             get_func_from_str(params.function_name)(params=params, data=data)         )     print(feed_result.feed_time)     results.append(feed_result)     print(\"Deleting data\")     delete_data(data) <pre>--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_16_0_feed_sync', num_docs=1000, max_connections=16, function_name='feed_sync', max_workers=None, max_queue_size=None, num_concurrent_requests=None)\n15.175757884979248\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_0_feed_sync', num_docs=1000, max_connections=64, function_name='feed_sync', max_workers=None, max_queue_size=None, num_concurrent_requests=None)\n12.517201900482178\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_16_0_feed_async', num_docs=1000, max_connections=16, function_name='feed_async', max_workers=None, max_queue_size=None, num_concurrent_requests=1000)\n4.953256130218506\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_16_0_feed_async', num_docs=1000, max_connections=16, function_name='feed_async', max_workers=None, max_queue_size=None, num_concurrent_requests=10000)\n4.914812088012695\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_0_feed_async', num_docs=1000, max_connections=64, function_name='feed_async', max_workers=None, max_queue_size=None, num_concurrent_requests=1000)\n4.711783170700073\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_0_feed_async', num_docs=1000, max_connections=64, function_name='feed_async', max_workers=None, max_queue_size=None, num_concurrent_requests=10000)\n4.942464113235474\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_16_feed_iterable', num_docs=1000, max_connections=64, function_name='feed_iterable', max_workers=16, max_queue_size=1000, num_concurrent_requests=None)\n5.707854270935059\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_16_feed_iterable', num_docs=1000, max_connections=64, function_name='feed_iterable', max_workers=16, max_queue_size=10000, num_concurrent_requests=None)\n5.798462867736816\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_64_feed_iterable', num_docs=1000, max_connections=64, function_name='feed_iterable', max_workers=64, max_queue_size=1000, num_concurrent_requests=None)\n5.706255674362183\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_64_feed_iterable', num_docs=1000, max_connections=64, function_name='feed_iterable', max_workers=64, max_queue_size=10000, num_concurrent_requests=None)\n5.976051330566406\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_128_16_feed_iterable', num_docs=1000, max_connections=128, function_name='feed_iterable', max_workers=16, max_queue_size=1000, num_concurrent_requests=None)\n5.959493160247803\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_128_16_feed_iterable', num_docs=1000, max_connections=128, function_name='feed_iterable', max_workers=16, max_queue_size=10000, num_concurrent_requests=None)\n5.757789134979248\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_128_64_feed_iterable', num_docs=1000, max_connections=128, function_name='feed_iterable', max_workers=64, max_queue_size=1000, num_concurrent_requests=None)\n5.612061023712158\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_128_64_feed_iterable', num_docs=1000, max_connections=128, function_name='feed_iterable', max_workers=64, max_queue_size=10000, num_concurrent_requests=None)\n5.622947692871094\nDeleting data\n</pre> In\u00a0[15]: Copied! <pre># Create a pandas DataFrame with the results\nimport pandas as pd\n\ndf = pd.DataFrame([result.__dict__ for result in results])\ndf[\"requests_per_second\"] = df[\"num_docs\"] / df[\"feed_time\"]\ndf\n</pre> # Create a pandas DataFrame with the results import pandas as pd  df = pd.DataFrame([result.__dict__ for result in results]) df[\"requests_per_second\"] = df[\"num_docs\"] / df[\"feed_time\"] df Out[15]: name num_docs max_connections function_name max_workers max_queue_size num_concurrent_requests feed_time requests_per_second 0 1000_16_0_feed_sync 1000 16 feed_sync NaN NaN NaN 15.175758 65.894567 1 1000_64_0_feed_sync 1000 64 feed_sync NaN NaN NaN 12.517202 79.890059 2 1000_16_0_feed_async 1000 16 feed_async NaN NaN 1000.0 4.953256 201.887400 3 1000_16_0_feed_async 1000 16 feed_async NaN NaN 10000.0 4.914812 203.466579 4 1000_64_0_feed_async 1000 64 feed_async NaN NaN 1000.0 4.711783 212.233875 5 1000_64_0_feed_async 1000 64 feed_async NaN NaN 10000.0 4.942464 202.328227 6 1000_64_16_feed_iterable 1000 64 feed_iterable 16.0 1000.0 NaN 5.707854 175.197185 7 1000_64_16_feed_iterable 1000 64 feed_iterable 16.0 10000.0 NaN 5.798463 172.459499 8 1000_64_64_feed_iterable 1000 64 feed_iterable 64.0 1000.0 NaN 5.706256 175.246266 9 1000_64_64_feed_iterable 1000 64 feed_iterable 64.0 10000.0 NaN 5.976051 167.334573 10 1000_128_16_feed_iterable 1000 128 feed_iterable 16.0 1000.0 NaN 5.959493 167.799505 11 1000_128_16_feed_iterable 1000 128 feed_iterable 16.0 10000.0 NaN 5.757789 173.677774 12 1000_128_64_feed_iterable 1000 128 feed_iterable 64.0 1000.0 NaN 5.612061 178.187656 13 1000_128_64_feed_iterable 1000 128 feed_iterable 64.0 10000.0 NaN 5.622948 177.842664 In\u00a0[16]: Copied! <pre>import plotly.express as px\n\n\ndef plot_performance(df: pd.DataFrame):\n    # Create a scatter plot with logarithmic scale for both axes using Plotly Express\n    fig = px.scatter(\n        df,\n        x=\"num_docs\",\n        y=\"requests_per_second\",\n        color=\"function_name\",  # Defines color based on different functions\n        log_x=True,  # Set x-axis to logarithmic scale\n        log_y=False,  # If you also want the y-axis in logarithmic scale, set this to True\n        title=\"Performance: Requests per Second vs. Number of Documents\",\n        labels={  # Customizing axis labels\n            \"num_docs\": \"Number of Documents\",\n            \"requests_per_second\": \"Requests per Second\",\n            \"max_workers\": \"max_workers\",\n            \"max_queue_size\": \"max_queue_size\",\n            \"max_connections\": \"max_connections\",\n            \"num_concurrent_requests\": \"num_concurrent_requests\",\n        },\n        template=\"plotly_white\",  # This sets the style to a white background, adhering to Tufte's minimalist principles\n        hover_data=[\n            \"max_workers\",\n            \"max_queue_size\",\n            \"max_connections\",\n            \"num_concurrent_requests\",\n        ],  # Additional information to show on hover\n    )\n\n    # Update layout for better readability, similar to 'talk' context in Seaborn\n    fig.update_layout(\n        font=dict(\n            size=16,  # Adjusting font size for better visibility, similar to 'talk' context\n        ),\n        legend_title_text=\"Function Details\",  # Custom legend title\n        legend=dict(\n            title_font_size=16,\n            x=800,  # Adjusting legend position similar to bbox_to_anchor in Matplotlib\n            xanchor=\"auto\",\n            y=1,\n            yanchor=\"auto\",\n        ),\n        width=800,  # Adjusting width of the plot\n    )\n    fig.update_xaxes(\n        tickvals=[1000, 10000, 100000],  # Set specific tick values\n        ticktext=[\"1k\", \"10k\", \"100k\"],  # Set corresponding tick labels\n    )\n\n    fig.update_traces(\n        marker=dict(size=12, opacity=0.7)\n    )  # Adjust marker size and opacity\n    # Show plot\n    fig.show()\n    # Save plot as HTML file\n    fig.write_html(\"performance.html\")\n\n\nplot_performance(df)\n</pre> import plotly.express as px   def plot_performance(df: pd.DataFrame):     # Create a scatter plot with logarithmic scale for both axes using Plotly Express     fig = px.scatter(         df,         x=\"num_docs\",         y=\"requests_per_second\",         color=\"function_name\",  # Defines color based on different functions         log_x=True,  # Set x-axis to logarithmic scale         log_y=False,  # If you also want the y-axis in logarithmic scale, set this to True         title=\"Performance: Requests per Second vs. Number of Documents\",         labels={  # Customizing axis labels             \"num_docs\": \"Number of Documents\",             \"requests_per_second\": \"Requests per Second\",             \"max_workers\": \"max_workers\",             \"max_queue_size\": \"max_queue_size\",             \"max_connections\": \"max_connections\",             \"num_concurrent_requests\": \"num_concurrent_requests\",         },         template=\"plotly_white\",  # This sets the style to a white background, adhering to Tufte's minimalist principles         hover_data=[             \"max_workers\",             \"max_queue_size\",             \"max_connections\",             \"num_concurrent_requests\",         ],  # Additional information to show on hover     )      # Update layout for better readability, similar to 'talk' context in Seaborn     fig.update_layout(         font=dict(             size=16,  # Adjusting font size for better visibility, similar to 'talk' context         ),         legend_title_text=\"Function Details\",  # Custom legend title         legend=dict(             title_font_size=16,             x=800,  # Adjusting legend position similar to bbox_to_anchor in Matplotlib             xanchor=\"auto\",             y=1,             yanchor=\"auto\",         ),         width=800,  # Adjusting width of the plot     )     fig.update_xaxes(         tickvals=[1000, 10000, 100000],  # Set specific tick values         ticktext=[\"1k\", \"10k\", \"100k\"],  # Set corresponding tick labels     )      fig.update_traces(         marker=dict(size=12, opacity=0.7)     )  # Adjust marker size and opacity     # Show plot     fig.show()     # Save plot as HTML file     fig.write_html(\"performance.html\")   plot_performance(df) <p>Here is the corresponding plot when run with 1k, 10k, and 100k documents:</p> <p></p> <p>Interesting. Let's try to summarize the insights we got from this experiment:</p> <ul> <li>The <code>feed_sync</code> method is the slowest, and does not benefit much from increasing <code>max_connections</code>. As there is no concurrency, and each request is blocking, this will be bound by the network latency, which in many cases will be a lot higher than when running against a local VespaDocker instance. For example, if you have 100ms latency against your Vespa instance, you can only feed 10 documents per second using the <code>VespaSync</code> method.</li> <li>The <code>feed_async</code> method is the fastest, and benefits slightly from increasing <code>max_connections</code> regardless of the number of documents. This method is non-blocking, and will likely be even more beneficial when running against a remote Vespa instance, such as Vespa Cloud, when network latency is higher.</li> <li>The <code>feed_iterable</code> performance is somewhere in between the other two methods, and benefits a lot from increasing <code>num_workers</code> when the number of documents increases.</li> </ul> <p>We have not looked at multiprocessing, but there is definitively room to utilize more cores to improve further upon these results. But there is one alternative that it is interesting to compare against, the Vespa CLI.</p> In\u00a0[17]: Copied! <pre>for n in num_docs:\n    print(f\"Getting dataset with {n} docs...\")\n    # First, let's load the dataset in non-streaming mode this time, as we want to save it to a jsonl file\n    dataset_cli = load_dataset(\n        \"Cohere/wikipedia-2023-11-embed-multilingual-v3\",\n        \"simple\",\n        split=f\"train[:{n}]\",  # Notice the slicing here, see https://huggingface.co/docs/datasets/loading#slice-splits\n        streaming=False,\n    )\n    # Map to the format expected by the CLI.\n    # Note that this differs a little bit from the format expected by the Python API.\n    dataset_cli = dataset_cli.map(\n        lambda x: {\n            \"put\": f\"id:pyvespa-feed:doc::{x['_id']}-json\",\n            \"fields\": {\"text\": x[\"text\"]},\n        }\n    ).select_columns([\"put\", \"fields\"])\n    # Save to a jsonl file\n    assert len(dataset_cli) == n\n    dataset_cli.to_json(f\"vespa_feed-{n}.json\", orient=\"records\", lines=True)\n</pre> for n in num_docs:     print(f\"Getting dataset with {n} docs...\")     # First, let's load the dataset in non-streaming mode this time, as we want to save it to a jsonl file     dataset_cli = load_dataset(         \"Cohere/wikipedia-2023-11-embed-multilingual-v3\",         \"simple\",         split=f\"train[:{n}]\",  # Notice the slicing here, see https://huggingface.co/docs/datasets/loading#slice-splits         streaming=False,     )     # Map to the format expected by the CLI.     # Note that this differs a little bit from the format expected by the Python API.     dataset_cli = dataset_cli.map(         lambda x: {             \"put\": f\"id:pyvespa-feed:doc::{x['_id']}-json\",             \"fields\": {\"text\": x[\"text\"]},         }     ).select_columns([\"put\", \"fields\"])     # Save to a jsonl file     assert len(dataset_cli) == n     dataset_cli.to_json(f\"vespa_feed-{n}.json\", orient=\"records\", lines=True) <pre>Getting dataset with 1000 docs...\n</pre> <pre>Creating json from Arrow format:   0%|          | 0/1 [00:00&lt;?, ?ba/s]</pre> <p>Let's look at the first line of one of the saved files to verify the format.</p> In\u00a0[18]: Copied! <pre>from pprint import pprint\nimport json\n\nwith open(\"vespa_feed-1000.json\", \"r\") as f:\n    sample = f.readline()\n    pprint(json.loads(sample))\n</pre> from pprint import pprint import json  with open(\"vespa_feed-1000.json\", \"r\") as f:     sample = f.readline()     pprint(json.loads(sample)) <pre>{'fields': {'text': 'April (Apr.) is the fourth month of the year in the '\n                    'Julian and Gregorian calendars, and comes between March '\n                    'and May. It is one of the four months to have 30 days.'},\n 'put': 'id:pyvespa-feed:doc::20231101.simple_1_0-json'}\n</pre> <p>Ok, now we are ready to feed the data using Vespa CLI. We also want to capture the output of feed statistics for each file.</p> In\u00a0[19]: Copied! <pre>cli_results = {}\nfor n in num_docs:\n    print(f\"Feeding {n} docs...\")\n    output_list = !vespa feed vespa_feed-{n}.json\n    results = json.loads(\"\".join(output_list))\n    pprint(results)\n    cli_results[n] = results\n</pre> cli_results = {} for n in num_docs:     print(f\"Feeding {n} docs...\")     output_list = !vespa feed vespa_feed-{n}.json     results = json.loads(\"\".join(output_list))     pprint(results)     cli_results[n] = results <pre>Feeding 1000 docs...\n{'feeder.error.count': 0,\n 'feeder.inflight.count': 0,\n 'feeder.ok.count': 1000,\n 'feeder.ok.rate': 1000.0,\n 'feeder.operation.count': 1000,\n 'feeder.seconds': 0.826,\n 'http.exception.count': 0,\n 'http.request.MBps': 1.423,\n 'http.request.bytes': 1422960,\n 'http.request.count': 4817,\n 'http.response.MBps': 0.712,\n 'http.response.bytes': 712421,\n 'http.response.code.counts': {'200': 1000, '429': 3817},\n 'http.response.count': 4817,\n 'http.response.error.count': 3817,\n 'http.response.latency.millis.avg': 107,\n 'http.response.latency.millis.max': 342,\n 'http.response.latency.millis.min': 0}\n</pre> In\u00a0[20]: Copied! <pre>cli_results\n</pre> cli_results Out[20]: <pre>{1000: {'feeder.operation.count': 1000,\n  'feeder.seconds': 0.826,\n  'feeder.ok.count': 1000,\n  'feeder.ok.rate': 1000.0,\n  'feeder.error.count': 0,\n  'feeder.inflight.count': 0,\n  'http.request.count': 4817,\n  'http.request.bytes': 1422960,\n  'http.request.MBps': 1.423,\n  'http.exception.count': 0,\n  'http.response.count': 4817,\n  'http.response.bytes': 712421,\n  'http.response.MBps': 0.712,\n  'http.response.error.count': 3817,\n  'http.response.latency.millis.min': 0,\n  'http.response.latency.millis.avg': 107,\n  'http.response.latency.millis.max': 342,\n  'http.response.code.counts': {'200': 1000, '429': 3817}}}</pre> In\u00a0[21]: Copied! <pre># Let's add the CLI results to the DataFrame\ndf_cli = pd.DataFrame(\n    [\n        {\n            \"name\": f\"{n}_cli\",\n            \"num_docs\": n,\n            \"max_connections\": \"unknown\",\n            \"function_name\": \"cli\",\n            \"max_workers\": \"unknown\",\n            \"max_queue_size\": \"n/a\",\n            \"feed_time\": result[\"feeder.seconds\"],\n        }\n        for n, result in cli_results.items()\n    ]\n)\ndf_cli[\"requests_per_second\"] = df_cli[\"num_docs\"] / df_cli[\"feed_time\"]\ndf_cli\n</pre> # Let's add the CLI results to the DataFrame df_cli = pd.DataFrame(     [         {             \"name\": f\"{n}_cli\",             \"num_docs\": n,             \"max_connections\": \"unknown\",             \"function_name\": \"cli\",             \"max_workers\": \"unknown\",             \"max_queue_size\": \"n/a\",             \"feed_time\": result[\"feeder.seconds\"],         }         for n, result in cli_results.items()     ] ) df_cli[\"requests_per_second\"] = df_cli[\"num_docs\"] / df_cli[\"feed_time\"] df_cli Out[21]: name num_docs max_connections function_name max_workers max_queue_size feed_time requests_per_second 0 1000_cli 1000 unknown cli unknown n/a 0.826 1210.653753 In\u00a0[22]: Copied! <pre>df_total = pd.concat([df, df_cli])\n\nplot_performance(df_total)\n</pre> df_total = pd.concat([df, df_cli])  plot_performance(df_total) <p>And here is the corresponding plot when run with 1k, 10k, and 100k documents:</p> <p></p> <p>As you can tell, the CLI is orders of magnitude faster.</p> <ul> <li>Prefer to use the CLI if you care about performance. \ud83d\ude80</li> <li>If you are using Python, prefer the async method, as it is the fastest way to feed data using <code>pyvespa</code>.</li> </ul> In\u00a0[23]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"examples/feed_performance.html#feeding-performance","title":"Feeding performance\u00b6","text":"<p>This explorative notebook intends to shine some light on the different modes of feeding documents to Vespa. We will look at these 4 different methods:</p> <ol> <li>Using <code>VespaSync</code>.</li> <li>Using <code>VespaAsync</code>.</li> <li>Using <code>feed_iterable()</code></li> <li>Using Vespa CLI</li> </ol>"},{"location":"examples/feed_performance.html#create-an-application-package","title":"Create an application package\u00b6","text":"<p>The application package has all the Vespa configuration files.</p> <p>For this demo, we will just use a dummy application package without any indexing, just to ease the load for the server, as it is the clients we want to compare in this experiment.</p>"},{"location":"examples/feed_performance.html#deploy-the-vespa-application","title":"Deploy the Vespa application\u00b6","text":"<p>Deploy <code>package</code> on the local machine using Docker, without leaving the notebook, by creating an instance of VespaDocker. <code>VespaDocker</code> connects to the local Docker daemon socket and starts the Vespa docker image.</p> <p>If this step fails, please check that the Docker daemon is running, and that the Docker daemon socket can be used by clients (Configurable under advanced settings in Docker Desktop).</p>"},{"location":"examples/feed_performance.html#preparing-the-data","title":"Preparing the data\u00b6","text":"<p>In this example we use HF Datasets library to stream the \"Cohere/wikipedia-2023-11-embed-multilingual-v3\" dataset and index in our newly deployed Vespa instance.</p> <p>The dataset contains wikipedia-pages, and their corresponding embeddings.</p> <p>For this exploration we will just use the <code>id</code> and <code>text</code>-fields</p> <p>The following uses the stream option of datasets to stream the data without downloading all the contents locally.</p> <p>The <code>map</code> functionality allows us to convert the dataset fields into the expected feed format for <code>pyvespa</code> which expects a dict with the keys <code>id</code> and <code>fields</code>:</p> <p><code>{\u00a0\"id\": \"vespa-document-id\", \"fields\": {\"vespa_field\": \"vespa-field-value\"}}</code></p>"},{"location":"examples/feed_performance.html#utility-function-to-stream-different-number-of-documents","title":"Utility function to stream different number of documents\u00b6","text":""},{"location":"examples/feed_performance.html#a-dataclass-to-store-the-parameters-and-results-of-the-different-feeding-methods","title":"A dataclass to store the parameters and results of the different feeding methods\u00b6","text":""},{"location":"examples/feed_performance.html#a-common-callback-function-to-notify-if-something-goes-wrong","title":"A common callback function to notify if something goes wrong\u00b6","text":""},{"location":"examples/feed_performance.html#defining-our-feeding-functions","title":"Defining our feeding functions\u00b6","text":""},{"location":"examples/feed_performance.html#defining-our-hyperparameters","title":"Defining our hyperparameters\u00b6","text":""},{"location":"examples/feed_performance.html#function-to-clean-up-after-each-feed","title":"Function to clean up after each feed\u00b6","text":"<p>For a fair comparison, we will delete the data before feeding it again.</p>"},{"location":"examples/feed_performance.html#main-experiment-loop","title":"Main experiment loop\u00b6","text":""},{"location":"examples/feed_performance.html#feeding-with-vespa-cli","title":"Feeding with Vespa CLI\u00b6","text":"<p>Vespa CLI is a command-line interface for interacting with Vespa.</p> <p>Among many useful features are a <code>vespa feed</code> command that is the recommended way of feeding large datasets into Vespa. This is optimized for high feeding performance, and it will be interesting to get a feel for how performant feeding to a local Vespa instance is using the CLI.</p> <p>Note that comparing feeding with the CLI is not entirely fair, as the CLI relies on prepared data files, while the pyvespa methods are streaming the data before feeding it.</p>"},{"location":"examples/feed_performance.html#prepare-the-data-for-vespa-cli","title":"Prepare the data for Vespa CLI\u00b6","text":"<p>Vespa CLI can feed data from either many .json files or a single .jsonl file with many documents. The json format needs to be in the following format:</p> <pre>{\n  \"put\": \"id:namespace:document-type::document-id\",\n  \"fields\": {\n    \"field1\": \"value1\",\n    \"field2\": \"value2\"\n  }\n}\n</pre> <p>Where, <code>put</code> is the document operation in this case. Other allowed operations are <code>get</code>, <code>update</code> and <code>remove</code>.</p> <p>For reference, see https://docs.vespa.ai/en/vespa-cli#cheat-sheet</p>"},{"location":"examples/feed_performance.html#getting-the-datasets-as-jsonl-files","title":"Getting the datasets as .jsonl files\u00b6","text":"<p>Now, let`s save the dataset to 3 different jsonl files of 1k, 10k, and 100k documents.</p>"},{"location":"examples/feed_performance.html#conclusion","title":"Conclusion\u00b6","text":""},{"location":"examples/feed_performance.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"examples/feed_performance.html#next-steps","title":"Next steps\u00b6","text":"<p>Check out some of the other examples in the documentation.</p>"},{"location":"examples/feed_performance_cloud.html","title":"Feed performance cloud","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Install Vespa CLI. The <code>vespacli</code> python package is just a thin wrapper, allowing for installation through pypi.</p> <p>Do NOT install if you already have the Vespa CLI installed.</p> <p>Install pyvespa, and other dependencies.</p> In\u00a0[1]: Copied! <pre>!pip3 install vespacli pyvespa datasets plotly&gt;=5.20\n</pre> !pip3 install vespacli pyvespa datasets plotly&gt;=5.20 <pre>zsh:1: 5.20 not found\n</pre> In\u00a0[2]: Copied! <pre>from vespa.package import (\n    ApplicationPackage,\n    Field,\n    Schema,\n    Document,\n    FieldSet,\n    HNSW,\n)\n\n# Define the application name (can NOT contain `_` or `-`)\n\napplication = \"feedperformancecloud\"\n\n\npackage = ApplicationPackage(\n    name=application,\n    schema=[\n        Schema(\n            name=\"doc\",\n            document=Document(\n                fields=[\n                    Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n                    Field(name=\"text\", type=\"string\", indexing=[\"index\", \"summary\"]),\n                    Field(\n                        name=\"embedding\",\n                        type=\"tensor&lt;float&gt;(x[1024])\",\n                        # Note that we are NOT embedding with a vespa model here, but that is also possible.\n                        indexing=[\"summary\", \"attribute\", \"index\"],\n                        ann=HNSW(distance_metric=\"angular\"),\n                    ),\n                ]\n            ),\n            fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n        )\n    ],\n)\n</pre> from vespa.package import (     ApplicationPackage,     Field,     Schema,     Document,     FieldSet,     HNSW, )  # Define the application name (can NOT contain `_` or `-`)  application = \"feedperformancecloud\"   package = ApplicationPackage(     name=application,     schema=[         Schema(             name=\"doc\",             document=Document(                 fields=[                     Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),                     Field(name=\"text\", type=\"string\", indexing=[\"index\", \"summary\"]),                     Field(                         name=\"embedding\",                         type=\"tensor(x[1024])\",                         # Note that we are NOT embedding with a vespa model here, but that is also possible.                         indexing=[\"summary\", \"attribute\", \"index\"],                         ann=HNSW(distance_metric=\"angular\"),                     ),                 ]             ),             fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],         )     ], ) <p>Note that the <code>ApplicationPackage</code> name cannot have <code>-</code> or <code>_</code>.</p> <p>Follow the instructions from the output above and add the control-plane key in the console at <code>https://console.vespa-cloud.com/tenant/TENANT_NAME/account/keys</code> (replace TENANT_NAME with your tenant name).</p> In\u00a0[3]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=application,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly   vespa_cloud = VespaCloud(     tenant=tenant_name,     application=application,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=package, ) <pre>Setting application...\nRunning: vespa config set application vespa-team.feedperformancecloud\nSetting target cloud...\nRunning: vespa config set target cloud\n\nApi-key found for control plane access. Using api-key.\n</pre> <p><code>app</code> now holds a reference to a VespaCloud instance.</p> In\u00a0[4]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 9 of dev-aws-us-east-1c for vespa-team.feedperformancecloud. This may take a few minutes the first time.\nINFO    [07:22:29]  Deploying platform version 8.392.14 and application dev build 7 for dev-aws-us-east-1c of default ...\nINFO    [07:22:30]  Using CA signed certificate version 1\nINFO    [07:22:30]  Using 1 nodes in container cluster 'feedperformancecloud_container'\nWARNING [07:22:33]  Auto-overriding validation which would be disallowed in production: certificate-removal: Data plane certificate(s) from cluster 'feedperformancecloud_container' is removed (removed certificates: [CN=cloud.vespa.example]) This can cause client connection issues.. To allow this add &lt;allow until='yyyy-mm-dd'&gt;certificate-removal&lt;/allow&gt; to validation-overrides.xml, see https://docs.vespa.ai/en/reference/validation-overrides.html\nINFO    [07:22:34]  Session 304192 for tenant 'vespa-team' prepared and activated.\nINFO    [07:22:35]  ######## Details for all nodes ########\nINFO    [07:22:35]  h95731a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:22:35]  --- platform vespa/cloud-tenant-rhel8:8.392.14\nINFO    [07:22:35]  --- container on port 4080 has not started \nINFO    [07:22:35]  --- metricsproxy-container on port 19092 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  h95729b.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:22:35]  --- platform vespa/cloud-tenant-rhel8:8.392.14\nINFO    [07:22:35]  --- storagenode on port 19102 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  --- searchnode on port 19107 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  --- distributor on port 19111 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  --- metricsproxy-container on port 19092 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  h93272g.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:22:35]  --- platform vespa/cloud-tenant-rhel8:8.392.14\nINFO    [07:22:35]  --- logserver-container on port 4080 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  --- metricsproxy-container on port 19092 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  h93272h.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:22:35]  --- platform vespa/cloud-tenant-rhel8:8.392.14\nINFO    [07:22:35]  --- container-clustercontroller on port 19050 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  --- metricsproxy-container on port 19092 has config generation 304192, wanted is 304192\nINFO    [07:22:42]  Found endpoints:\nINFO    [07:22:42]  - dev.aws-us-east-1c\nINFO    [07:22:42]   |-- https://b48e8812.bc737822.z.vespa-app.cloud/ (cluster 'feedperformancecloud_container')\nINFO    [07:22:44]  Deployment of new application complete!\nFound mtls endpoint for feedperformancecloud_container\nURL: https://b48e8812.bc737822.z.vespa-app.cloud/\nConnecting to https://b48e8812.bc737822.z.vespa-app.cloud/\nUsing mtls_key_cert Authentication against endpoint https://b48e8812.bc737822.z.vespa-app.cloud//ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p>Note that if you already have a Vespa Cloud instance running, the recommended way to initialize a <code>Vespa</code> instance is directly, by passing the <code>endpoint</code> and <code>tenant</code> parameters to the <code>Vespa</code> constructor, along with either:</p> <ol> <li>Key/cert for dataplane authentication (generated as part of deployment, copied into the application package, in <code>/security/clients.pem</code>, and <code>~/.vespa/mytenant.myapplication/data-plane-public-cert.pem</code> and <code>~/.vespa/mytenant.myapplication/data-plane-private-key.pem</code>).</li> </ol> <pre>from vespa.application import Vespa\n\napp: Vespa = Vespa(\n    url=\"https://my-endpoint.z.vespa-app.cloud\",\n    tenant=\"my-tenant\",\n    key_file=\"path/to/private-key.pem\",\n    cert_file=\"path/to/certificate.pem\",\n)\n</pre> <ol> <li>Using a token (must be generated in Vespa Cloud Console and defined in the application package, see https://cloud.vespa.ai/en/security/guide.</li> </ol> <pre>from vespa.application import Vespa\nimport os\n\napp: Vespa = Vespa(\n    url=\"https://my-endpoint.z.vespa-app.cloud\",\n    tenant=\"my-tenant\",\n    vespa_cloud_secret_token=os.getenv(\"VESPA_CLOUD_SECRET_TOKEN\"),\n)\n</pre> In\u00a0[5]: Copied! <pre>app.get_application_status()\n</pre> app.get_application_status() <pre>Using mtls_key_cert Authentication against endpoint https://b48e8812.bc737822.z.vespa-app.cloud//ApplicationStatus\n</pre> Out[5]: <pre>&lt;Response [200]&gt;</pre> In\u00a0[6]: Copied! <pre>from datasets import load_dataset\n</pre> from datasets import load_dataset <pre>/Users/thomas/.pyenv/versions/3.9.19/envs/pyvespa-dev/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[7]: Copied! <pre>def get_dataset(n_docs: int = 1000):\n    dataset = load_dataset(\n        \"Cohere/wikipedia-2023-11-embed-multilingual-v3\",\n        \"simple\",\n        split=f\"train[:{n_docs}]\",\n    )\n    dataset = dataset.map(\n        lambda x: {\n            \"id\": x[\"_id\"] + \"-iter\",\n            \"fields\": {\"text\": x[\"text\"], \"embedding\": x[\"emb\"]},\n        }\n    ).select_columns([\"id\", \"fields\"])\n    return dataset\n</pre> def get_dataset(n_docs: int = 1000):     dataset = load_dataset(         \"Cohere/wikipedia-2023-11-embed-multilingual-v3\",         \"simple\",         split=f\"train[:{n_docs}]\",     )     dataset = dataset.map(         lambda x: {             \"id\": x[\"_id\"] + \"-iter\",             \"fields\": {\"text\": x[\"text\"], \"embedding\": x[\"emb\"]},         }     ).select_columns([\"id\", \"fields\"])     return dataset In\u00a0[8]: Copied! <pre>from dataclasses import dataclass\nfrom typing import Callable, Optional, Iterable, Dict\n\n\n@dataclass\nclass FeedParams:\n    name: str\n    num_docs: int\n    max_connections: int\n    function_name: str\n    max_workers: Optional[int] = None\n    max_queue_size: Optional[int] = None\n\n\n@dataclass\nclass FeedResult(FeedParams):\n    feed_time: Optional[float] = None\n</pre> from dataclasses import dataclass from typing import Callable, Optional, Iterable, Dict   @dataclass class FeedParams:     name: str     num_docs: int     max_connections: int     function_name: str     max_workers: Optional[int] = None     max_queue_size: Optional[int] = None   @dataclass class FeedResult(FeedParams):     feed_time: Optional[float] = None In\u00a0[9]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         ) In\u00a0[10]: Copied! <pre>import time\nimport asyncio\nfrom vespa.application import Vespa\n</pre> import time import asyncio from vespa.application import Vespa In\u00a0[11]: Copied! <pre>def feed_iterable(app: Vespa, params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:\n    start = time.time()\n    app.feed_iterable(\n        data,\n        schema=\"doc\",\n        namespace=\"pyvespa-feed\",\n        operation_type=\"feed\",\n        max_queue_size=params.max_queue_size,\n        max_workers=params.max_workers,\n        max_connections=params.max_connections,\n        callback=callback,\n    )\n    end = time.time()\n    sync_feed_time = end - start\n    return FeedResult(\n        **params.__dict__,\n        feed_time=sync_feed_time,\n    )\n\n\ndef feed_async_iterable(\n    app: Vespa, params: FeedParams, data: Iterable[Dict]\n) -&gt; FeedResult:\n    start = time.time()\n    app.feed_async_iterable(\n        data,\n        schema=\"doc\",\n        namespace=\"pyvespa-feed\",\n        operation_type=\"feed\",\n        max_queue_size=params.max_queue_size,\n        max_workers=params.max_workers,\n        max_connections=params.max_connections,\n        callback=callback,\n    )\n    end = time.time()\n    sync_feed_time = end - start\n    return FeedResult(\n        **params.__dict__,\n        feed_time=sync_feed_time,\n    )\n</pre> def feed_iterable(app: Vespa, params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:     start = time.time()     app.feed_iterable(         data,         schema=\"doc\",         namespace=\"pyvespa-feed\",         operation_type=\"feed\",         max_queue_size=params.max_queue_size,         max_workers=params.max_workers,         max_connections=params.max_connections,         callback=callback,     )     end = time.time()     sync_feed_time = end - start     return FeedResult(         **params.__dict__,         feed_time=sync_feed_time,     )   def feed_async_iterable(     app: Vespa, params: FeedParams, data: Iterable[Dict] ) -&gt; FeedResult:     start = time.time()     app.feed_async_iterable(         data,         schema=\"doc\",         namespace=\"pyvespa-feed\",         operation_type=\"feed\",         max_queue_size=params.max_queue_size,         max_workers=params.max_workers,         max_connections=params.max_connections,         callback=callback,     )     end = time.time()     sync_feed_time = end - start     return FeedResult(         **params.__dict__,         feed_time=sync_feed_time,     ) In\u00a0[12]: Copied! <pre>from itertools import product\n\n# We will only run for up to 10 000 documents here as notebook is run as part of CI.\n\nnum_docs = [\n    1000,\n    5_000,\n    10_000,\n]\nparams_by_function = {\n    \"feed_async_iterable\": {\n        \"num_docs\": num_docs,\n        \"max_connections\": [1],\n        \"max_workers\": [64],\n        \"max_queue_size\": [2500],\n    },\n    \"feed_iterable\": {\n        \"num_docs\": num_docs,\n        \"max_connections\": [64],\n        \"max_workers\": [64],\n        \"max_queue_size\": [2500],\n    },\n}\n\nfeed_params = []\n# Create one FeedParams instance of each permutation\nfor func, parameters in params_by_function.items():\n    print(f\"Function: {func}\")\n    keys, values = zip(*parameters.items())\n    for combination in product(*values):\n        settings = dict(zip(keys, combination))\n        print(settings)\n        feed_params.append(\n            FeedParams(\n                name=f\"{settings['num_docs']}_{settings['max_connections']}_{settings.get('max_workers', 0)}_{func}\",\n                function_name=func,\n                **settings,\n            )\n        )\n    print(\"\\n\")  # Just to add space between different functions\n</pre> from itertools import product  # We will only run for up to 10 000 documents here as notebook is run as part of CI.  num_docs = [     1000,     5_000,     10_000, ] params_by_function = {     \"feed_async_iterable\": {         \"num_docs\": num_docs,         \"max_connections\": [1],         \"max_workers\": [64],         \"max_queue_size\": [2500],     },     \"feed_iterable\": {         \"num_docs\": num_docs,         \"max_connections\": [64],         \"max_workers\": [64],         \"max_queue_size\": [2500],     }, }  feed_params = [] # Create one FeedParams instance of each permutation for func, parameters in params_by_function.items():     print(f\"Function: {func}\")     keys, values = zip(*parameters.items())     for combination in product(*values):         settings = dict(zip(keys, combination))         print(settings)         feed_params.append(             FeedParams(                 name=f\"{settings['num_docs']}_{settings['max_connections']}_{settings.get('max_workers', 0)}_{func}\",                 function_name=func,                 **settings,             )         )     print(\"\\n\")  # Just to add space between different functions <pre>Function: feed_async_iterable\n{'num_docs': 1000, 'max_connections': 1, 'max_workers': 64, 'max_queue_size': 2500}\n{'num_docs': 5000, 'max_connections': 1, 'max_workers': 64, 'max_queue_size': 2500}\n{'num_docs': 10000, 'max_connections': 1, 'max_workers': 64, 'max_queue_size': 2500}\n\n\nFunction: feed_iterable\n{'num_docs': 1000, 'max_connections': 64, 'max_workers': 64, 'max_queue_size': 2500}\n{'num_docs': 5000, 'max_connections': 64, 'max_workers': 64, 'max_queue_size': 2500}\n{'num_docs': 10000, 'max_connections': 64, 'max_workers': 64, 'max_queue_size': 2500}\n\n\n</pre> In\u00a0[13]: Copied! <pre>print(f\"Total number of feed_params: {len(feed_params)}\")\n</pre> print(f\"Total number of feed_params: {len(feed_params)}\") <pre>Total number of feed_params: 6\n</pre> <p>Now, we will need a way to retrieve the callable function from the function name.</p> In\u00a0[14]: Copied! <pre># Get reference to function from string name\ndef get_func_from_str(func_name: str) -&gt; Callable:\n    return globals()[func_name]\n</pre> # Get reference to function from string name def get_func_from_str(func_name: str) -&gt; Callable:     return globals()[func_name] In\u00a0[15]: Copied! <pre>from typing import Iterable, Dict\nfrom vespa.application import Vespa\n\n\ndef delete_data(app: Vespa, data: Iterable[Dict]):\n    app.feed_iterable(\n        iter=data,\n        schema=\"doc\",\n        namespace=\"pyvespa-feed\",\n        operation_type=\"delete\",\n        callback=callback,\n        max_workers=16,\n        max_connections=16,\n    )\n</pre> from typing import Iterable, Dict from vespa.application import Vespa   def delete_data(app: Vespa, data: Iterable[Dict]):     app.feed_iterable(         iter=data,         schema=\"doc\",         namespace=\"pyvespa-feed\",         operation_type=\"delete\",         callback=callback,         max_workers=16,         max_connections=16,     ) <p>The line below is used to make the code run in Jupyter, as it is already running an event loop</p> In\u00a0[16]: Copied! <pre>import nest_asyncio\n\nnest_asyncio.apply()\n</pre> import nest_asyncio  nest_asyncio.apply() In\u00a0[17]: Copied! <pre>results = []\nfor params in feed_params:\n    print(\"-\" * 50)\n    print(\"Starting feed with params:\")\n    print(params)\n    data = get_dataset(params.num_docs)\n    if \"xxx\" not in params.function_name:\n        if \"feed_sync\" in params.function_name:\n            print(\"Skipping feed_sync\")\n            continue\n        feed_result = get_func_from_str(params.function_name)(\n            app=app, params=params, data=data\n        )\n    else:\n        feed_result = asyncio.run(\n            get_func_from_str(params.function_name)(app=app, params=params, data=data)\n        )\n    print(feed_result.feed_time)\n    results.append(feed_result)\n    print(\"Deleting data\")\n    time.sleep(3)\n    delete_data(app, data)\n</pre> results = [] for params in feed_params:     print(\"-\" * 50)     print(\"Starting feed with params:\")     print(params)     data = get_dataset(params.num_docs)     if \"xxx\" not in params.function_name:         if \"feed_sync\" in params.function_name:             print(\"Skipping feed_sync\")             continue         feed_result = get_func_from_str(params.function_name)(             app=app, params=params, data=data         )     else:         feed_result = asyncio.run(             get_func_from_str(params.function_name)(app=app, params=params, data=data)         )     print(feed_result.feed_time)     results.append(feed_result)     print(\"Deleting data\")     time.sleep(3)     delete_data(app, data) <pre>--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_1_64_feed_async_iterable', num_docs=1000, max_connections=1, function_name='feed_async_iterable', max_workers=64, max_queue_size=2500)\n</pre> <pre>Using mtls_key_cert Authentication against endpoint https://b48e8812.bc737822.z.vespa-app.cloud//ApplicationStatus\n7.062151908874512\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='5000_1_64_feed_async_iterable', num_docs=5000, max_connections=1, function_name='feed_async_iterable', max_workers=64, max_queue_size=2500)\n20.979923963546753\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='10000_1_64_feed_async_iterable', num_docs=10000, max_connections=1, function_name='feed_async_iterable', max_workers=64, max_queue_size=2500)\n41.321199893951416\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_64_feed_iterable', num_docs=1000, max_connections=64, function_name='feed_iterable', max_workers=64, max_queue_size=2500)\n16.278107166290283\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='5000_64_64_feed_iterable', num_docs=5000, max_connections=64, function_name='feed_iterable', max_workers=64, max_queue_size=2500)\n78.27990508079529\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='10000_64_64_feed_iterable', num_docs=10000, max_connections=64, function_name='feed_iterable', max_workers=64, max_queue_size=2500)\n156.38266611099243\nDeleting data\n</pre> In\u00a0[18]: Copied! <pre># Create a pandas DataFrame with the results\nimport pandas as pd\n\ndf = pd.DataFrame([result.__dict__ for result in results])\ndf[\"requests_per_second\"] = df[\"num_docs\"] / df[\"feed_time\"]\ndf\n</pre> # Create a pandas DataFrame with the results import pandas as pd  df = pd.DataFrame([result.__dict__ for result in results]) df[\"requests_per_second\"] = df[\"num_docs\"] / df[\"feed_time\"] df Out[18]: name num_docs max_connections function_name max_workers max_queue_size feed_time requests_per_second 0 1000_1_64_feed_async_iterable 1000 1 feed_async_iterable 64 2500 7.062152 141.599899 1 5000_1_64_feed_async_iterable 5000 1 feed_async_iterable 64 2500 20.979924 238.323075 2 10000_1_64_feed_async_iterable 10000 1 feed_async_iterable 64 2500 41.321200 242.006525 3 1000_64_64_feed_iterable 1000 64 feed_iterable 64 2500 16.278107 61.432204 4 5000_64_64_feed_iterable 5000 64 feed_iterable 64 2500 78.279905 63.873353 5 10000_64_64_feed_iterable 10000 64 feed_iterable 64 2500 156.382666 63.945706 In\u00a0[19]: Copied! <pre>import plotly.express as px\n\n\ndef plot_performance(df: pd.DataFrame):\n    # Create a scatter plot with logarithmic scale for both axes using Plotly Express\n    fig = px.scatter(\n        df,\n        x=\"num_docs\",\n        y=\"requests_per_second\",\n        color=\"function_name\",  # Defines color based on different functions\n        log_x=True,  # Set x-axis to logarithmic scale\n        log_y=False,  # If you also want the y-axis in logarithmic scale, set this to True\n        title=\"Performance: Requests per Second vs. Number of Documents\",\n        labels={  # Customizing axis labels\n            \"num_docs\": \"Number of Documents\",\n            \"requests_per_second\": \"Requests per Second\",\n            \"max_workers\": \"max_workers\",\n            \"max_queue_size\": \"max_queue_size\",\n        },\n        template=\"plotly_white\",  # This sets the style to a white background, adhering to Tufte's minimalist principles\n        hover_data=[\n            \"max_workers\",\n            \"max_queue_size\",\n            \"max_connections\",\n        ],  # Additional information to show on hover\n    )\n\n    # Update layout for better readability, similar to 'talk' context in Seaborn\n    fig.update_layout(\n        font=dict(\n            size=16,  # Adjusting font size for better visibility, similar to 'talk' context\n        ),\n        legend_title_text=\"Function Details\",  # Custom legend title\n        legend=dict(\n            title_font_size=16,\n            x=800,  # Adjusting legend position similar to bbox_to_anchor in Matplotlib\n            xanchor=\"auto\",\n            y=1,\n            yanchor=\"auto\",\n        ),\n        width=800,  # Adjusting width of the plot\n    )\n    fig.update_xaxes(\n        tickvals=[1000, 5000, 10000],  # Set specific tick values\n        ticktext=[\"1k\", \"5k\", \"10k\"],  # Set corresponding tick labels\n    )\n\n    fig.update_traces(\n        marker=dict(size=12, opacity=0.7)\n    )  # Adjust marker size and opacity\n    # Show plot\n    fig.show()\n    # Save plot as HTML file\n    fig.write_html(\"performance.html\")\n\n\nplot_performance(df)\n</pre> import plotly.express as px   def plot_performance(df: pd.DataFrame):     # Create a scatter plot with logarithmic scale for both axes using Plotly Express     fig = px.scatter(         df,         x=\"num_docs\",         y=\"requests_per_second\",         color=\"function_name\",  # Defines color based on different functions         log_x=True,  # Set x-axis to logarithmic scale         log_y=False,  # If you also want the y-axis in logarithmic scale, set this to True         title=\"Performance: Requests per Second vs. Number of Documents\",         labels={  # Customizing axis labels             \"num_docs\": \"Number of Documents\",             \"requests_per_second\": \"Requests per Second\",             \"max_workers\": \"max_workers\",             \"max_queue_size\": \"max_queue_size\",         },         template=\"plotly_white\",  # This sets the style to a white background, adhering to Tufte's minimalist principles         hover_data=[             \"max_workers\",             \"max_queue_size\",             \"max_connections\",         ],  # Additional information to show on hover     )      # Update layout for better readability, similar to 'talk' context in Seaborn     fig.update_layout(         font=dict(             size=16,  # Adjusting font size for better visibility, similar to 'talk' context         ),         legend_title_text=\"Function Details\",  # Custom legend title         legend=dict(             title_font_size=16,             x=800,  # Adjusting legend position similar to bbox_to_anchor in Matplotlib             xanchor=\"auto\",             y=1,             yanchor=\"auto\",         ),         width=800,  # Adjusting width of the plot     )     fig.update_xaxes(         tickvals=[1000, 5000, 10000],  # Set specific tick values         ticktext=[\"1k\", \"5k\", \"10k\"],  # Set corresponding tick labels     )      fig.update_traces(         marker=dict(size=12, opacity=0.7)     )  # Adjust marker size and opacity     # Show plot     fig.show()     # Save plot as HTML file     fig.write_html(\"performance.html\")   plot_performance(df) <p>Interesting. Let's try to summarize the insights we got from this experiment:</p> <ul> <li>The <code>feed_async_iterable</code> method is approximately 3x faster than the <code>feed_iterable</code> method for this specific setup.</li> <li>Note that this will vary depending on the network latency between the client and the Vespa instance.</li> <li>If you are feeding from a cloud instance with less latency to the Vespa instance, the difference between the methods will be less, and the <code>feed_iterable</code> method might even be faster.</li> </ul> <ul> <li>Still prefer to use the Vespa CLI if you really care about performance. \ud83d\ude80</li> <li>If you want to use pyvespa, prefer the <code>feed_async_iterable</code>- method, if you are I/O-bound.</li> </ul> In\u00a0[26]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete() <pre>Deactivated vespa-team.feedperformancecloud in dev.aws-us-east-1c\nDeleted instance vespa-team.feedperformancecloud.default\n</pre>"},{"location":"examples/feed_performance_cloud.html#feeding-to-vespa-cloud","title":"Feeding to Vespa Cloud\u00b6","text":"<p>Our previous notebook, we demonstrated one way of benchmarking feed performance to a local Vespa instance running in Docker. In this notebook, we will look at the same methods but how feeding to Vespa Cloud affects the performance of the different methods.</p> <p>The key difference between feeding to a local Vespa instance and a Vespa Cloud instance is the network latency. Additionally, we will introduce embedding in Vespa at feed time, which is a realistic scenario for many use cases.</p> <p>We will look at these 3 different methods:</p> <ol> <li>Using <code>feed_iterable()</code> - which uses threading to parallelize the feed operation. Best for CPU-bound operations.</li> <li>Using <code>feed_async_iterable()</code> - which uses asyncio to parallelize the feed operation. Also uses <code>httpx</code> with HTTP/2-support. Performs best for IO-bound operations.</li> <li>Using Vespa CLI.</li> </ol>"},{"location":"examples/feed_performance_cloud.html#create-an-application-package","title":"Create an application package\u00b6","text":"<p>The application package has all the Vespa configuration files.</p> <p>For this demo, we will use a simple application package</p>"},{"location":"examples/feed_performance_cloud.html#deploy-the-vespa-application","title":"Deploy the Vespa application\u00b6","text":"<p>Deploy <code>package</code> on the local machine using Docker, without leaving the notebook, by creating an instance of VespaDocker. <code>VespaDocker</code> connects to the local Docker daemon socket and starts the Vespa docker image.</p> <p>If this step fails, please check that the Docker daemon is running, and that the Docker daemon socket can be used by clients (Configurable under advanced settings in Docker Desktop).</p>"},{"location":"examples/feed_performance_cloud.html#preparing-the-data","title":"Preparing the data\u00b6","text":"<p>In this example we use HF Datasets library to stream the \"Cohere/wikipedia-2023-11-embed-multilingual-v3\" dataset and index in our newly deployed Vespa instance.</p> <p>The dataset contains Wikipedia-pages, and their corresponding embeddings.</p> <p>For this exploration, we will use the <code>id</code>, <code>text</code> and <code>embedding</code>-fields</p> <p>The following uses the stream option of datasets to stream the data without downloading all the contents locally.</p> <p>The <code>map</code> functionality allows us to convert the dataset fields into the expected feed format for <code>pyvespa</code> which expects a dict with the keys <code>id</code> and <code>fields</code>:</p> <p><code>{\u00a0\"id\": \"vespa-document-id\", \"fields\": {\"vespa_field\": \"vespa-field-value\"}}</code></p>"},{"location":"examples/feed_performance_cloud.html#utility-function-to-create-a-dataset-with-different-number-of-documents","title":"Utility function to create a dataset with different number of documents\u00b6","text":""},{"location":"examples/feed_performance_cloud.html#a-dataclass-to-store-the-parameters-and-results-of-the-different-feeding-methods","title":"A dataclass to store the parameters and results of the different feeding methods\u00b6","text":""},{"location":"examples/feed_performance_cloud.html#a-common-callback-function-to-notify-if-something-goes-wrong","title":"A common callback function to notify if something goes wrong\u00b6","text":""},{"location":"examples/feed_performance_cloud.html#defining-our-feeding-functions","title":"Defining our feeding functions\u00b6","text":""},{"location":"examples/feed_performance_cloud.html#defining-our-hyperparameters","title":"Defining our hyperparameters\u00b6","text":""},{"location":"examples/feed_performance_cloud.html#function-to-clean-up-after-each-feed","title":"Function to clean up after each feed\u00b6","text":"<p>For a fair comparison, we will delete the data before feeding it again.</p>"},{"location":"examples/feed_performance_cloud.html#main-experiment-loop","title":"Main experiment loop\u00b6","text":""},{"location":"examples/feed_performance_cloud.html#plotting-the-results","title":"Plotting the results\u00b6","text":"<p>Let's plot the results to see how the different methods compare.</p>"},{"location":"examples/feed_performance_cloud.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"examples/feed_performance_cloud.html#next-steps","title":"Next steps\u00b6","text":"<p>Check out some of the other examples in the documentation.</p>"},{"location":"examples/lightgbm-with-categorical-mapping.html","title":"Lightgbm with categorical mapping","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Install and load required packages.</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install numpy pandas pyvespa lightgbm\n</pre> !pip3 install numpy pandas pyvespa lightgbm In\u00a0[3]: Copied! <pre>import json\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n</pre> import json import lightgbm as lgb import numpy as np import pandas as pd <p>Simulate data that will be used to train the LightGBM model. Note that Vespa does not automatically recognize the feature names <code>feature_1</code>, <code>feature_2</code> and <code>feature_3</code>. When creating the application package we need to map those variables to something that the Vespa application recognizes, such as a document attribute or query value.</p> In\u00a0[4]: Copied! <pre># Create random training set\nfeatures = pd.DataFrame(\n    {\n        \"feature_1\": np.random.random(100),\n        \"feature_2\": np.random.random(100),\n        \"feature_3\": pd.Series(\n            np.random.choice([\"a\", \"b\", \"c\"], size=100), dtype=\"category\"\n        ),\n    }\n)\nfeatures.head()\n</pre> # Create random training set features = pd.DataFrame(     {         \"feature_1\": np.random.random(100),         \"feature_2\": np.random.random(100),         \"feature_3\": pd.Series(             np.random.choice([\"a\", \"b\", \"c\"], size=100), dtype=\"category\"         ),     } ) features.head() Out[4]: feature_1 feature_2 feature_3 0 0.856415 0.550705 a 1 0.615107 0.509030 a 2 0.089759 0.667729 c 3 0.161664 0.361693 b 4 0.841505 0.967227 b <p>Create a target variable that depends on <code>feature_1</code>, <code>feature_2</code> and <code>feature_3</code>:</p> In\u00a0[5]: Copied! <pre>numeric_features = pd.get_dummies(features)\ntargets = (\n    (\n        numeric_features[\"feature_1\"]\n        + numeric_features[\"feature_2\"]\n        - 0.5 * numeric_features[\"feature_3_a\"]\n        + 0.5 * numeric_features[\"feature_3_c\"]\n    )\n    &gt; 1.0\n) * 1.0\ntargets\n</pre> numeric_features = pd.get_dummies(features) targets = (     (         numeric_features[\"feature_1\"]         + numeric_features[\"feature_2\"]         - 0.5 * numeric_features[\"feature_3_a\"]         + 0.5 * numeric_features[\"feature_3_c\"]     )     &gt; 1.0 ) * 1.0 targets Out[5]: <pre>0     0.0\n1     0.0\n2     1.0\n3     0.0\n4     1.0\n     ... \n95    1.0\n96    1.0\n97    0.0\n98    1.0\n99    1.0\nLength: 100, dtype: float64</pre> <p>Train the LightGBM model on the simulated data,</p> In\u00a0[6]: Copied! <pre>training_set = lgb.Dataset(features, targets)\n\n# Train the model\nparams = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"num_leaves\": 3,\n}\nmodel = lgb.train(params, training_set, num_boost_round=5)\n</pre> training_set = lgb.Dataset(features, targets)  # Train the model params = {     \"objective\": \"binary\",     \"metric\": \"binary_logloss\",     \"num_leaves\": 3, } model = lgb.train(params, training_set, num_boost_round=5) <pre>[LightGBM] [Info] Number of positive: 48, number of negative: 52\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000404 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 74\n[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 3\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.480000 -&gt; initscore=-0.080043\n[LightGBM] [Info] Start training from score -0.080043\n</pre> <p>Create the application package and map the LightGBM feature names to the related Vespa names.</p> <p>In this example we are going to assume that <code>feature_1</code> represents the document field <code>numeric</code> and map <code>feature_1</code> to <code>attribute(numeric)</code> through the use of a Vespa <code>Function</code> in the corresponding <code>RankProfile</code>. <code>feature_2</code> maps to a <code>value</code> that will be sent along with the query, and this is represented in Vespa by mapping <code>query(value)</code> to <code>feature_2</code>. Lastly, the categorical feature is mapped from <code>attribute(categorical)</code> to <code>feature_3</code>.</p> In\u00a0[7]: Copied! <pre>from vespa.package import ApplicationPackage, Field, RankProfile, Function\n\napp_package = ApplicationPackage(name=\"lightgbm\")\napp_package.schema.add_fields(\n    Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n    Field(name=\"numeric\", type=\"double\", indexing=[\"summary\", \"attribute\"]),\n    Field(name=\"categorical\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n)\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"classify\",\n        functions=[\n            Function(name=\"feature_1\", expression=\"attribute(numeric)\"),\n            Function(name=\"feature_2\", expression=\"query(value)\"),\n            Function(name=\"feature_3\", expression=\"attribute(categorical)\"),\n        ],\n        first_phase=\"lightgbm('lightgbm_model.json')\",\n    )\n)\n</pre> from vespa.package import ApplicationPackage, Field, RankProfile, Function  app_package = ApplicationPackage(name=\"lightgbm\") app_package.schema.add_fields(     Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),     Field(name=\"numeric\", type=\"double\", indexing=[\"summary\", \"attribute\"]),     Field(name=\"categorical\", type=\"string\", indexing=[\"summary\", \"attribute\"]), ) app_package.schema.add_rank_profile(     RankProfile(         name=\"classify\",         functions=[             Function(name=\"feature_1\", expression=\"attribute(numeric)\"),             Function(name=\"feature_2\", expression=\"query(value)\"),             Function(name=\"feature_3\", expression=\"attribute(categorical)\"),         ],         first_phase=\"lightgbm('lightgbm_model.json')\",     ) ) <p>We can check how the Vespa search defition file will look like. Note that <code>feature_1</code>, <code>feature_2</code> and <code>feature_3</code> required by the LightGBM model are now defined on the schema definition:</p> In\u00a0[8]: Copied! <pre>print(app_package.schema.schema_to_text)\n</pre> print(app_package.schema.schema_to_text) <pre>schema lightgbm {\n    document lightgbm {\n        field id type string {\n            indexing: summary | attribute\n        }\n        field numeric type double {\n            indexing: summary | attribute\n        }\n        field categorical type string {\n            indexing: summary | attribute\n        }\n    }\n    rank-profile classify {\n        function feature_1() {\n            expression {\n                attribute(numeric)\n            }\n        }\n        function feature_2() {\n            expression {\n                query(value)\n            }\n        }\n        function feature_3() {\n            expression {\n                attribute(categorical)\n            }\n        }\n        first-phase {\n            expression {\n                lightgbm('lightgbm_model.json')\n            }\n        }\n    }\n}\n</pre> <p>We can export the application package files to disk:</p> In\u00a0[9]: Copied! <pre>from pathlib import Path\n\nPath(\"lightgbm\").mkdir(parents=True, exist_ok=True)\napp_package.to_files(\"lightgbm\")\n</pre> from pathlib import Path  Path(\"lightgbm\").mkdir(parents=True, exist_ok=True) app_package.to_files(\"lightgbm\") <p>Note that we don't have any models under the <code>models</code> folder. We need to export the lightGBM model that we trained earlier to <code>models/lightgbm.json</code>.</p> In\u00a0[13]: Copied! <pre>!tree lightgbm\n</pre> !tree lightgbm <pre>lightgbm\n\u251c\u2500\u2500 files\n\u251c\u2500\u2500 models\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgbm_model.json\n\u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgbm.sd\n\u251c\u2500\u2500 search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 query-profiles\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 default.xml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 types\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 root.xml\n\u2514\u2500\u2500 services.xml\n\n7 directories, 5 files\n</pre> In\u00a0[12]: Copied! <pre>with open(\"lightgbm/models/lightgbm_model.json\", \"w\") as f:\n    json.dump(model.dump_model(), f, indent=2)\n</pre> with open(\"lightgbm/models/lightgbm_model.json\", \"w\") as f:     json.dump(model.dump_model(), f, indent=2) <p>Now we can see that the model is where Vespa expects it to be:</p> In\u00a0[14]: Copied! <pre>!tree lightgbm\n</pre> !tree lightgbm <pre>lightgbm\n\u251c\u2500\u2500 files\n\u251c\u2500\u2500 models\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgbm_model.json\n\u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgbm.sd\n\u251c\u2500\u2500 search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 query-profiles\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 default.xml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 types\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 root.xml\n\u2514\u2500\u2500 services.xml\n\n7 directories, 5 files\n</pre> <p>Deploy the application package from disk with Docker:</p> In\u00a0[15]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy_from_disk(\n    application_name=\"lightgbm\", application_root=\"lightgbm\"\n)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy_from_disk(     application_name=\"lightgbm\", application_root=\"lightgbm\" ) <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 15/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 20/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 25/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p>Feed the simulated data. To feed data in batch we need to create a list of dictionaries containing id and fields keys:</p> In\u00a0[16]: Copied! <pre>feed_batch = [\n    {\n        \"id\": idx,\n        \"fields\": {\n            \"id\": idx,\n            \"numeric\": row[\"feature_1\"],\n            \"categorical\": row[\"feature_3\"],\n        },\n    }\n    for idx, row in features.iterrows()\n]\n</pre> feed_batch = [     {         \"id\": idx,         \"fields\": {             \"id\": idx,             \"numeric\": row[\"feature_1\"],             \"categorical\": row[\"feature_3\"],         },     }     for idx, row in features.iterrows() ] In\u00a0[17]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Document {id} was not fed to Vespa due to error: {response.get_json()}\")\n\n\napp.feed_iterable(feed_batch, callback=callback)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Document {id} was not fed to Vespa due to error: {response.get_json()}\")   app.feed_iterable(feed_batch, callback=callback) <p>Predict with the trained LightGBM model so that we can later compare with the predictions returned by Vespa.</p> In\u00a0[18]: Copied! <pre>features[\"model_prediction\"] = model.predict(features)\n</pre> features[\"model_prediction\"] = model.predict(features) In\u00a0[19]: Copied! <pre>features\n</pre> features Out[19]: feature_1 feature_2 feature_3 model_prediction 0 0.856415 0.550705 a 0.402572 1 0.615107 0.509030 a 0.356262 2 0.089759 0.667729 c 0.641578 3 0.161664 0.361693 b 0.388184 4 0.841505 0.967227 b 0.632525 ... ... ... ... ... 95 0.087768 0.451850 c 0.641578 96 0.839063 0.644387 b 0.632525 97 0.725573 0.327668 a 0.376350 98 0.937481 0.199995 b 0.376350 99 0.918530 0.734004 a 0.402572 <p>100 rows \u00d7 4 columns</p> <p>Create a <code>compute_vespa_relevance</code> function that takes a document <code>id</code> and a query <code>value</code> and return the LightGBM model deployed.</p> In\u00a0[20]: Copied! <pre>def compute_vespa_relevance(id_value: int):\n    hits = app.query(\n        body={\n            \"yql\": \"select * from sources * where id = {}\".format(str(id_value)),\n            \"ranking\": \"classify\",\n            \"ranking.features.query(value)\": features.loc[id_value, \"feature_2\"],\n            \"hits\": 1,\n        }\n    ).hits\n    return hits[0][\"relevance\"]\n\n\ncompute_vespa_relevance(id_value=0)\n</pre> def compute_vespa_relevance(id_value: int):     hits = app.query(         body={             \"yql\": \"select * from sources * where id = {}\".format(str(id_value)),             \"ranking\": \"classify\",             \"ranking.features.query(value)\": features.loc[id_value, \"feature_2\"],             \"hits\": 1,         }     ).hits     return hits[0][\"relevance\"]   compute_vespa_relevance(id_value=0) Out[20]: <pre>0.4025720849980601</pre> <p>Loop through the <code>features</code> to compute a vespa prediction for all the data points, so that we can compare it to the predictions made by the model outside Vespa.</p> In\u00a0[21]: Copied! <pre>vespa_relevance = []\nfor idx, row in features.iterrows():\n    vespa_relevance.append(compute_vespa_relevance(id_value=idx))\nfeatures[\"vespa_relevance\"] = vespa_relevance\n</pre> vespa_relevance = [] for idx, row in features.iterrows():     vespa_relevance.append(compute_vespa_relevance(id_value=idx)) features[\"vespa_relevance\"] = vespa_relevance In\u00a0[22]: Copied! <pre>features\n</pre> features Out[22]: feature_1 feature_2 feature_3 model_prediction vespa_relevance 0 0.856415 0.550705 a 0.402572 0.402572 1 0.615107 0.509030 a 0.356262 0.356262 2 0.089759 0.667729 c 0.641578 0.641578 3 0.161664 0.361693 b 0.388184 0.388184 4 0.841505 0.967227 b 0.632525 0.632525 ... ... ... ... ... ... 95 0.087768 0.451850 c 0.641578 0.641578 96 0.839063 0.644387 b 0.632525 0.632525 97 0.725573 0.327668 a 0.376350 0.376350 98 0.937481 0.199995 b 0.376350 0.376350 99 0.918530 0.734004 a 0.402572 0.402572 <p>100 rows \u00d7 5 columns</p> <p>Predictions from the model should be equal to predictions from Vespa, showing the model was correctly deployed to Vespa.</p> In\u00a0[23]: Copied! <pre>assert features[\"model_prediction\"].tolist() == features[\"vespa_relevance\"].tolist()\n</pre> assert features[\"model_prediction\"].tolist() == features[\"vespa_relevance\"].tolist() In\u00a0[24]: Copied! <pre>!rm -fr lightgbm\nvespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> !rm -fr lightgbm vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"examples/lightgbm-with-categorical-mapping.html#lightgbm-mapping-model-features-to-vespa-features","title":"LightGBM: Mapping model features to Vespa features\u00b6","text":"<p>The main goal of this tutorial is to show how to deploy a LightGBM model with feature names that do not match Vespa feature names.</p> <p>The following tasks will be accomplished throughout the tutorial:</p> <ol> <li>Train a LightGBM classification model with generic feature names that will not be available in the Vespa application.</li> <li>Create an application package and include a mapping from Vespa feature names to LightGBM model feature names.</li> <li>Create Vespa application package files and export then to an application folder.</li> <li>Export the trained LightGBM model to the Vespa application folder.</li> <li>Deploy the Vespa application using the application folder.</li> <li>Feed data to the Vespa application.</li> <li>Assert that the LightGBM predictions from the deployed model are correct.</li> </ol>"},{"location":"examples/lightgbm-with-categorical-mapping.html#setup","title":"Setup\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#create-data","title":"Create data\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#fit-lightgbm-model","title":"Fit lightgbm model\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#vespa-application-package","title":"Vespa application package\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#export-the-model","title":"Export the model\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#deploy-the-application","title":"Deploy the application\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#feed-the-data","title":"Feed the data\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#model-predictions","title":"Model predictions\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#query","title":"Query\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#compare-model-and-vespa-predictions","title":"Compare model and Vespa predictions\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#clean-environment","title":"Clean environment\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html","title":"Lightgbm with categorical","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Install and load required packages.</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install numpy pandas pyvespa lightgbm\n</pre> !pip3 install numpy pandas pyvespa lightgbm In\u00a0[3]: Copied! <pre>import json\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n</pre> import json import lightgbm as lgb import numpy as np import pandas as pd <p>Generate a toy dataset to follow along. Note that we set the column names in a format that Vespa understands. <code>query(value)</code> means that the user will send a parameter named <code>value</code> along with the query. <code>attribute(field)</code> means that <code>field</code> is a document attribute defined in a schema. In the example below we have a query parameter named <code>value</code> and two document's attributes, <code>numeric</code> and <code>categorical</code>. If we want <code>lightgbm</code> to handle categorical variables we should use <code>dtype=\"category\"</code> when creating the dataframe, as shown below.</p> In\u00a0[4]: Copied! <pre># Create random training set\nfeatures = pd.DataFrame(\n    {\n        \"query(value)\": np.random.random(100),\n        \"attribute(numeric)\": np.random.random(100),\n        \"attribute(categorical)\": pd.Series(\n            np.random.choice([\"a\", \"b\", \"c\"], size=100), dtype=\"category\"\n        ),\n    }\n)\nfeatures.head()\n</pre> # Create random training set features = pd.DataFrame(     {         \"query(value)\": np.random.random(100),         \"attribute(numeric)\": np.random.random(100),         \"attribute(categorical)\": pd.Series(             np.random.choice([\"a\", \"b\", \"c\"], size=100), dtype=\"category\"         ),     } ) features.head() Out[4]: query(value) attribute(numeric) attribute(categorical) 0 0.437748 0.442222 c 1 0.957135 0.323047 b 2 0.514168 0.426117 a 3 0.713511 0.886630 b 4 0.626918 0.663179 c <p>We generate the target variable as a function of the three features defined above:</p> In\u00a0[5]: Copied! <pre>numeric_features = pd.get_dummies(features)\ntargets = (\n    (\n        numeric_features[\"query(value)\"]\n        + numeric_features[\"attribute(numeric)\"]\n        - 0.5 * numeric_features[\"attribute(categorical)_a\"]\n        + 0.5 * numeric_features[\"attribute(categorical)_c\"]\n    )\n    &gt; 1.0\n) * 1.0\ntargets\n</pre> numeric_features = pd.get_dummies(features) targets = (     (         numeric_features[\"query(value)\"]         + numeric_features[\"attribute(numeric)\"]         - 0.5 * numeric_features[\"attribute(categorical)_a\"]         + 0.5 * numeric_features[\"attribute(categorical)_c\"]     )     &gt; 1.0 ) * 1.0 targets Out[5]: <pre>0     1.0\n1     1.0\n2     0.0\n3     1.0\n4     1.0\n     ... \n95    0.0\n96    1.0\n97    0.0\n98    0.0\n99    1.0\nLength: 100, dtype: float64</pre> <p>Train an LightGBM model with a binary loss function:</p> In\u00a0[6]: Copied! <pre>training_set = lgb.Dataset(features, targets)\n\n# Train the model\nparams = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"num_leaves\": 3,\n}\nmodel = lgb.train(params, training_set, num_boost_round=5)\n</pre> training_set = lgb.Dataset(features, targets)  # Train the model params = {     \"objective\": \"binary\",     \"metric\": \"binary_logloss\",     \"num_leaves\": 3, } model = lgb.train(params, training_set, num_boost_round=5) <pre>[LightGBM] [Info] Number of positive: 48, number of negative: 52\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000484 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 74\n[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 3\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.480000 -&gt; initscore=-0.080043\n[LightGBM] [Info] Start training from score -0.080043\n</pre> <p>Create a Vespa application package. The model expects two document attributes, <code>numeric</code> and <code>categorical</code>. We can use the model in the first-phase ranking by using the <code>lightgbm</code> rank feature.</p> In\u00a0[7]: Copied! <pre>from vespa.package import ApplicationPackage, Field, RankProfile\n\napp_package = ApplicationPackage(name=\"lightgbm\")\napp_package.schema.add_fields(\n    Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n    Field(name=\"numeric\", type=\"double\", indexing=[\"summary\", \"attribute\"]),\n    Field(name=\"categorical\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n)\napp_package.schema.add_rank_profile(\n    RankProfile(name=\"classify\", first_phase=\"lightgbm('lightgbm_model.json')\")\n)\n</pre> from vespa.package import ApplicationPackage, Field, RankProfile  app_package = ApplicationPackage(name=\"lightgbm\") app_package.schema.add_fields(     Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),     Field(name=\"numeric\", type=\"double\", indexing=[\"summary\", \"attribute\"]),     Field(name=\"categorical\", type=\"string\", indexing=[\"summary\", \"attribute\"]), ) app_package.schema.add_rank_profile(     RankProfile(name=\"classify\", first_phase=\"lightgbm('lightgbm_model.json')\") ) <p>We can check how the Vespa search defition file will look like:</p> In\u00a0[8]: Copied! <pre>print(app_package.schema.schema_to_text)\n</pre> print(app_package.schema.schema_to_text) <pre>schema lightgbm {\n    document lightgbm {\n        field id type string {\n            indexing: summary | attribute\n        }\n        field numeric type double {\n            indexing: summary | attribute\n        }\n        field categorical type string {\n            indexing: summary | attribute\n        }\n    }\n    rank-profile classify {\n        first-phase {\n            expression {\n                lightgbm('lightgbm_model.json')\n            }\n        }\n    }\n}\n</pre> <p>We can export the application package files to disk:</p> In\u00a0[9]: Copied! <pre>from pathlib import Path\n\nPath(\"lightgbm\").mkdir(parents=True, exist_ok=True)\napp_package.to_files(\"lightgbm\")\n</pre> from pathlib import Path  Path(\"lightgbm\").mkdir(parents=True, exist_ok=True) app_package.to_files(\"lightgbm\") <p>Note that we don't have any models under the <code>models</code> folder. We need to export the lightGBM model that we trained earlier to <code>models/lightgbm.json</code>.</p> In\u00a0[10]: Copied! <pre>!tree lightgbm\n</pre> !tree lightgbm <pre>lightgbm\n\u251c\u2500\u2500 files\n\u251c\u2500\u2500 models\n\u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgbm.sd\n\u251c\u2500\u2500 search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 query-profiles\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 default.xml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 types\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 root.xml\n\u2514\u2500\u2500 services.xml\n\n7 directories, 4 files\n</pre> In\u00a0[11]: Copied! <pre>with open(\"lightgbm/models/lightgbm_model.json\", \"w\") as f:\n    json.dump(model.dump_model(), f, indent=2)\n</pre> with open(\"lightgbm/models/lightgbm_model.json\", \"w\") as f:     json.dump(model.dump_model(), f, indent=2) <p>Now we can see that the model is where Vespa expects it to be:</p> In\u00a0[12]: Copied! <pre>!tree lightgbm\n</pre> !tree lightgbm <pre>lightgbm\n\u251c\u2500\u2500 files\n\u251c\u2500\u2500 models\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgbm_model.json\n\u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgbm.sd\n\u251c\u2500\u2500 search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 query-profiles\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 default.xml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 types\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 root.xml\n\u2514\u2500\u2500 services.xml\n\n7 directories, 5 files\n</pre> <p>Deploy the application package from disk with Docker:</p> In\u00a0[13]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy_from_disk(\n    application_name=\"lightgbm\", application_root=\"lightgbm\"\n)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy_from_disk(     application_name=\"lightgbm\", application_root=\"lightgbm\" ) <pre>Waiting for configuration server, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p>Feed the simulated data. To feed data in batch we need to create a list of dictionaries containing <code>id</code> and <code>fields</code> keys:</p> In\u00a0[14]: Copied! <pre>feed_batch = [\n    {\n        \"id\": idx,\n        \"fields\": {\n            \"id\": idx,\n            \"numeric\": row[\"attribute(numeric)\"],\n            \"categorical\": row[\"attribute(categorical)\"],\n        },\n    }\n    for idx, row in features.iterrows()\n]\n</pre> feed_batch = [     {         \"id\": idx,         \"fields\": {             \"id\": idx,             \"numeric\": row[\"attribute(numeric)\"],             \"categorical\": row[\"attribute(categorical)\"],         },     }     for idx, row in features.iterrows() ] <p>Feed the batch of data:</p> In\u00a0[15]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Document {id} was not fed to Vespa due to error: {response.get_json()}\")\n\n\napp.feed_iterable(feed_batch, callback=callback)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Document {id} was not fed to Vespa due to error: {response.get_json()}\")   app.feed_iterable(feed_batch, callback=callback) <p>Predict with the trained LightGBM model so that we can later compare with the predictions returned by Vespa.</p> In\u00a0[16]: Copied! <pre>features[\"model_prediction\"] = model.predict(features)\n</pre> features[\"model_prediction\"] = model.predict(features) In\u00a0[17]: Copied! <pre>features\n</pre> features Out[17]: query(value) attribute(numeric) attribute(categorical) model_prediction 0 0.437748 0.442222 c 0.645663 1 0.957135 0.323047 b 0.645663 2 0.514168 0.426117 a 0.354024 3 0.713511 0.886630 b 0.645663 4 0.626918 0.663179 c 0.645663 ... ... ... ... ... 95 0.208583 0.103319 c 0.352136 96 0.882902 0.224213 c 0.645663 97 0.604831 0.675583 a 0.354024 98 0.278674 0.008019 b 0.352136 99 0.417318 0.616241 b 0.645663 <p>100 rows \u00d7 4 columns</p> <p>Create a <code>compute_vespa_relevance</code> function that takes a document <code>id</code> and a query <code>value</code> and return the LightGBM model deployed.</p> In\u00a0[18]: Copied! <pre>def compute_vespa_relevance(id_value: int):\n    hits = app.query(\n        body={\n            \"yql\": \"select * from sources * where id = {}\".format(str(id_value)),\n            \"ranking\": \"classify\",\n            \"ranking.features.query(value)\": features.loc[id_value, \"query(value)\"],\n            \"hits\": 1,\n        }\n    ).hits\n    return hits[0][\"relevance\"]\n\n\ncompute_vespa_relevance(id_value=0)\n</pre> def compute_vespa_relevance(id_value: int):     hits = app.query(         body={             \"yql\": \"select * from sources * where id = {}\".format(str(id_value)),             \"ranking\": \"classify\",             \"ranking.features.query(value)\": features.loc[id_value, \"query(value)\"],             \"hits\": 1,         }     ).hits     return hits[0][\"relevance\"]   compute_vespa_relevance(id_value=0) Out[18]: <pre>0.645662636917761</pre> <p>Loop through the <code>features</code> to compute a vespa prediction for all the data points, so that we can compare it to the predictions made by the model outside Vespa.</p> In\u00a0[19]: Copied! <pre>vespa_relevance = []\nfor idx, row in features.iterrows():\n    vespa_relevance.append(compute_vespa_relevance(id_value=idx))\nfeatures[\"vespa_relevance\"] = vespa_relevance\n</pre> vespa_relevance = [] for idx, row in features.iterrows():     vespa_relevance.append(compute_vespa_relevance(id_value=idx)) features[\"vespa_relevance\"] = vespa_relevance In\u00a0[20]: Copied! <pre>features\n</pre> features Out[20]: query(value) attribute(numeric) attribute(categorical) model_prediction vespa_relevance 0 0.437748 0.442222 c 0.645663 0.645663 1 0.957135 0.323047 b 0.645663 0.645663 2 0.514168 0.426117 a 0.354024 0.354024 3 0.713511 0.886630 b 0.645663 0.645663 4 0.626918 0.663179 c 0.645663 0.645663 ... ... ... ... ... ... 95 0.208583 0.103319 c 0.352136 0.352136 96 0.882902 0.224213 c 0.645663 0.645663 97 0.604831 0.675583 a 0.354024 0.354024 98 0.278674 0.008019 b 0.352136 0.352136 99 0.417318 0.616241 b 0.645663 0.645663 <p>100 rows \u00d7 5 columns</p> <p>Predictions from the model should be equal to predictions from Vespa, showing the model was correctly deployed to Vespa.</p> In\u00a0[21]: Copied! <pre>assert features[\"model_prediction\"].tolist() == features[\"vespa_relevance\"].tolist()\n</pre> assert features[\"model_prediction\"].tolist() == features[\"vespa_relevance\"].tolist() In\u00a0[22]: Copied! <pre>!rm -fr lightgbm\nvespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> !rm -fr lightgbm vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"examples/lightgbm-with-categorical.html#lightgbm-training-the-model-with-vespa-features","title":"LightGBM: Training the model with Vespa features\u00b6","text":"<p>The main goal of this tutorial is to deploy and use a LightGBM model in a Vespa application. The following tasks will be accomplished throughout the tutorial:</p> <ol> <li>Train a LightGBM classification model with variable names supported by Vespa.</li> <li>Create Vespa application package files and export then to an application folder.</li> <li>Export the trained LightGBM model to the Vespa application folder.</li> <li>Deploy the Vespa application using the application folder.</li> <li>Feed data to the Vespa application.</li> <li>Assert that the LightGBM predictions from the deployed model are correct.</li> </ol>"},{"location":"examples/lightgbm-with-categorical.html#setup","title":"Setup\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#create-data","title":"Create data\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#fit-lightgbm-model","title":"Fit lightgbm model\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#vespa-application-package","title":"Vespa application package\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#export-the-model","title":"Export the model\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#deploy-the-application","title":"Deploy the application\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#feed-the-data","title":"Feed the data\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#model-predictions","title":"Model predictions\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#query","title":"Query\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#compare-model-and-vespa-predictions","title":"Compare model and Vespa predictions\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#clean-environment","title":"Clean environment\u00b6","text":""},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html","title":"Mixedbread binary embeddings with sentence transformers cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa sentence-transformers vespacli\n</pre> !pip3 install -U pyvespa sentence-transformers vespacli In\u00a0[1]: Copied! <pre>from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    \"mixedbread-ai/mxbai-embed-large-v1\",\n    prompts={\n        \"retrieval\": \"Represent this sentence for searching relevant passages: \",\n    },\n    default_prompt_name=\"retrieval\",\n)\n</pre> from sentence_transformers import SentenceTransformer  model = SentenceTransformer(     \"mixedbread-ai/mxbai-embed-large-v1\",     prompts={         \"retrieval\": \"Represent this sentence for searching relevant passages: \",     },     default_prompt_name=\"retrieval\", ) <pre>Default prompt name is set to 'retrieval'. This prompt will be applied to all `encode()` calls, except if `encode()` is called with `prompt` or `prompt_name` parameters.\n</pre> In\u00a0[4]: Copied! <pre>documents = [\n    \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",\n    \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\",\n    \"Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.\",\n    \"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\",\n]\n</pre> documents = [     \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",     \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\",     \"Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.\",     \"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\", ] <p>Run embedding inference, notice how we specify <code>precision=\"binary\"</code>.</p> In\u00a0[5]: Copied! <pre>binary_embeddings = model.encode(documents, precision=\"binary\")\n</pre> binary_embeddings = model.encode(documents, precision=\"binary\") In\u00a0[8]: Copied! <pre>print(\n    \"Binary embedding shape {} with type {}\".format(\n        binary_embeddings.shape, binary_embeddings.dtype\n    )\n)\n</pre> print(     \"Binary embedding shape {} with type {}\".format(         binary_embeddings.shape, binary_embeddings.dtype     ) ) <pre>Binary embedding shape (4, 128) with type int8\n</pre> In\u00a0[9]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\nmy_schema = Schema(\n    name=\"doc\",\n    mode=\"index\",\n    document=Document(\n        fields=[\n            Field(\n                name=\"doc_id\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                match=[\"word\"],\n                rank=\"filter\",\n            ),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"binary_vector\",\n                type=\"tensor&lt;int8&gt;(x[128])\",\n                indexing=[\"attribute\", \"index\"],\n                attribute=[\"distance-metric: hamming\"],\n            ),\n        ]\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  my_schema = Schema(     name=\"doc\",     mode=\"index\",     document=Document(         fields=[             Field(                 name=\"doc_id\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 match=[\"word\"],                 rank=\"filter\",             ),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"binary_vector\",                 type=\"tensor(x[128])\",                 indexing=[\"attribute\", \"index\"],                 attribute=[\"distance-metric: hamming\"],             ),         ]     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])], ) <p>We must add the schema to a Vespa application package. This consists of configuration files, schemas, models, and possibly even custom code (plugins).</p> In\u00a0[15]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"mixedbreadai\"\nvespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema])\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"mixedbreadai\" vespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema]) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p><code>unpack_bits</code> unpacks the binary representation into a 1024-dimensional float vector doc.</p> <p>We define two tensor inputs, one compact binary representation that is used for the nearestNeighbor search and one full version that is used in ranking.</p> In\u00a0[16]: Copied! <pre>from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function\n\n\nrerank = RankProfile(\n    name=\"rerank\",\n    inputs=[\n        (\"query(q_binary)\", \"tensor&lt;int8&gt;(x[128])\"),\n        (\"query(q_full)\", \"tensor&lt;float&gt;(x[1024])\"),\n    ],\n    functions=[\n        Function(  # this returns a tensor&lt;float&gt;(x[1024]) with values -1 or 1\n            name=\"unpack_binary_representation\",\n            expression=\"2*unpack_bits(attribute(binary_vector)) -1\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(\n        expression=\"closeness(field, binary_vector)\"  # 1/(1 + hamming_distance). Calculated between the binary query and the binary_vector\n    ),\n    second_phase=SecondPhaseRanking(\n        expression=\"sum( query(q_full)* unpack_binary_representation )\",  # re-rank using the dot product between float query and the unpacked binary representation\n        rerank_count=100,\n    ),\n    match_features=[\"distance(field, binary_vector)\"],\n)\nmy_schema.add_rank_profile(rerank)\n</pre> from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function   rerank = RankProfile(     name=\"rerank\",     inputs=[         (\"query(q_binary)\", \"tensor(x[128])\"),         (\"query(q_full)\", \"tensor(x[1024])\"),     ],     functions=[         Function(  # this returns a tensor(x[1024]) with values -1 or 1             name=\"unpack_binary_representation\",             expression=\"2*unpack_bits(attribute(binary_vector)) -1\",         )     ],     first_phase=FirstPhaseRanking(         expression=\"closeness(field, binary_vector)\"  # 1/(1 + hamming_distance). Calculated between the binary query and the binary_vector     ),     second_phase=SecondPhaseRanking(         expression=\"sum( query(q_full)* unpack_binary_representation )\",  # re-rank using the dot product between float query and the unpacked binary representation         rerank_count=100,     ),     match_features=[\"distance(field, binary_vector)\"], ) my_schema.add_rank_profile(rerank) In\u00a0[22]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[23]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 1 of dev-aws-us-east-1c for samples.mixedbreadai. This may take a few minutes the first time.\nINFO    [22:14:39]  Deploying platform version 8.322.22 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [22:14:39]  Using CA signed certificate version 0\nINFO    [22:14:46]  Using 1 nodes in container cluster 'mixedbreadai_container'\nINFO    [22:15:18]  Session 2205 for tenant 'samples' prepared and activated.\nINFO    [22:15:21]  ######## Details for all nodes ########\nINFO    [22:15:35]  h90193a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:15:35]  --- platform vespa/cloud-tenant-rhel8:8.322.22 &lt;-- :\nINFO    [22:15:35]  --- logserver-container on port 4080 has not started \nINFO    [22:15:35]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:15:35]  h90971b.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:15:35]  --- platform vespa/cloud-tenant-rhel8:8.322.22 &lt;-- :\nINFO    [22:15:35]  --- container-clustercontroller on port 19050 has not started \nINFO    [22:15:35]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:15:35]  h91168a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:15:35]  --- platform vespa/cloud-tenant-rhel8:8.322.22 &lt;-- :\nINFO    [22:15:35]  --- storagenode on port 19102 has not started \nINFO    [22:15:35]  --- searchnode on port 19107 has not started \nINFO    [22:15:35]  --- distributor on port 19111 has not started \nINFO    [22:15:35]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:15:35]  h91567a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:15:35]  --- platform vespa/cloud-tenant-rhel8:8.322.22 &lt;-- :\nINFO    [22:15:35]  --- container on port 4080 has not started \nINFO    [22:15:35]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:16:41]  Waiting for convergence of 10 services across 4 nodes\nINFO    [22:16:41]  1/1 nodes upgrading platform\nINFO    [22:16:41]  2 application services still deploying\nDEBUG   [22:16:41]  h91567a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nDEBUG   [22:16:41]  --- platform vespa/cloud-tenant-rhel8:8.322.22 &lt;-- :\nDEBUG   [22:16:41]  --- container on port 4080 has not started \nDEBUG   [22:16:41]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:17:11]  Found endpoints:\nINFO    [22:17:11]  - dev.aws-us-east-1c\nINFO    [22:17:11]   |-- https://cf949f23.b8a7f611.z.vespa-app.cloud/ (cluster 'mixedbreadai_container')\nINFO    [22:17:12]  Installation succeeded!\nUsing mTLS (key,cert) Authentication against endpoint https://cf949f23.b8a7f611.z.vespa-app.cloud//ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[24]: Copied! <pre>from vespa.io import VespaResponse\n\nfor i, doc in enumerate(documents):\n    response: VespaResponse = app.feed_data_point(\n        schema=\"doc\",\n        data_id=str(i),\n        fields={\n            \"doc_id\": str(i),\n            \"text\": doc,\n            \"binary_vector\": binary_embeddings[i].tolist(),\n        },\n    )\n    assert response.is_successful()\n</pre> from vespa.io import VespaResponse  for i, doc in enumerate(documents):     response: VespaResponse = app.feed_data_point(         schema=\"doc\",         data_id=str(i),         fields={             \"doc_id\": str(i),             \"text\": doc,             \"binary_vector\": binary_embeddings[i].tolist(),         },     )     assert response.is_successful() In\u00a0[54]: Copied! <pre>query = \"Who was Isac Newton?\"\n# This returns the float version\nquery_embedding_float = model.encode([query])\n</pre> query = \"Who was Isac Newton?\" # This returns the float version query_embedding_float = model.encode([query]) In\u00a0[\u00a0]: Copied! <pre>from sentence_transformers.quantization import quantize_embeddings\n\nquery_embedding_binary = quantize_embeddings(query_embedding_float, precision=\"binary\")\n</pre> from sentence_transformers.quantization import quantize_embeddings  query_embedding_binary = quantize_embeddings(query_embedding_float, precision=\"binary\") <p>Now, we use nearestNeighbor search to retrieve 100 hits (<code>targetHits</code>) using the configured distance-metric (hamming distance). The retrieved hits are exposed to the \u2039espa ranking framework, where we re-rank using the dot product between the float tensor and the unpacked binary vector.</p> In\u00a0[55]: Copied! <pre>response = app.query(\n    yql=\"select * from doc where {targetHits:100}nearestNeighbor(binary_vector,q_binary)\",\n    ranking=\"rerank\",\n    body={\n        \"input.query(q_binary)\": query_embedding_binary[0].tolist(),\n        \"input.query(q_full)\": query_embedding_float[0].tolist(),\n    },\n)\nassert response.is_successful()\n</pre> response = app.query(     yql=\"select * from doc where {targetHits:100}nearestNeighbor(binary_vector,q_binary)\",     ranking=\"rerank\",     body={         \"input.query(q_binary)\": query_embedding_binary[0].tolist(),         \"input.query(q_full)\": query_embedding_float[0].tolist(),     }, ) assert response.is_successful() In\u00a0[56]: Copied! <pre>import json\n\nprint(json.dumps(response.hits, indent=2))\n</pre> import json  print(json.dumps(response.hits, indent=2)) <pre>[\n  {\n    \"id\": \"id:doc:doc::2\",\n    \"relevance\": 177.8957977294922,\n    \"source\": \"mixedbreadai_content\",\n    \"fields\": {\n      \"matchfeatures\": {\n        \"closeness(field,binary_vector)\": 0.003484320557491289,\n        \"distance(field,binary_vector)\": 286.0\n      },\n      \"sddocname\": \"doc\",\n      \"documentid\": \"id:doc:doc::2\",\n      \"doc_id\": \"2\",\n      \"text\": \"Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.\"\n    }\n  },\n  {\n    \"id\": \"id:doc:doc::1\",\n    \"relevance\": 144.52731323242188,\n    \"source\": \"mixedbreadai_content\",\n    \"fields\": {\n      \"matchfeatures\": {\n        \"closeness(field,binary_vector)\": 0.002890173410404624,\n        \"distance(field,binary_vector)\": 345.0\n      },\n      \"sddocname\": \"doc\",\n      \"documentid\": \"id:doc:doc::1\",\n      \"doc_id\": \"1\",\n      \"text\": \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\"\n    }\n  },\n  {\n    \"id\": \"id:doc:doc::0\",\n    \"relevance\": 138.78799438476562,\n    \"source\": \"mixedbreadai_content\",\n    \"fields\": {\n      \"matchfeatures\": {\n        \"closeness(field,binary_vector)\": 0.00273224043715847,\n        \"distance(field,binary_vector)\": 365.0\n      },\n      \"sddocname\": \"doc\",\n      \"documentid\": \"id:doc:doc::0\",\n      \"doc_id\": \"0\",\n      \"text\": \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\"\n    }\n  },\n  {\n    \"id\": \"id:doc:doc::3\",\n    \"relevance\": 115.2405776977539,\n    \"source\": \"mixedbreadai_content\",\n    \"fields\": {\n      \"matchfeatures\": {\n        \"closeness(field,binary_vector)\": 0.002652519893899204,\n        \"distance(field,binary_vector)\": 376.0\n      },\n      \"sddocname\": \"doc\",\n      \"documentid\": \"id:doc:doc::3\",\n      \"doc_id\": \"3\",\n      \"text\": \"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\"\n    }\n  }\n]\n</pre> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#using-mixedbreadai-embedding-model-with-support-for-binary-vectors","title":"Using Mixedbread.ai embedding model with support for binary vectors\u00b6","text":"<p>Check out the amazing blog post: Binary and Scalar Embedding Quantization for Significantly Faster &amp; Cheaper Retrieval</p> <p>Binarization is significant because:</p> <ul> <li>Binarization reduces the storage footprint from 1024 floats (4096 bytes) per vector to 128 int8 (128 bytes).</li> <li>32x less data to store</li> <li>Faster distance calculations using hamming distance, which Vespa natively supports for bits packed into int8 precision. More on hamming distance in Vespa.</li> </ul> <p>Vespa supports <code>hamming</code> distance with and without hnsw indexing.</p> <p>For those wanting to learn more about binary vectors, we recommend our 2021 blog series on Billion-scale vector search with Vespa and Billion-scale vector search with Vespa - part two.</p> <p>This notebook demonstrates how to use the Mixedbread mixedbread-ai/mxbai-embed-large-v1 model with support for binary vectors with Vespa. The notebook example also includes a re-ranking phase that uses the float query vector version for improved accuracy. The re-ranking step makes the model perform at 96.45% of the full float version, with a 32x decrease in storage footprint.</p> <p></p> <p></p> <p>Install the dependencies:</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#examining-the-embeddings-using-sentence-transformers","title":"Examining the embeddings using sentence-transformers\u00b6","text":"<p>Read the blog post for <code>sentence-transformer</code> usage.</p> <p>sentence-transformer API. Model card: mixedbread-ai/mxbai-embed-large-v1.</p> <p>Load the model using the sentence-transformers library:</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#some-sample-documents","title":"Some sample documents\u00b6","text":"<p>Define a few sample documents that we want to embed</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>First, we define a Vespa schema with the fields we want to store and their type.</p> <p>Notice the <code>binary_vector</code> field that defines an indexed (dense) Vespa tensor with the dimension name <code>x[128]</code>.</p> <p>The indexing statement includes <code>index</code> which means that Vespa will use HNSW indexing for this field.</p> <p>Also notice the configuration of distance-metric where we specify <code>hamming</code>.</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#feed-our-sample-documents-and-their-binary-embedding-representation","title":"Feed our sample documents and their binary embedding representation\u00b6","text":"<p>With few documents, we use the synchronous API. Read more in reads and writes.</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> <li>Practical Nearest Neighbor Search Guide</li> </ul> <p>In this case, we use quantization.quantize_embeddings after first obtaining the float version, this to avoid running the model inference twice.</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#summary","title":"Summary\u00b6","text":"<p>Binary embeddings is an exciting development, as it reduces storage (32) and speed up vector searches as the hamming distance is much more efficient than distance metrics like angular or euclidean.</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#clean-up","title":"Clean up\u00b6","text":"<p>We can now delete the cloud instance:</p>"},{"location":"examples/mother-of-all-embedding-models-cloud.html","title":"Mother of all embedding models cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa FlagEmbedding vespacli\n</pre> !pip3 install -U pyvespa FlagEmbedding vespacli In\u00a0[\u00a0]: Copied! <pre>from FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=False)\n</pre> from FlagEmbedding import BGEM3FlagModel  model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=False) In\u00a0[3]: Copied! <pre>passage = [\n    \"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\"\n]\n</pre> passage = [     \"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\" ] In\u00a0[\u00a0]: Copied! <pre>passage_embeddings = model.encode(\n    passage, return_dense=True, return_sparse=True, return_colbert_vecs=True\n)\n</pre> passage_embeddings = model.encode(     passage, return_dense=True, return_sparse=True, return_colbert_vecs=True ) In\u00a0[5]: Copied! <pre>passage_embeddings.keys()\n</pre> passage_embeddings.keys() Out[5]: <pre>dict_keys(['dense_vecs', 'lexical_weights', 'colbert_vecs'])</pre> In\u00a0[6]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\nm_schema = Schema(\n    name=\"m\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"lexical_rep\",\n                type=\"tensor&lt;bfloat16&gt;(t{})\",\n                indexing=[\"summary\", \"attribute\"],\n            ),\n            Field(\n                name=\"dense_rep\",\n                type=\"tensor&lt;bfloat16&gt;(x[1024])\",\n                indexing=[\"summary\", \"attribute\"],\n                attribute=[\"distance-metric: angular\"],\n            ),\n            Field(\n                name=\"colbert_rep\",\n                type=\"tensor&lt;bfloat16&gt;(t{}, x[1024])\",\n                indexing=[\"summary\", \"attribute\"],\n            ),\n        ],\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  m_schema = Schema(     name=\"m\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"lexical_rep\",                 type=\"tensor(t{})\",                 indexing=[\"summary\", \"attribute\"],             ),             Field(                 name=\"dense_rep\",                 type=\"tensor(x[1024])\",                 indexing=[\"summary\", \"attribute\"],                 attribute=[\"distance-metric: angular\"],             ),             Field(                 name=\"colbert_rep\",                 type=\"tensor(t{}, x[1024])\",                 indexing=[\"summary\", \"attribute\"],             ),         ],     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])], ) <p>The above defines our <code>m</code> schema with the original text and the three different representations</p> In\u00a0[7]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"m\"\nvespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[m_schema])\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"m\" vespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[m_schema]) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p>We define three functions that implement the three different scoring functions for the different representations</p> <ul> <li>dense (dense cosine similarity)</li> <li>sparse (sparse dot product)</li> <li>max_sim (The colbert max sim operation)</li> </ul> <p>Then, we combine these three scoring functions using a linear combination with weights, as suggested by the authors here.</p> In\u00a0[8]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking\n\n\nsemantic = RankProfile(\n    name=\"m3hybrid\",\n    inputs=[\n        (\"query(q_dense)\", \"tensor&lt;bfloat16&gt;(x[1024])\"),\n        (\"query(q_lexical)\", \"tensor&lt;bfloat16&gt;(t{})\"),\n        (\"query(q_colbert)\", \"tensor&lt;bfloat16&gt;(qt{}, x[1024])\"),\n        (\"query(q_len_colbert)\", \"float\"),\n    ],\n    functions=[\n        Function(\n            name=\"dense\",\n            expression=\"cosine_similarity(query(q_dense), attribute(dense_rep),x)\",\n        ),\n        Function(\n            name=\"lexical\", expression=\"sum(query(q_lexical) * attribute(lexical_rep))\"\n        ),\n        Function(\n            name=\"max_sim\",\n            expression=\"sum(reduce(sum(query(q_colbert) * attribute(colbert_rep) , x),max, t),qt)/query(q_len_colbert)\",\n        ),\n    ],\n    first_phase=FirstPhaseRanking(\n        expression=\"0.4*dense + 0.2*lexical +  0.4*max_sim\", rank_score_drop_limit=0.0\n    ),\n    match_features=[\"dense\", \"lexical\", \"max_sim\", \"bm25(text)\"],\n)\nm_schema.add_rank_profile(semantic)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking   semantic = RankProfile(     name=\"m3hybrid\",     inputs=[         (\"query(q_dense)\", \"tensor(x[1024])\"),         (\"query(q_lexical)\", \"tensor(t{})\"),         (\"query(q_colbert)\", \"tensor(qt{}, x[1024])\"),         (\"query(q_len_colbert)\", \"float\"),     ],     functions=[         Function(             name=\"dense\",             expression=\"cosine_similarity(query(q_dense), attribute(dense_rep),x)\",         ),         Function(             name=\"lexical\", expression=\"sum(query(q_lexical) * attribute(lexical_rep))\"         ),         Function(             name=\"max_sim\",             expression=\"sum(reduce(sum(query(q_colbert) * attribute(colbert_rep) , x),max, t),qt)/query(q_len_colbert)\",         ),     ],     first_phase=FirstPhaseRanking(         expression=\"0.4*dense + 0.2*lexical +  0.4*max_sim\", rank_score_drop_limit=0.0     ),     match_features=[\"dense\", \"lexical\", \"max_sim\", \"bm25(text)\"], ) m_schema.add_rank_profile(semantic) <p>The <code>m3hybrid</code> rank-profile above defines the query input embedding type and a similarities function that uses a Vespa tensor compute function that calculates the M3 similarities for dense, lexical, and the max_sim for the colbert representations.</p> <p>The profile only defines a single ranking phase, using a linear combination of multiple features using the suggested weighting.</p> <p>Using match-features, Vespa returns selected features along with the hit in the SERP (result page). We also include BM25. We can view BM25 as the fourth dimension. Especially for long-context retrieval, it can be helpful compared to the neural representations.</p> In\u00a0[13]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[14]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 1 of dev-aws-us-east-1c for samples.m. This may take a few minutes the first time.\nINFO    [22:13:09]  Deploying platform version 8.299.14 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [22:13:10]  Using CA signed certificate version 0\nINFO    [22:13:10]  Using 1 nodes in container cluster 'm_container'\nINFO    [22:13:14]  Session 939 for tenant 'samples' prepared and activated.\nINFO    [22:13:17]  ######## Details for all nodes ########\nINFO    [22:13:31]  h88976d.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:13:31]  --- platform vespa/cloud-tenant-rhel8:8.299.14 &lt;-- :\nINFO    [22:13:31]  --- container-clustercontroller on port 19050 has not started \nINFO    [22:13:31]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:13:31]  h89388b.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:13:31]  --- platform vespa/cloud-tenant-rhel8:8.299.14 &lt;-- :\nINFO    [22:13:31]  --- storagenode on port 19102 has not started \nINFO    [22:13:31]  --- searchnode on port 19107 has not started \nINFO    [22:13:31]  --- distributor on port 19111 has not started \nINFO    [22:13:31]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:13:31]  h90001a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:13:31]  --- platform vespa/cloud-tenant-rhel8:8.299.14 &lt;-- :\nINFO    [22:13:31]  --- logserver-container on port 4080 has not started \nINFO    [22:13:31]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:13:31]  h90550a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:13:31]  --- platform vespa/cloud-tenant-rhel8:8.299.14 &lt;-- :\nINFO    [22:13:31]  --- container on port 4080 has not started \nINFO    [22:13:31]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:14:31]  Found endpoints:\nINFO    [22:14:31]  - dev.aws-us-east-1c\nINFO    [22:14:31]   |-- https://d29bf3e7.f064e220.z.vespa-app.cloud/ (cluster 'm_container')\nINFO    [22:14:32]  Installation succeeded!\nUsing mTLS (key,cert) Authentication against endpoint https://d29bf3e7.f064e220.z.vespa-app.cloud//ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[15]: Copied! <pre>vespa_fields = {\n    \"text\": passage[0],\n    \"lexical_rep\": {\n        key: float(value)\n        for key, value in passage_embeddings[\"lexical_weights\"][0].items()\n    },\n    \"dense_rep\": passage_embeddings[\"dense_vecs\"][0].tolist(),\n    \"colbert_rep\": {\n        index: passage_embeddings[\"colbert_vecs\"][0][index].tolist()\n        for index in range(passage_embeddings[\"colbert_vecs\"][0].shape[0])\n    },\n}\n</pre> vespa_fields = {     \"text\": passage[0],     \"lexical_rep\": {         key: float(value)         for key, value in passage_embeddings[\"lexical_weights\"][0].items()     },     \"dense_rep\": passage_embeddings[\"dense_vecs\"][0].tolist(),     \"colbert_rep\": {         index: passage_embeddings[\"colbert_vecs\"][0][index].tolist()         for index in range(passage_embeddings[\"colbert_vecs\"][0].shape[0])     }, } In\u00a0[17]: Copied! <pre>from vespa.io import VespaResponse\n\nresponse: VespaResponse = app.feed_data_point(\n    schema=\"m\", data_id=0, fields=vespa_fields\n)\nassert response.is_successful()\n</pre> from vespa.io import VespaResponse  response: VespaResponse = app.feed_data_point(     schema=\"m\", data_id=0, fields=vespa_fields ) assert response.is_successful() In\u00a0[\u00a0]: Copied! <pre>query = [\"What is BGE M3?\"]\nquery_embeddings = model.encode(\n    query, return_dense=True, return_sparse=True, return_colbert_vecs=True\n)\n</pre> query = [\"What is BGE M3?\"] query_embeddings = model.encode(     query, return_dense=True, return_sparse=True, return_colbert_vecs=True ) <p>The M3 colbert scoring function needs the query length to normalize the score to the range 0 to 1. This helps when combining the score with the other scoring functions.</p> In\u00a0[19]: Copied! <pre>query_length = query_embeddings[\"colbert_vecs\"][0].shape[0]\n</pre> query_length = query_embeddings[\"colbert_vecs\"][0].shape[0] In\u00a0[20]: Copied! <pre>query_fields = {\n    \"input.query(q_lexical)\": {\n        key: float(value)\n        for key, value in query_embeddings[\"lexical_weights\"][0].items()\n    },\n    \"input.query(q_dense)\": query_embeddings[\"dense_vecs\"][0].tolist(),\n    \"input.query(q_colbert)\": str(\n        {\n            index: query_embeddings[\"colbert_vecs\"][0][index].tolist()\n            for index in range(query_embeddings[\"colbert_vecs\"][0].shape[0])\n        }\n    ),\n    \"input.query(q_len_colbert)\": query_length,\n}\n</pre> query_fields = {     \"input.query(q_lexical)\": {         key: float(value)         for key, value in query_embeddings[\"lexical_weights\"][0].items()     },     \"input.query(q_dense)\": query_embeddings[\"dense_vecs\"][0].tolist(),     \"input.query(q_colbert)\": str(         {             index: query_embeddings[\"colbert_vecs\"][0][index].tolist()             for index in range(query_embeddings[\"colbert_vecs\"][0].shape[0])         }     ),     \"input.query(q_len_colbert)\": query_length, } In\u00a0[21]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select id, text from m where userQuery() or ({targetHits:10}nearestNeighbor(dense_rep,q_dense))\",\n    ranking=\"m3hybrid\",\n    query=query[0],\n    body={**query_fields},\n)\nassert response.is_successful()\nprint(json.dumps(response.hits[0], indent=2))\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select id, text from m where userQuery() or ({targetHits:10}nearestNeighbor(dense_rep,q_dense))\",     ranking=\"m3hybrid\",     query=query[0],     body={**query_fields}, ) assert response.is_successful() print(json.dumps(response.hits[0], indent=2)) <pre>{\n  \"id\": \"index:m_content/0/cfcd2084234135f700f08abf\",\n  \"relevance\": 0.5993361056332731,\n  \"source\": \"m_content\",\n  \"fields\": {\n    \"matchfeatures\": {\n      \"bm25(text)\": 0.8630462173553426,\n      \"dense\": 0.6258970723760484,\n      \"lexical\": 0.1941967010498047,\n      \"max_sim\": 0.7753448411822319\n    },\n    \"text\": \"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\"\n  }\n}\n</pre> <p>Notice the <code>matchfeatures</code> that returns the configured match-features from the rank-profile. We can use these to compare the torch model scoring with the computations specified in Vespa.</p> <p>Now, we can compare the Vespa computed scores with the model torch code and they line up perfectly</p> In\u00a0[22]: Copied! <pre>model.compute_lexical_matching_score(\n    passage_embeddings[\"lexical_weights\"][0], query_embeddings[\"lexical_weights\"][0]\n)\n</pre> model.compute_lexical_matching_score(     passage_embeddings[\"lexical_weights\"][0], query_embeddings[\"lexical_weights\"][0] ) Out[22]: <pre>0.19554455392062664</pre> In\u00a0[23]: Copied! <pre>query_embeddings[\"dense_vecs\"][0] @ passage_embeddings[\"dense_vecs\"][0].T\n</pre> query_embeddings[\"dense_vecs\"][0] @ passage_embeddings[\"dense_vecs\"][0].T Out[23]: <pre>0.6259037</pre> In\u00a0[24]: Copied! <pre>model.colbert_score(\n    query_embeddings[\"colbert_vecs\"][0], passage_embeddings[\"colbert_vecs\"][0]\n)\n</pre> model.colbert_score(     query_embeddings[\"colbert_vecs\"][0], passage_embeddings[\"colbert_vecs\"][0] ) Out[24]: <pre>tensor(0.7797)</pre> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/mother-of-all-embedding-models-cloud.html#bge-m3-the-mother-of-all-embedding-models","title":"BGE-M3 - The Mother of all embedding models\u00b6","text":"<p>BAAI released BGE-M3 on January 30th, a new member of the BGE model series.</p> <p>M3 stands for Multi-linguality (100+ languages), Multi-granularities (input length up to 8192), Multi-Functionality (unification of dense, lexical, multi-vec (colbert) retrieval).</p> <p>This notebook demonstrates how to use the BGE-M3 embeddings and represent all three embedding representations in Vespa! Vespa is the only scalable serving engine that can handle all M3 representations.</p> <p>This code is inspired by the README from the model hub BAAI/bge-m3.</p> <p></p> <p>Let's get started! First, install dependencies:</p>"},{"location":"examples/mother-of-all-embedding-models-cloud.html#explore-the-multiple-representations-of-m3","title":"Explore the multiple representations of M3\u00b6","text":"<p>When encoding text, we can ask for the representations we want</p> <ul> <li>Sparse vectors with weights for the token IDs (from the multilingual tokenization process)</li> <li>Dense (DPR) regular text embeddings</li> <li>Multi-Dense (ColBERT) - contextualized multi-token vectors</li> </ul> <p>Let us dive into it - To use this model on the CPU we set <code>use_fp16</code> to False, for GPU inference, it is recommended to use <code>use_fp16=True</code> for accelerated inference.</p>"},{"location":"examples/mother-of-all-embedding-models-cloud.html#a-demo-passage","title":"A demo passage\u00b6","text":"<p>Let us encode a simple passage</p>"},{"location":"examples/mother-of-all-embedding-models-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type. We use Vespa tensors to represent the three different M3 representations.</p> <ul> <li>We use a mapped tensor denoted by <code>t{}</code> to represent the sparse lexical representation</li> <li>We use an indexed tensor denoted by <code>x[1024]</code> to represent the dense single vector representation of 1024 dimensions</li> <li>For the colbert_rep (multi-vector), we use a mixed tensor that combines a mapped and an indexed dimension. This mixed tensor allows us to represent variable lengths.</li> </ul> <p>We use <code>bfloat16</code> tensor cell type, saving 50% storage compared to <code>float</code>.</p>"},{"location":"examples/mother-of-all-embedding-models-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/mother-of-all-embedding-models-cloud.html#feed-the-m3-representations","title":"Feed the M3 representations\u00b6","text":"<p>We convert the three different representations to Vespa feed format</p>"},{"location":"examples/mother-of-all-embedding-models-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Now, we can also query our data.</p> <p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> </ul>"},{"location":"examples/mother-of-all-embedding-models-cloud.html#that-is-it","title":"That is it!\u00b6","text":"<p>That is how easy it is to represent the brand new M3 FlagEmbedding representations in Vespa! Read more in the M3 technical report.</p> <p>We can go ahead and delete the Vespa cloud instance we deployed by:</p>"},{"location":"examples/multi-vector-indexing.html","title":"Multi vector indexing","text":"Refer to troubleshooting     for any problem when running this guide.  <p>This notebook requires pyvespa &gt;= 0.37.1, ZSTD, and the Vespa CLI.</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install pyvespa\n</pre> !pip3 install pyvespa In\u00a0[1]: Copied! <pre>from vespa.package import (\n    ApplicationPackage,\n    Component,\n    Parameter,\n    Field,\n    HNSW,\n    RankProfile,\n    Function,\n    FirstPhaseRanking,\n    SecondPhaseRanking,\n    FieldSet,\n    DocumentSummary,\n    Summary,\n)\nfrom pathlib import Path\nimport json\n\napp_package = ApplicationPackage(\n    name=\"wiki\",\n    components=[\n        Component(\n            id=\"e5-small-q\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\"transformer-model\", {\"path\": \"model/e5-small-v2-int8.onnx\"}),\n                Parameter(\"tokenizer-model\", {\"path\": \"model/tokenizer.json\"}),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import (     ApplicationPackage,     Component,     Parameter,     Field,     HNSW,     RankProfile,     Function,     FirstPhaseRanking,     SecondPhaseRanking,     FieldSet,     DocumentSummary,     Summary, ) from pathlib import Path import json  app_package = ApplicationPackage(     name=\"wiki\",     components=[         Component(             id=\"e5-small-q\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(\"transformer-model\", {\"path\": \"model/e5-small-v2-int8.onnx\"}),                 Parameter(\"tokenizer-model\", {\"path\": \"model/tokenizer.json\"}),             ],         )     ], ) In\u00a0[2]: Copied! <pre>app_package.schema.add_fields(\n    Field(name=\"id\", type=\"int\", indexing=[\"attribute\", \"summary\"]),\n    Field(\n        name=\"title\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n    ),\n    Field(\n        name=\"url\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n    ),\n    Field(\n        name=\"paragraphs\",\n        type=\"array&lt;string&gt;\",\n        indexing=[\"index\", \"summary\"],\n        index=\"enable-bm25\",\n        bolding=True,\n    ),\n    Field(\n        name=\"paragraph_embeddings\",\n        type=\"tensor&lt;float&gt;(p{},x[384])\",\n        indexing=[\"input paragraphs\", \"embed\", \"index\", \"attribute\"],\n        ann=HNSW(distance_metric=\"angular\"),\n        is_document_field=False,\n    ),\n    #\n    # Alteratively, for exact distance calculation not using HNSW:\n    #\n    # Field(name=\"paragraph_embeddings\", type=\"tensor&lt;float&gt;(p{},x[384])\",\n    #       indexing=[\"input paragraphs\", \"embed\", \"attribute\"],\n    #       attribute=[\"distance-metric: angular\"],\n    #       is_document_field=False)\n)\n</pre> app_package.schema.add_fields(     Field(name=\"id\", type=\"int\", indexing=[\"attribute\", \"summary\"]),     Field(         name=\"title\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"     ),     Field(         name=\"url\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"     ),     Field(         name=\"paragraphs\",         type=\"array\",         indexing=[\"index\", \"summary\"],         index=\"enable-bm25\",         bolding=True,     ),     Field(         name=\"paragraph_embeddings\",         type=\"tensor(p{},x[384])\",         indexing=[\"input paragraphs\", \"embed\", \"index\", \"attribute\"],         ann=HNSW(distance_metric=\"angular\"),         is_document_field=False,     ),     #     # Alteratively, for exact distance calculation not using HNSW:     #     # Field(name=\"paragraph_embeddings\", type=\"tensor(p{},x[384])\",     #       indexing=[\"input paragraphs\", \"embed\", \"attribute\"],     #       attribute=[\"distance-metric: angular\"],     #       is_document_field=False) ) <p>One field of particular interest is <code>paragraph_embeddings</code>. Note that we are not feeding embeddings to this instance. Instead, the embeddings are generated by using the embed feature, using the model configured at start. Read more in Text embedding made simple.</p> <p>Looking closely at the code, <code>paragraph_embeddings</code> uses <code>is_document_field=False</code>, meaning it will read another field as input (here <code>paragraph</code>), and run <code>embed</code> on it.</p> <p>As only one model is configured, <code>embed</code> will use that one - it is possible to configure mode models and use <code>embed model-id</code> as well.</p> <p>As the code comment illustrates, there can be different distrance metrics used, as well as using an exact or approximate nearest neighbor search.</p> In\u00a0[3]: Copied! <pre>app_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"semantic\",\n        inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n        inherits=\"default\",\n        first_phase=\"cos(distance(field,paragraph_embeddings))\",\n        match_features=[\"closest(paragraph_embeddings)\"],\n    )\n)\n\napp_package.schema.add_rank_profile(\n    RankProfile(name=\"bm25\", first_phase=\"2*bm25(title) + bm25(paragraphs)\")\n)\n\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"hybrid\",\n        inherits=\"semantic\",\n        functions=[\n            Function(\n                name=\"avg_paragraph_similarity\",\n                expression=\"\"\"reduce(\n                              sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x),\n                              avg,\n                              p\n                          )\"\"\",\n            ),\n            Function(\n                name=\"max_paragraph_similarity\",\n                expression=\"\"\"reduce(\n                              sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x),\n                              max,\n                              p\n                          )\"\"\",\n            ),\n            Function(\n                name=\"all_paragraph_similarities\",\n                expression=\"sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x)\",\n            ),\n        ],\n        first_phase=FirstPhaseRanking(\n            expression=\"cos(distance(field,paragraph_embeddings))\"\n        ),\n        second_phase=SecondPhaseRanking(\n            expression=\"firstPhase + avg_paragraph_similarity() + log( bm25(title) + bm25(paragraphs) + bm25(url))\"\n        ),\n        match_features=[\n            \"closest(paragraph_embeddings)\",\n            \"firstPhase\",\n            \"bm25(title)\",\n            \"bm25(paragraphs)\",\n            \"avg_paragraph_similarity\",\n            \"max_paragraph_similarity\",\n            \"all_paragraph_similarities\",\n        ],\n    )\n)\n</pre> app_package.schema.add_rank_profile(     RankProfile(         name=\"semantic\",         inputs=[(\"query(q)\", \"tensor(x[384])\")],         inherits=\"default\",         first_phase=\"cos(distance(field,paragraph_embeddings))\",         match_features=[\"closest(paragraph_embeddings)\"],     ) )  app_package.schema.add_rank_profile(     RankProfile(name=\"bm25\", first_phase=\"2*bm25(title) + bm25(paragraphs)\") )  app_package.schema.add_rank_profile(     RankProfile(         name=\"hybrid\",         inherits=\"semantic\",         functions=[             Function(                 name=\"avg_paragraph_similarity\",                 expression=\"\"\"reduce(                               sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x),                               avg,                               p                           )\"\"\",             ),             Function(                 name=\"max_paragraph_similarity\",                 expression=\"\"\"reduce(                               sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x),                               max,                               p                           )\"\"\",             ),             Function(                 name=\"all_paragraph_similarities\",                 expression=\"sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x)\",             ),         ],         first_phase=FirstPhaseRanking(             expression=\"cos(distance(field,paragraph_embeddings))\"         ),         second_phase=SecondPhaseRanking(             expression=\"firstPhase + avg_paragraph_similarity() + log( bm25(title) + bm25(paragraphs) + bm25(url))\"         ),         match_features=[             \"closest(paragraph_embeddings)\",             \"firstPhase\",             \"bm25(title)\",             \"bm25(paragraphs)\",             \"avg_paragraph_similarity\",             \"max_paragraph_similarity\",             \"all_paragraph_similarities\",         ],     ) ) In\u00a0[4]: Copied! <pre>app_package.schema.add_field_set(\n    FieldSet(name=\"default\", fields=[\"title\", \"url\", \"paragraphs\"])\n)\n</pre> app_package.schema.add_field_set(     FieldSet(name=\"default\", fields=[\"title\", \"url\", \"paragraphs\"]) ) In\u00a0[5]: Copied! <pre>app_package.schema.add_document_summary(\n    DocumentSummary(\n        name=\"minimal\",\n        summary_fields=[Summary(\"id\", \"int\"), Summary(\"title\", \"string\")],\n    )\n)\n</pre> app_package.schema.add_document_summary(     DocumentSummary(         name=\"minimal\",         summary_fields=[Summary(\"id\", \"int\"), Summary(\"title\", \"string\")],     ) ) In\u00a0[6]: Copied! <pre>Path(\"pkg\").mkdir(parents=True, exist_ok=True)\napp_package.to_files(\"pkg\")\n</pre> Path(\"pkg\").mkdir(parents=True, exist_ok=True) app_package.to_files(\"pkg\") <p>It is a good idea to inspect the files exported into <code>pkg</code> - these are files referred to in the Vespa Documentation.</p> In\u00a0[7]: Copied! <pre>! mkdir -p pkg/model\n! curl -L -o pkg/model/tokenizer.json \\\n  https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\n\n! curl -L -o pkg/model/e5-small-v2-int8.onnx \\\n  https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\n</pre> ! mkdir -p pkg/model ! curl -L -o pkg/model/tokenizer.json \\   https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json  ! curl -L -o pkg/model/e5-small-v2-int8.onnx \\   https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx <pre>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  694k  100  694k    0     0  2473k      0 --:--:-- --:--:-- --:--:-- 2508k\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 32.3M  100 32.3M    0     0  27.1M      0  0:00:01  0:00:01 --:--:-- 53.0M\n</pre> In\u00a0[8]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy_from_disk(application_name=\"wiki\", application_root=\"pkg\")\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy_from_disk(application_name=\"wiki\", application_root=\"pkg\") <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[9]: Copied! <pre>! curl -s -H \"Accept:application/vnd.github.v3.raw\" \\\n  https://api.github.com/repos/vespa-engine/sample-apps/contents/multi-vector-indexing/ext/articles.jsonl.zst | \\\n  zstdcat - &gt; articles.jsonl\n</pre> ! curl -s -H \"Accept:application/vnd.github.v3.raw\" \\   https://api.github.com/repos/vespa-engine/sample-apps/contents/multi-vector-indexing/ext/articles.jsonl.zst | \\   zstdcat - &gt; articles.jsonl <p>I you do not have ZSTD install, get <code>articles.jsonl.zip</code> and unzip it instead.</p> <p>Feed and index the Wikipedia articles using the Vespa CLI. As part of feeding, <code>embed</code> is called on each article, and the output of this is stored in the <code>paragraph_embeddings</code> field:</p> In\u00a0[10]: Copied! <pre>! vespa config set target local\n! vespa feed articles.jsonl\n</pre> ! vespa config set target local ! vespa feed articles.jsonl <pre>{\n  \"feeder.seconds\": 1.448,\n  \"feeder.ok.count\": 8,\n  \"feeder.ok.rate\": 5.524,\n  \"feeder.error.count\": 0,\n  \"feeder.inflight.count\": 0,\n  \"http.request.count\": 8,\n  \"http.request.bytes\": 12958,\n  \"http.request.MBps\": 0.009,\n  \"http.exception.count\": 0,\n  \"http.response.count\": 8,\n  \"http.response.bytes\": 674,\n  \"http.response.MBps\": 0.000,\n  \"http.response.error.count\": 0,\n  \"http.response.latency.millis.min\": 728,\n  \"http.response.latency.millis.avg\": 834,\n  \"http.response.latency.millis.max\": 1446,\n  \"http.response.code.counts\": {\n    \"200\": 8\n  }\n}\n</pre> <p>Note that creating embeddings is computationally expensive, but this is a small dataset with only 8 articles, so will be done in a few seconds.</p> <p>The Vespa instance is now populated with the Wikipedia articles, with generated embeddings, and ready for queries. The next sections have examples of various kinds of queries to run on the dataset.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\n\nresult: VespaQueryResponse = app.query(\n    body={\n        \"yql\": \"select * from wiki where true\",\n        \"ranking.profile\": \"unranked\",\n        \"hits\": 2,\n    }\n)\nif not result.is_successful():\n    raise ValueError(result.get_json())\nif len(result.hits) != 2:\n    raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits)))\nprint(json.dumps(result.hits, indent=4))\n</pre> from vespa.io import VespaQueryResponse  result: VespaQueryResponse = app.query(     body={         \"yql\": \"select * from wiki where true\",         \"ranking.profile\": \"unranked\",         \"hits\": 2,     } ) if not result.is_successful():     raise ValueError(result.get_json()) if len(result.hits) != 2:     raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits))) print(json.dumps(result.hits, indent=4)) In\u00a0[\u00a0]: Copied! <pre>result = app.query(\n    body={\n        \"yql\": \"select * from wiki where userQuery()\",\n        \"query\": 24,\n        \"ranking.profile\": \"bm25\",\n        \"hits\": 2,\n    }\n)\nif len(result.hits) != 2:\n    raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits)))\nprint(json.dumps(result.hits, indent=4))\n</pre> result = app.query(     body={         \"yql\": \"select * from wiki where userQuery()\",         \"query\": 24,         \"ranking.profile\": \"bm25\",         \"hits\": 2,     } ) if len(result.hits) != 2:     raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits))) print(json.dumps(result.hits, indent=4)) In\u00a0[14]: Copied! <pre>result = app.query(\n    body={\n        \"yql\": \"select * from wiki where {targetHits:2}nearestNeighbor(paragraph_embeddings,q)\",\n        \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",\n        \"ranking.profile\": \"semantic\",\n        \"presentation.format.tensors\": \"short-value\",\n        \"hits\": 2,\n    }\n)\nresult.hits\nif len(result.hits) != 2:\n    raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits)))\nprint(json.dumps(result.hits, indent=4))\n</pre> result = app.query(     body={         \"yql\": \"select * from wiki where {targetHits:2}nearestNeighbor(paragraph_embeddings,q)\",         \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",         \"ranking.profile\": \"semantic\",         \"presentation.format.tensors\": \"short-value\",         \"hits\": 2,     } ) result.hits if len(result.hits) != 2:     raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits))) print(json.dumps(result.hits, indent=4)) <pre>[\n    {\n        \"id\": \"id:wikipedia:wiki::9985\",\n        \"relevance\": 0.8807156260391702,\n        \"source\": \"wiki_content\",\n        \"fields\": {\n            \"matchfeatures\": {\n                \"closest(paragraph_embeddings)\": {\n                    \"4\": 1.0\n                }\n            },\n            \"sddocname\": \"wiki\",\n            \"paragraphs\": [\n                \"The 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and is divided into 24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only in the US and the English speaking parts of Canada) as military time or (only in the United Kingdom and now very rarely) as continental time. In some parts of the world, it is called railway time. Also, the international standard notation of time (ISO 8601) is based on this format.\",\n                \"A time in the 24-hour clock is written in the form hours:minutes (for example, 01:23), or hours:minutes:seconds (01:23:45). Numbers under 10 have a zero in front (called a leading zero); e.g. 09:07. Under the 24-hour clock system, the day begins at midnight, 00:00, and the last minute of the day begins at 23:59 and ends at 24:00, which is identical to 00:00 of the following day. 12:00 can only be mid-day. Midnight is called 24:00 and is used to mean the end of the day and 00:00 is used to mean the beginning of the day. For example, you would say \\\"Tuesday at 24:00\\\" and \\\"Wednesday at 00:00\\\" to mean exactly the same time.\",\n                \"However, the US military prefers not to say 24:00 - they do not like to have two names for the same thing, so they always say \\\"23:59\\\", which is one minute before midnight.\",\n                \"24-hour clock time is used in computers, military, public safety, and transport. In many Asian, European and Latin American countries people use it to write the time. Many European people use it in speaking.\",\n                \"In railway timetables 24:00 means the \\\"end\\\" of the day. For example, a train due to arrive at a station during the last minute of a day arrives at 24:00; but trains which depart during the first minute of the day go at 00:00.\"\n            ],\n            \"documentid\": \"id:wikipedia:wiki::9985\",\n            \"title\": \"24-hour clock\",\n            \"url\": \"https://simple.wikipedia.org/wiki?curid=9985\"\n        }\n    },\n    {\n        \"id\": \"id:wikipedia:wiki::59079\",\n        \"relevance\": 0.7972394509946005,\n        \"source\": \"wiki_content\",\n        \"fields\": {\n            \"matchfeatures\": {\n                \"closest(paragraph_embeddings)\": {\n                    \"4\": 1.0\n                }\n            },\n            \"sddocname\": \"wiki\",\n            \"paragraphs\": [\n                \"Logic gates are digital components. They normally work at only two levels of voltage, a positive level and zero level. Commonly they work based on two states: \\\"On\\\" and \\\"Off\\\". In the On state, voltage is positive. In the Off state, the voltage is at zero. The On state usually uses a voltage in the range of 3.5 to 5 volts. This range can be lower for some uses.\",\n                \"Logic gates compare the state at their inputs to decide what the state at their output should be. A logic gate is \\\"on\\\" or active when its rules are correctly met. At this time, electricity is flowing through the gate and the voltage at its output is at the level of its On state.\",\n                \"Logic gates are electronic versions of Boolean logic. Truth tables will tell you what the output will be, depending on the inputs.\",\n                \"AND gates have two inputs. The output of an AND gate is on only if both inputs are on. If at least one of the inputs is off, the output will be off.\",\n                \"Using the image at the right, if \\\"A\\\" and \\\"B\\\" are both in an On state, the output (out) will be an On state. If either \\\"A\\\" or \\\"B\\\" is in an Off state, the output will also be in an Off state. \\\"A\\\" and \\\"B\\\" must be On for the output to be On.\",\n                \"OR gates have two inputs. The output of an OR gate will be on if at least one of the inputs are on. If both inputs are off, the output will be off.\",\n                \"Using the image at the right, if either \\\"A\\\" or \\\"B\\\" is On, the output (\\\"out\\\") will also be On. If both \\\"A\\\" and \\\"B\\\" are Off, the output will be Off.\",\n                \"The NOT logic gate has only one input. If the input is On then the output will be Off. In other words, the NOT logic gate changes the signal from On to Off or from Off to On. It is sometimes called an inverter.\",\n                \"XOR (\\\"exclusive or\\\") gates have two inputs. The output of a XOR gate will be true only if the two inputs are different from each other. If both inputs are the same, the output will be off.\",\n                \"NAND means not both. It is called NAND because it means \\\"not and.\\\" This means that it will always output true unless both inputs are on.\",\n                \"XNOR means \\\"not exclusive or.\\\" This means that it will only output true if both inputs are the same. It is the opposite of a XOR logic gate.\"\n            ],\n            \"documentid\": \"id:wikipedia:wiki::59079\",\n            \"title\": \"Logic gate\",\n            \"url\": \"https://simple.wikipedia.org/wiki?curid=59079\"\n        }\n    }\n]\n</pre> <p>An interesting question then is, of the paragraphs in the document, which one was the closest? When analysing ranking, using match-features lets you export the scores used in the ranking calculations, see closest - from the result above:</p> <pre><code> \"matchfeatures\": {\n                \"closest(paragraph_embeddings)\": {\n                    \"4\": 1.0\n                }\n}\n</code></pre> <p>This means, the tensor of index 4 has the closest match. With this, it is straight forward to feed articles with an array of paragraphs and highlight the best matching paragraph in the document!</p> In\u00a0[17]: Copied! <pre>def find_best_paragraph(hit: dict) -&gt; str:\n    paragraphs = hit[\"fields\"][\"paragraphs\"]\n    match_features = hit[\"fields\"][\"matchfeatures\"]\n    index = int(list(match_features[\"closest(paragraph_embeddings)\"].keys())[0])\n    return paragraphs[index]\n</pre> def find_best_paragraph(hit: dict) -&gt; str:     paragraphs = hit[\"fields\"][\"paragraphs\"]     match_features = hit[\"fields\"][\"matchfeatures\"]     index = int(list(match_features[\"closest(paragraph_embeddings)\"].keys())[0])     return paragraphs[index] In\u00a0[18]: Copied! <pre>find_best_paragraph(result.hits[0])\n</pre> find_best_paragraph(result.hits[0]) Out[18]: <pre>'In railway timetables 24:00 means the \"end\" of the day. For example, a train due to arrive at a station during the last minute of a day arrives at 24:00; but trains which depart during the first minute of the day go at 00:00.'</pre> In\u00a0[20]: Copied! <pre>result = app.query(\n    body={\n        \"yql\": \"select * from wiki where userQuery() or ({targetHits:1}nearestNeighbor(paragraph_embeddings,q))\",\n        \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",\n        \"query\": \"what does 24 mean in the context of railways\",\n        \"ranking.profile\": \"hybrid\",\n        \"presentation.format.tensors\": \"short-value\",\n        \"hits\": 1,\n    }\n)\nif len(result.hits) != 1:\n    raise ValueError(\"Expected 1 hits, got {}\".format(len(result.hits)))\nprint(json.dumps(result.hits, indent=4))\n</pre> result = app.query(     body={         \"yql\": \"select * from wiki where userQuery() or ({targetHits:1}nearestNeighbor(paragraph_embeddings,q))\",         \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",         \"query\": \"what does 24 mean in the context of railways\",         \"ranking.profile\": \"hybrid\",         \"presentation.format.tensors\": \"short-value\",         \"hits\": 1,     } ) if len(result.hits) != 1:     raise ValueError(\"Expected 1 hits, got {}\".format(len(result.hits))) print(json.dumps(result.hits, indent=4)) <pre>[\n    {\n        \"id\": \"id:wikipedia:wiki::9985\",\n        \"relevance\": 4.163399168193791,\n        \"source\": \"wiki_content\",\n        \"fields\": {\n            \"matchfeatures\": {\n                \"bm25(paragraphs)\": 10.468827250036052,\n                \"bm25(title)\": 1.1272217840066168,\n                \"closest(paragraph_embeddings)\": {\n                    \"4\": 1.0\n                },\n                \"firstPhase\": 0.8807156260391702,\n                \"all_paragraph_similarities\": {\n                    \"1\": 0.8030083179473877,\n                    \"2\": 0.7992785573005676,\n                    \"3\": 0.8273358345031738,\n                    \"4\": 0.8807156085968018,\n                    \"0\": 0.849757194519043\n                },\n                \"avg_paragraph_similarity\": 0.8320191025733947,\n                \"max_paragraph_similarity\": 0.8807156085968018\n            },\n            \"sddocname\": \"wiki\",\n            \"paragraphs\": [\n                \"&lt;hi&gt;The&lt;/hi&gt; &lt;hi&gt;24&lt;/hi&gt;-hour clock is a way &lt;hi&gt;of&lt;/hi&gt; telling &lt;hi&gt;the&lt;/hi&gt; time &lt;hi&gt;in&lt;/hi&gt; which &lt;hi&gt;the&lt;/hi&gt; day runs from midnight to midnight and is divided into &lt;hi&gt;24&lt;/hi&gt; hours, numbered from 0 to 23. It &lt;hi&gt;does&lt;/hi&gt; not use a.m. or p.m. This system is also referred to (only &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; US and &lt;hi&gt;the&lt;/hi&gt; English speaking parts &lt;hi&gt;of&lt;/hi&gt; Canada) as military time or (only &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; United Kingdom and now very rarely) as continental time. &lt;hi&gt;In&lt;/hi&gt; some parts &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; world, it is called &lt;hi&gt;railway&lt;/hi&gt; time. Also, &lt;hi&gt;the&lt;/hi&gt; international standard notation &lt;hi&gt;of&lt;/hi&gt; time (ISO 8601) is based on this format.\",\n                \"A time &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; &lt;hi&gt;24&lt;/hi&gt;-hour clock is written &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; form hours:minutes (for example, 01:23), or hours:minutes:seconds (01:23:45). Numbers under 10 have a zero &lt;hi&gt;in&lt;/hi&gt; front (called a leading zero); e.g. 09:07. Under &lt;hi&gt;the&lt;/hi&gt; &lt;hi&gt;24&lt;/hi&gt;-hour clock system, &lt;hi&gt;the&lt;/hi&gt; day begins at midnight, 00:00, and &lt;hi&gt;the&lt;/hi&gt; last minute &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day begins at 23:59 and ends at &lt;hi&gt;24&lt;/hi&gt;:00, which is identical to 00:00 &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; following day. 12:00 can only be mid-day. Midnight is called &lt;hi&gt;24&lt;/hi&gt;:00 and is used to &lt;hi&gt;mean&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; end &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day and 00:00 is used to &lt;hi&gt;mean&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; beginning &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day. For example, you would say \\\"Tuesday at &lt;hi&gt;24&lt;/hi&gt;:00\\\" and \\\"Wednesday at 00:00\\\" to &lt;hi&gt;mean&lt;/hi&gt; exactly &lt;hi&gt;the&lt;/hi&gt; same time.\",\n                \"However, &lt;hi&gt;the&lt;/hi&gt; US military prefers not to say &lt;hi&gt;24&lt;/hi&gt;:00 - they &lt;hi&gt;do&lt;/hi&gt; not like to have two names for &lt;hi&gt;the&lt;/hi&gt; same thing, so they always say \\\"23:59\\\", which is one minute before midnight.\",\n                \"&lt;hi&gt;24&lt;/hi&gt;-hour clock time is used &lt;hi&gt;in&lt;/hi&gt; computers, military, public safety, and transport. &lt;hi&gt;In&lt;/hi&gt; many Asian, European and Latin American countries people use it to write &lt;hi&gt;the&lt;/hi&gt; time. Many European people use it &lt;hi&gt;in&lt;/hi&gt; speaking.\",\n                \"&lt;hi&gt;In&lt;/hi&gt; &lt;hi&gt;railway&lt;/hi&gt; timetables &lt;hi&gt;24&lt;/hi&gt;:00 means &lt;hi&gt;the&lt;/hi&gt; \\\"end\\\" &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day. For example, a train due to arrive at a station during &lt;hi&gt;the&lt;/hi&gt; last minute &lt;hi&gt;of&lt;/hi&gt; a day arrives at &lt;hi&gt;24&lt;/hi&gt;:00; but trains which depart during &lt;hi&gt;the&lt;/hi&gt; first minute &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day go at 00:00.\"\n            ],\n            \"documentid\": \"id:wikipedia:wiki::9985\",\n            \"title\": \"24-hour clock\",\n            \"url\": \"https://simple.wikipedia.org/wiki?curid=9985\"\n        }\n    }\n]\n</pre> <p>This case combines exact search with nearestNeighbor search. The <code>hybrid</code> rank-profile above also calculates several additional features using tensor expressions:</p> <ul> <li><code>firstPhase</code> is the score of the first ranking phase, configured in the hybrid profile as <code>cos(distance(field, paragraph_embeddings))</code>.</li> <li><code>all_paragraph_similarities</code> returns all the similarity scores for all paragraphs.</li> <li><code>avg_paragraph_similarity</code> is the average similarity score across all the paragraphs.</li> <li><code>max_paragraph_similarity</code> is the same as <code>firstPhase</code>, but computed using a tensor expression.</li> </ul> <p>These additional features are calculated during second-phase ranking to limit the number of vector computations.</p> <p>The Tensor Playground is useful to play with tensor expressions.</p> <p>The Hybrid Search blog post series is a good read to learn more about hybrid ranking!</p> In\u00a0[23]: Copied! <pre>def find_paragraph_scores(hit: dict) -&gt; str:\n    paragraphs = hit[\"fields\"][\"paragraphs\"]\n    match_features = hit[\"fields\"][\"matchfeatures\"]\n    indexes = [int(v) for v in match_features[\"all_paragraph_similarities\"]]\n    scores = list(match_features[\"all_paragraph_similarities\"].values())\n    return list(zip([paragraphs[i] for i in indexes], scores))\n</pre> def find_paragraph_scores(hit: dict) -&gt; str:     paragraphs = hit[\"fields\"][\"paragraphs\"]     match_features = hit[\"fields\"][\"matchfeatures\"]     indexes = [int(v) for v in match_features[\"all_paragraph_similarities\"]]     scores = list(match_features[\"all_paragraph_similarities\"].values())     return list(zip([paragraphs[i] for i in indexes], scores)) In\u00a0[24]: Copied! <pre>find_paragraph_scores(result.hits[0])\n</pre> find_paragraph_scores(result.hits[0]) Out[24]: <pre>[('A time &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; &lt;hi&gt;24&lt;/hi&gt;-hour clock is written &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; form hours:minutes (for example, 01:23), or hours:minutes:seconds (01:23:45). Numbers under 10 have a zero &lt;hi&gt;in&lt;/hi&gt; front (called a leading zero); e.g. 09:07. Under &lt;hi&gt;the&lt;/hi&gt; &lt;hi&gt;24&lt;/hi&gt;-hour clock system, &lt;hi&gt;the&lt;/hi&gt; day begins at midnight, 00:00, and &lt;hi&gt;the&lt;/hi&gt; last minute &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day begins at 23:59 and ends at &lt;hi&gt;24&lt;/hi&gt;:00, which is identical to 00:00 &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; following day. 12:00 can only be mid-day. Midnight is called &lt;hi&gt;24&lt;/hi&gt;:00 and is used to &lt;hi&gt;mean&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; end &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day and 00:00 is used to &lt;hi&gt;mean&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; beginning &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day. For example, you would say \"Tuesday at &lt;hi&gt;24&lt;/hi&gt;:00\" and \"Wednesday at 00:00\" to &lt;hi&gt;mean&lt;/hi&gt; exactly &lt;hi&gt;the&lt;/hi&gt; same time.',\n  0.8030083179473877),\n ('However, &lt;hi&gt;the&lt;/hi&gt; US military prefers not to say &lt;hi&gt;24&lt;/hi&gt;:00 - they &lt;hi&gt;do&lt;/hi&gt; not like to have two names for &lt;hi&gt;the&lt;/hi&gt; same thing, so they always say \"23:59\", which is one minute before midnight.',\n  0.7992785573005676),\n ('&lt;hi&gt;24&lt;/hi&gt;-hour clock time is used &lt;hi&gt;in&lt;/hi&gt; computers, military, public safety, and transport. &lt;hi&gt;In&lt;/hi&gt; many Asian, European and Latin American countries people use it to write &lt;hi&gt;the&lt;/hi&gt; time. Many European people use it &lt;hi&gt;in&lt;/hi&gt; speaking.',\n  0.8273358345031738),\n ('&lt;hi&gt;In&lt;/hi&gt; &lt;hi&gt;railway&lt;/hi&gt; timetables &lt;hi&gt;24&lt;/hi&gt;:00 means &lt;hi&gt;the&lt;/hi&gt; \"end\" &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day. For example, a train due to arrive at a station during &lt;hi&gt;the&lt;/hi&gt; last minute &lt;hi&gt;of&lt;/hi&gt; a day arrives at &lt;hi&gt;24&lt;/hi&gt;:00; but trains which depart during &lt;hi&gt;the&lt;/hi&gt; first minute &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day go at 00:00.',\n  0.8807156085968018),\n ('&lt;hi&gt;The&lt;/hi&gt; &lt;hi&gt;24&lt;/hi&gt;-hour clock is a way &lt;hi&gt;of&lt;/hi&gt; telling &lt;hi&gt;the&lt;/hi&gt; time &lt;hi&gt;in&lt;/hi&gt; which &lt;hi&gt;the&lt;/hi&gt; day runs from midnight to midnight and is divided into &lt;hi&gt;24&lt;/hi&gt; hours, numbered from 0 to 23. It &lt;hi&gt;does&lt;/hi&gt; not use a.m. or p.m. This system is also referred to (only &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; US and &lt;hi&gt;the&lt;/hi&gt; English speaking parts &lt;hi&gt;of&lt;/hi&gt; Canada) as military time or (only &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; United Kingdom and now very rarely) as continental time. &lt;hi&gt;In&lt;/hi&gt; some parts &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; world, it is called &lt;hi&gt;railway&lt;/hi&gt; time. Also, &lt;hi&gt;the&lt;/hi&gt; international standard notation &lt;hi&gt;of&lt;/hi&gt; time (ISO 8601) is based on this format.',\n  0.849757194519043)]</pre> In\u00a0[25]: Copied! <pre>result = app.query(\n    body={\n        \"yql\": 'select * from wiki where url contains \"9985\" and userQuery() or ({targetHits:1}nearestNeighbor(paragraph_embeddings,q))',\n        \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",\n        \"query\": \"what does 24 mean in the context of railways\",\n        \"ranking.profile\": \"hybrid\",\n        \"bolding\": False,\n        \"presentation.format.tensors\": \"short-value\",\n        \"hits\": 1,\n    }\n)\nif len(result.hits) != 1:\n    raise ValueError(\"Expected one hit, got {}\".format(len(result.hits)))\nprint(json.dumps(result.hits, indent=4))\n</pre> result = app.query(     body={         \"yql\": 'select * from wiki where url contains \"9985\" and userQuery() or ({targetHits:1}nearestNeighbor(paragraph_embeddings,q))',         \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",         \"query\": \"what does 24 mean in the context of railways\",         \"ranking.profile\": \"hybrid\",         \"bolding\": False,         \"presentation.format.tensors\": \"short-value\",         \"hits\": 1,     } ) if len(result.hits) != 1:     raise ValueError(\"Expected one hit, got {}\".format(len(result.hits))) print(json.dumps(result.hits, indent=4)) <pre>[\n    {\n        \"id\": \"id:wikipedia:wiki::9985\",\n        \"relevance\": 4.307079208249452,\n        \"source\": \"wiki_content\",\n        \"fields\": {\n            \"matchfeatures\": {\n                \"bm25(paragraphs)\": 10.468827250036052,\n                \"bm25(title)\": 1.1272217840066168,\n                \"closest(paragraph_embeddings)\": {\n                    \"type\": \"tensor&lt;float&gt;(p{})\",\n                    \"cells\": {\n                        \"4\": 1.0\n                    }\n                },\n                \"firstPhase\": 0.8807156260391702,\n                \"all_paragraph_similarities\": {\n                    \"type\": \"tensor&lt;float&gt;(p{})\",\n                    \"cells\": {\n                        \"1\": 0.8030083179473877,\n                        \"2\": 0.7992785573005676,\n                        \"3\": 0.8273358345031738,\n                        \"4\": 0.8807156085968018,\n                        \"0\": 0.849757194519043\n                    }\n                },\n                \"avg_paragraph_similarity\": 0.8320191025733947,\n                \"max_paragraph_similarity\": 0.8807156085968018\n            },\n            \"sddocname\": \"wiki\",\n            \"paragraphs\": [\n                \"The 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and is divided into 24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only in the US and the English speaking parts of Canada) as military time or (only in the United Kingdom and now very rarely) as continental time. In some parts of the world, it is called railway time. Also, the international standard notation of time (ISO 8601) is based on this format.\",\n                \"A time in the 24-hour clock is written in the form hours:minutes (for example, 01:23), or hours:minutes:seconds (01:23:45). Numbers under 10 have a zero in front (called a leading zero); e.g. 09:07. Under the 24-hour clock system, the day begins at midnight, 00:00, and the last minute of the day begins at 23:59 and ends at 24:00, which is identical to 00:00 of the following day. 12:00 can only be mid-day. Midnight is called 24:00 and is used to mean the end of the day and 00:00 is used to mean the beginning of the day. For example, you would say \\\"Tuesday at 24:00\\\" and \\\"Wednesday at 00:00\\\" to mean exactly the same time.\",\n                \"However, the US military prefers not to say 24:00 - they do not like to have two names for the same thing, so they always say \\\"23:59\\\", which is one minute before midnight.\",\n                \"24-hour clock time is used in computers, military, public safety, and transport. In many Asian, European and Latin American countries people use it to write the time. Many European people use it in speaking.\",\n                \"In railway timetables 24:00 means the \\\"end\\\" of the day. For example, a train due to arrive at a station during the last minute of a day arrives at 24:00; but trains which depart during the first minute of the day go at 00:00.\"\n            ],\n            \"documentid\": \"id:wikipedia:wiki::9985\",\n            \"title\": \"24-hour clock\",\n            \"url\": \"https://simple.wikipedia.org/wiki?curid=9985\"\n        }\n    }\n]\n</pre> <p>In short, the above query demonstrates how easy it is to combine various ranking strategies, and also combine with filters.</p> <p>To learn more about pre-filtering vs post-filtering, read Filtering strategies and serving performance. Semantic search with multi-vector indexing is a great read overall for this domain.</p> In\u00a0[26]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"examples/multi-vector-indexing.html#multi-vector-indexing-with-hnsw","title":"Multi-vector indexing with HNSW\u00b6","text":"<p>This is the pyvespa steps of the multi-vector-indexing sample application. Go to the source for a full description and prerequisites, and read the blog post. Highlighted features:</p> <ul> <li>Approximate Nearest Neighbor Search - using HNSW or exact</li> <li>Use a Component to configure the Huggingface embedder.</li> <li>Using synthetic fields with auto-generated embeddings in data and query flow.</li> <li>Application package file export, model files in the application package, deployment from files.</li> <li>Multiphased ranking.</li> <li>How to control text search result highlighting.</li> </ul> <p>For simpler examples, see text search and pyvespa examples.</p> <p>Pyvespa is an add-on to Vespa, and this guide will export the application package containing <code>services.xml</code> and <code>wiki.sd</code>. The latter is the schema file for this application - knowing services.xml and schema files is useful when reading Vespa documentation.</p>"},{"location":"examples/multi-vector-indexing.html#create-the-application","title":"Create the application\u00b6","text":"<p>Configure the Vespa instance with a component loading the E5-small model. Components are used to plug in code and models to a Vespa application - read more:</p>"},{"location":"examples/multi-vector-indexing.html#configure-fields","title":"Configure fields\u00b6","text":"<p>Vespa has a variety of basic and complex field types. This application uses a combination of integer, text and tensor fields, making it easy to implement hybrid ranking use cases:</p>"},{"location":"examples/multi-vector-indexing.html#configure-rank-profiles","title":"Configure rank profiles\u00b6","text":"<p>A rank profile defines the computation for the ranking, with a wide range of possible features as input. Below you will find <code>first_phase</code> ranking using text ranking (<code>bm</code>), semantic ranking using vector distance (consider a tensor a vector here), and combinations of the two:</p>"},{"location":"examples/multi-vector-indexing.html#configure-fieldset","title":"Configure fieldset\u00b6","text":"<p>A fieldset is a way to configure search in multiple fields:</p>"},{"location":"examples/multi-vector-indexing.html#configure-document-summary","title":"Configure document summary\u00b6","text":"<p>A document summary is the collection of fields to return in query results - the default summary is used unless other specified in the query. Here we configure a <code>minimal</code> fieldset without the larger paragraph text/embedding fields:</p>"},{"location":"examples/multi-vector-indexing.html#export-the-configuration","title":"Export the configuration\u00b6","text":"<p>At this point, the application is well defined. Remember that the Component configuration at start configures model files to be found in a <code>model</code> directory. We must therefore export the configuration and add the models, before we can deploy to the Vespa instance. Export the application package:</p>"},{"location":"examples/multi-vector-indexing.html#download-model-files","title":"Download model files\u00b6","text":"<p>At this point, we can save the model files into the application package:</p>"},{"location":"examples/multi-vector-indexing.html#deploy-the-application","title":"Deploy the application\u00b6","text":"<p>As all the files in the app package are ready, we can start a Vespa instance - here using Docker. Deploy the app package:</p>"},{"location":"examples/multi-vector-indexing.html#feed-documents","title":"Feed documents\u00b6","text":"<p>Download the Wikipedia articles:</p>"},{"location":"examples/multi-vector-indexing.html#simple-retrieve-all-articles-with-undefined-ranking","title":"Simple retrieve all articles with undefined ranking\u00b6","text":"<p>Run a query selecting all documents, returning two of them. The rank profile is the built-in <code>unranked</code> which means no ranking calculations are done, the results are returned in random order:</p>"},{"location":"examples/multi-vector-indexing.html#traditional-keyword-search-with-bm25-ranking-on-the-article-level","title":"Traditional keyword search with BM25 ranking on the article level\u00b6","text":"<p>Run a text-search query and use the bm25 ranking profile configured at the start of this guide: <code>2*bm25(title) + bm25(paragraphs)</code>. Here, we use BM25 on the <code>title</code> and <code>paragraph</code> text fields, giving more weight to matches in title:</p>"},{"location":"examples/multi-vector-indexing.html#semantic-vector-search-on-the-paragraph-level","title":"Semantic vector search on the paragraph level\u00b6","text":"<p>This query creates an embedding of the query \"what does 24 mean in the context of railways\" and specifies the <code>semantic</code> ranking profile: <code>cos(distance(field,paragraph_embeddings))</code>. This will hence compute the distance between the vector in the query and the vectors computed when indexing: <code>\"input paragraphs\", \"embed\", \"index\", \"attribute\"</code>:</p>"},{"location":"examples/multi-vector-indexing.html#hybrid-search-and-ranking","title":"Hybrid search and ranking\u00b6","text":"<p>Hybrid combining keyword search on the article level with vector search in the paragraph index:</p>"},{"location":"examples/multi-vector-indexing.html#hybrid-search-and-filter","title":"Hybrid search and filter\u00b6","text":"<p>YQL is a structured query langauge. In the query examples, the user input is fed as-is using the <code>userQuery()</code> operator.</p> <p>Filters are normally separate from the user input, below is an example of adding a filter <code>url contains \"9985\"</code> to the YQL string.</p> <p>Finally, the use the Query API for other options, like highlighting - here disable bolding:</p>"},{"location":"examples/multi-vector-indexing.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html","title":"Multilingual multi vector reps with cohere cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa cohere==4.57 datasets vespacli\n</pre> !pip3 install -U pyvespa cohere==4.57 datasets vespacli In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\nlang = \"de\"  # Use the first 10K chunks from the German Wikipedia subset\ndocs = load_dataset(\n    \"Cohere/wikipedia-2023-11-embed-multilingual-v3-int8-binary\",\n    lang,\n    split=\"train\",\n    streaming=True,\n).take(10000)\n</pre> from datasets import load_dataset  lang = \"de\"  # Use the first 10K chunks from the German Wikipedia subset docs = load_dataset(     \"Cohere/wikipedia-2023-11-embed-multilingual-v3-int8-binary\",     lang,     split=\"train\",     streaming=True, ).take(10000) In\u00a0[160]: Copied! <pre>pages = dict()\nfor d in docs:\n    url = d[\"url\"]\n    if url not in pages:\n        pages[url] = [d]\n    else:\n        pages[url].append(d)\n</pre> pages = dict() for d in docs:     url = d[\"url\"]     if url not in pages:         pages[url] = [d]     else:         pages[url].append(d) In\u00a0[173]: Copied! <pre>print(len(list(pages.keys())))\n</pre> print(len(list(pages.keys()))) <pre>1866\n</pre> In\u00a0[174]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\nmy_schema = Schema(\n    name=\"page\",\n    mode=\"index\",\n    document=Document(\n        fields=[\n            Field(name=\"doc_id\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"language\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\", \"set_language\"],\n                match=[\"word\"],\n                rank=\"filter\",\n            ),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"chunks\",\n                type=\"array&lt;string&gt;\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"url\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"binary_vectors\",\n                type=\"tensor&lt;int8&gt;(chunk{}, x[128])\",\n                indexing=[\"attribute\", \"index\"],\n                attribute=[\"distance-metric: hamming\"],\n            ),\n            Field(\n                name=\"int8_vectors\",\n                type=\"tensor&lt;int8&gt;(chunk{}, x[1024])\",\n                indexing=[\"attribute\"],\n                attribute=[\"paged\"],\n            ),\n        ]\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"chunks\", \"title\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  my_schema = Schema(     name=\"page\",     mode=\"index\",     document=Document(         fields=[             Field(name=\"doc_id\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"language\",                 type=\"string\",                 indexing=[\"summary\", \"index\", \"set_language\"],                 match=[\"word\"],                 rank=\"filter\",             ),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"chunks\",                 type=\"array\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"url\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"binary_vectors\",                 type=\"tensor(chunk{}, x[128])\",                 indexing=[\"attribute\", \"index\"],                 attribute=[\"distance-metric: hamming\"],             ),             Field(                 name=\"int8_vectors\",                 type=\"tensor(chunk{}, x[1024])\",                 indexing=[\"attribute\"],                 attribute=[\"paged\"],             ),         ]     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"chunks\", \"title\"])], ) <p>We must add the schema to a Vespa application package. This consists of configuration files, schemas, models, and possibly even custom code (plugins).</p> In\u00a0[9]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"wikipedia\"\nvespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema])\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"wikipedia\" vespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema]) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p><code>unpack_bits</code> unpacks the binary representation into a 1024-dimensional float vector doc.</p> <p>We define two tensor inputs, one compact binary representation that is used for the nearestNeighbor search and one full version that is used in ranking.</p> In\u00a0[138]: Copied! <pre>from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function\n\n\nrerank = RankProfile(\n    name=\"rerank\",\n    inputs=[\n        (\"query(q_binary)\", \"tensor&lt;int8&gt;(x[128])\"),\n        (\"query(q_int8)\", \"tensor&lt;int8&gt;(x[1024])\"),\n        (\"query(q_full)\", \"tensor&lt;float&gt;(x[1024])\"),\n    ],\n    functions=[\n        Function(  # this returns a tensor&lt;float&gt;(chunk{}, x[1024]) with values -1 or 1\n            name=\"unpack_binary_representation\",\n            expression=\"2*unpack_bits(attribute(binary_vectors)) -1\",\n        ),\n        Function(\n            name=\"all_chunks_cosine\",\n            expression=\"cosine_similarity(query(q_int8), attribute(int8_vectors),x)\",\n        ),\n        Function(\n            name=\"int8_float_dot_products\",\n            expression=\"sum(query(q_full)*unpack_binary_representation,x)\",\n        ),\n    ],\n    first_phase=FirstPhaseRanking(\n        expression=\"reduce(int8_float_dot_products, max, chunk)\"\n    ),\n    second_phase=SecondPhaseRanking(\n        expression=\"reduce(all_chunks_cosine, max, chunk)\"  # rescoring using the full query and a unpacked binary_vector\n    ),\n    match_features=[\n        \"distance(field, binary_vectors)\",\n        \"all_chunks_cosine\",\n        \"firstPhase\",\n        \"bm25(title)\",\n        \"bm25(chunks)\",\n    ],\n)\nmy_schema.add_rank_profile(rerank)\n</pre> from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function   rerank = RankProfile(     name=\"rerank\",     inputs=[         (\"query(q_binary)\", \"tensor(x[128])\"),         (\"query(q_int8)\", \"tensor(x[1024])\"),         (\"query(q_full)\", \"tensor(x[1024])\"),     ],     functions=[         Function(  # this returns a tensor(chunk{}, x[1024]) with values -1 or 1             name=\"unpack_binary_representation\",             expression=\"2*unpack_bits(attribute(binary_vectors)) -1\",         ),         Function(             name=\"all_chunks_cosine\",             expression=\"cosine_similarity(query(q_int8), attribute(int8_vectors),x)\",         ),         Function(             name=\"int8_float_dot_products\",             expression=\"sum(query(q_full)*unpack_binary_representation,x)\",         ),     ],     first_phase=FirstPhaseRanking(         expression=\"reduce(int8_float_dot_products, max, chunk)\"     ),     second_phase=SecondPhaseRanking(         expression=\"reduce(all_chunks_cosine, max, chunk)\"  # rescoring using the full query and a unpacked binary_vector     ),     match_features=[         \"distance(field, binary_vectors)\",         \"all_chunks_cosine\",         \"firstPhase\",         \"bm25(title)\",         \"bm25(chunks)\",     ], ) my_schema.add_rank_profile(rerank) In\u00a0[24]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() In\u00a0[153]: Copied! <pre>def generate_vespa_feed_documents(pages):\n    for url, chunks in pages.items():\n        title = None\n        text_chunks = []\n        binary_vectors = {}\n        int8_vectors = {}\n        for chunk_id, chunk in enumerate(chunks):\n            title = chunk[\"title\"]\n            text = chunk[\"text\"]\n            text_chunks.append(text)\n            emb_ubinary = chunk[\"emb_ubinary\"]\n            emb_ubinary = [x - 128 for x in emb_ubinary]\n            emb_int8 = chunk[\"emb_int8\"]\n\n            binary_vectors[chunk_id] = emb_ubinary\n            int8_vectors[chunk_id] = emb_int8\n\n        vespa_json = {\n            \"id\": url,\n            \"fields\": {\n                \"doc_id\": url,\n                \"url\": url,\n                \"language\": lang,  # Assuming `lang` is defined somewhere\n                \"title\": title,\n                \"chunks\": text_chunks,\n                \"binary_vectors\": binary_vectors,\n                \"int8_vectors\": int8_vectors,\n            },\n        }\n        yield vespa_json\n</pre> def generate_vespa_feed_documents(pages):     for url, chunks in pages.items():         title = None         text_chunks = []         binary_vectors = {}         int8_vectors = {}         for chunk_id, chunk in enumerate(chunks):             title = chunk[\"title\"]             text = chunk[\"text\"]             text_chunks.append(text)             emb_ubinary = chunk[\"emb_ubinary\"]             emb_ubinary = [x - 128 for x in emb_ubinary]             emb_int8 = chunk[\"emb_int8\"]              binary_vectors[chunk_id] = emb_ubinary             int8_vectors[chunk_id] = emb_int8          vespa_json = {             \"id\": url,             \"fields\": {                 \"doc_id\": url,                 \"url\": url,                 \"language\": lang,  # Assuming `lang` is defined somewhere                 \"title\": title,                 \"chunks\": text_chunks,                 \"binary_vectors\": binary_vectors,                 \"int8_vectors\": int8_vectors,             },         }         yield vespa_json In\u00a0[154]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         ) In\u00a0[156]: Copied! <pre>app.feed_iterable(\n    iter=generate_vespa_feed_documents(pages),\n    schema=\"page\",\n    callback=callback,\n    max_queue_size=4000,\n    max_workers=16,\n    max_connections=16,\n)\n</pre> app.feed_iterable(     iter=generate_vespa_feed_documents(pages),     schema=\"page\",     callback=callback,     max_queue_size=4000,     max_workers=16,     max_connections=16, ) In\u00a0[48]: Copied! <pre>import cohere\n\n# Make sure that the environment variable CO_API_KEY is set to your API key\nco = cohere.Client()\n</pre> import cohere  # Make sure that the environment variable CO_API_KEY is set to your API key co = cohere.Client() In\u00a0[175]: Copied! <pre>query = 'Welche britische Rockband hat das Lied \"Spread Your Wings\"?'\n# Make sure to set input_type=\"search_query\" when getting the embeddings for the query.\n# We ask for 3 types of embeddings: float, binary, and int8\nquery_emb = co.embed(\n    [query],\n    model=\"embed-multilingual-v3.0\",\n    input_type=\"search_query\",\n    embedding_types=[\"float\", \"binary\", \"int8\"],\n)\n</pre> query = 'Welche britische Rockband hat das Lied \"Spread Your Wings\"?' # Make sure to set input_type=\"search_query\" when getting the embeddings for the query. # We ask for 3 types of embeddings: float, binary, and int8 query_emb = co.embed(     [query],     model=\"embed-multilingual-v3.0\",     input_type=\"search_query\",     embedding_types=[\"float\", \"binary\", \"int8\"], ) <p>Now, we use the nearestNeighbor query operator to to retrieve 1000 pages using hamming distance. This phase uses the minimum chunk-level distance for selecting pages. Essentially finding the best chunk in the page. This ensures diversity as we retrieve pages, not chunks.</p> <p>These hits are exposed to the configured ranking phases that perform the re-ranking.</p> <p>Notice the language parameter, for language-specific processing of the query.</p> In\u00a0[158]: Copied! <pre>from vespa.io import VespaQueryResponse\n\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select * from page where userQuery() or ({targetHits:1000, approximate:true}nearestNeighbor(binary_vectors,q_binary))\",\n    ranking=\"rerank\",\n    query=query,\n    language=\"de\",  # don't guess the language of the query\n    body={\n        \"presentation.format.tensors\": \"short-value\",\n        \"input.query(q_binary)\": query_emb.embeddings.binary[0],\n        \"input.query(q_full)\": query_emb.embeddings.float[0],\n        \"input.query(q_int8)\": query_emb.embeddings.int8[0],\n    },\n)\nassert response.is_successful()\nresponse.hits[0]\n</pre> from vespa.io import VespaQueryResponse   response: VespaQueryResponse = app.query(     yql=\"select * from page where userQuery() or ({targetHits:1000, approximate:true}nearestNeighbor(binary_vectors,q_binary))\",     ranking=\"rerank\",     query=query,     language=\"de\",  # don't guess the language of the query     body={         \"presentation.format.tensors\": \"short-value\",         \"input.query(q_binary)\": query_emb.embeddings.binary[0],         \"input.query(q_full)\": query_emb.embeddings.float[0],         \"input.query(q_int8)\": query_emb.embeddings.int8[0],     }, ) assert response.is_successful() response.hits[0] Out[158]: <pre>{'id': 'id:page:page::https:/de.wikipedia.org/wiki/Spread Your Wings',\n 'relevance': 0.8184863924980164,\n 'source': 'wikipedia_content',\n 'fields': {'matchfeatures': {'bm25(chunks)': 28.125529605038967,\n   'bm25(title)': 7.345395294159827,\n   'distance(field,binary_vectors)': 170.0,\n   'firstPhase': 8.274434089660645,\n   'all_chunks_cosine': {'0': 0.8184863924980164,\n    '1': 0.6203299760818481,\n    '2': 0.643619954586029,\n    '3': 0.6706648468971252,\n    '4': 0.524447500705719,\n    '5': 0.6730406880378723}},\n  'sddocname': 'page',\n  'documentid': 'id:page:page::https:/de.wikipedia.org/wiki/Spread Your Wings',\n  'doc_id': 'https://de.wikipedia.org/wiki/Spread%20Your%20Wings',\n  'language': 'de',\n  'title': 'Spread Your Wings',\n  'chunks': ['Spread Your Wings ist ein Lied der britischen Rockband Queen, das von deren Bassisten John Deacon geschrieben wurde. Es ist auf dem im Oktober 1977 erschienenen Album News of the World enthalten und wurde am 10. Februar 1978 in Europa als Single mit Sheer Heart Attack als B-Seite ver\u00f6ffentlicht. In Nordamerika wurde es nicht als Single ver\u00f6ffentlicht, sondern erschien stattdessen 1980 als B-Seite des Billboard Nummer-1-Hits Crazy Little Thing Called Love. Das Lied wurde zwar kein gro\u00dfer Hit in den Charts, ist aber unter Queen-Fans sehr beliebt.',\n   'Der Text beschreibt einen jungen Mann namens Sammy, der in einer Bar zum Putzen arbeitet (\u201cYou should\u2019ve been sweeping/up the Emerald bar\u201d). W\u00e4hrend sein Chef ihn in den Strophen beschimpft und sagt, er habe keinerlei Ambitionen und solle sich mit dem zufriedengeben, was er hat (\u201cYou\u2019ve got no real ambition,/you won\u2019t get very far/Sammy boy don\u2019t you know who you are/Why can\u2019t you be happy/at the Emerald bar\u201d), ermuntert ihn der Erz\u00e4hler im Refrain, seinen Tr\u00e4umen nachzugehen (\u201cspread your wings and fly away/Fly away, far away/Pull yourself together \u2018cause you know you should do better/That\u2019s because you\u2019re a free man.\u201d).',\n   'Das Lied ist im 4/4-Takt geschrieben, beginnt in der Tonart D-Dur, wechselt in der Bridge zu deren Paralleltonart h-Moll und endet wieder mit D-Dur. Es beginnt mit einem kurzen Piano-Intro, gefolgt von der ersten Strophe, die nur mit einer akustischen Gitarre, Piano und Hi-Hats begleitet wird, und dem Refrain, in dem die E-Gitarre und das Schlagzeug hinzukommen. Die Bridge besteht aus kurzen, langsamen Gitarrent\u00f6nen. Die zweite Strophe enth\u00e4lt im Gegensatz zur ersten beinahe von Anfang an E-Gitarren-Kl\u00e4nge und Schlagzeugt\u00f6ne. Darauf folgt nochmals der Refrain. Das Outro ist \u2013 abgesehen von zwei kurzen Rufen \u2013 instrumental. Es besteht aus einem l\u00e4ngeren Gitarrensolo, in dem \u2013 was f\u00fcr Queen \u00e4u\u00dferst ungew\u00f6hnlich ist \u2013 dieselbe Akkordfolge mehrere Male wiederholt wird und ab dem vierten Mal langsam ausblendet. Das ganze Lied enth\u00e4lt keinerlei Hintergrundgesang, sondern nur den Leadgesang von Freddie Mercury.',\n   'Das Musikvideo wurde ebenso wie das zu We Will Rock You im Januar 1978 im Garten von Roger Taylors damaligen Anwesen Millhanger House gedreht, welches sich im Dorf Thursley im S\u00fcdwesten der englischen Grafschaft Surrey befindet. Der Boden ist dabei von einer Eis- und Schneeschicht \u00fcberzogen, auf der die Musiker spielten.',\n   \"Brian May sagte dazu sp\u00e4ter: \u201cLooking back, it couldn't be done there \u2013 you couldn't do that!\u201d (\u201eWenn ich zur\u00fcckschaue, h\u00e4tte es nicht dort gemacht werden d\u00fcrfen \u2013 man konnte das nicht tun!\u201c)\",\n   'Das Lied wurde mehrfach gecovert, unter anderem von der deutschen Metal-Band Blind Guardian auf ihrem 1992 erschienenen Album Somewhere Far Beyond. Weitere Coverversionen gibt es u. a. von Jeff Scott Soto und Shawn Mars.'],\n  'url': 'https://de.wikipedia.org/wiki/Spread%20Your%20Wings'}}</pre> <p>Notice the returned hits. The <code>relevance</code> is the score assigned by the second-phase expression. Also notice, that we included bm25 scores in the match-features. In this case, they do not influence ranking. The bm25 over chunks is calculated across all the elements, like if it was a bag of words or a single field string.</p> <p>We now have the full Wikipedia context for all the retrieved pages. We have all the chunks and all the cosine similarity scores for all the chunks in the wikipedia page, and no need to duplicate title and url into separate retrievable units like with single-vector databases.</p> <p>In RAG applications, we can now choose how much context we want to input to the generative step:</p> <ul> <li>All the chunks</li> <li>Only the best k chunks with a threshold on the cosine similarity</li> <li>The adjacent chunks of the best chunk</li> </ul> <p>Or combinations of the above.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#multilingual-hybrid-search-with-cohere-binary-embeddings-and-vespa","title":"Multilingual Hybrid Search with Cohere binary embeddings and Vespa\u00b6","text":"<p>Cohere just released a new embedding API supporting binary vectors. Read the announcement in the blog post: Cohere int8 &amp; binary Embeddings - Scale Your Vector Database to Large Datasets.</p> <p>We are excited to announce that Cohere Embed is the first embedding model that natively supports int8 and binary embeddings.</p> <p>This notebook demonstrates:</p> <ul> <li>Building a multilingual search application over a sample of the German split of Wikipedia using binarized cohere embeddings</li> <li>Indexing multiple binary embeddings per document; without having to split the chunks across multiple retrievable units</li> <li>Hybrid search, combining the lexical matching capabilities of Vespa with Cohere binary embeddings</li> <li>Re-scoring the binarized vectors for improved accuracy</li> </ul> <p></p> <p>Install the dependencies:</p>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#dataset-exploration","title":"Dataset exploration\u00b6","text":"<p>Cohere has released a large Wikipedia dataset</p> <p>This dataset contains the wikimedia/wikipedia dataset dump from 2023-11-01 from Wikipedia in all 300+ languages. The embeddings are provided as int8 and ubinary that allow quick search and reduction of your vector index size up to 32.</p>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#aggregate-from-chunks-to-pages","title":"Aggregate from chunks to pages\u00b6","text":"<p>We want to aggregate the chunk &lt;&gt; vector representations into their natural retrievable unit - a Wikipedia page. We can still search the chunks and the chunk vector representation but retrieve pages instead of chunks. This avoids duplicating page-level metadata like url and title, while still being able to have meaningful semantic search representations. For RAG applications, this also means that we have the full page level context available when we retrieve information for the generative phase.</p>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>First, we define a Vespa schema with the fields we want to store and their type.</p> <p>We use Vespa's multi-vector indexing support - See Revolutionizing Semantic Search with Multi-Vector HNSW Indexing in Vespa for details. Highlights</p> <ul> <li>language for language-specific linguistic processing for keyword search</li> <li>Two named multi-vector representations with different precision and in-memory versus off-memory</li> <li>The named multi-vector representations holds the chunk-level embeddings</li> <li>Chunks is an array of string where we enable BM25</li> <li>Metadata for the page (url, title)</li> </ul>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#feed-the-wikipedia-pages-and-the-embedding-representations","title":"Feed the Wikipedia pages and the embedding representations\u00b6","text":"<p>Read more about feeding with pyvespa in PyVespa:reads and writes.</p> <p>In this case, we use a generator to yield document operations</p>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> <li>Practical Nearest Neighbor Search Guide</li> </ul> <p>To obtain the query embedding we use the Cohere embed API.</p>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#conclusions","title":"Conclusions\u00b6","text":"<p>These new Cohere binary embeddings are a huge step forward for cost-efficient vector search at scale and integrate perfectly with the rich feature set in Vespa. Including multilingual text search capabilities and hybrid search.</p>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#clean-up","title":"Clean up\u00b6","text":"<p>We can now delete the cloud instance:</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html","title":"pdf retrieval with ColQwen2 vlm Vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y\n</pre> !sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y <p>Now install the required python packages:</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install colpali-engine==0.3.1 pdf2image pypdf pyvespa vespacli requests numpy tqdm\n</pre> !pip3 install colpali-engine==0.3.1 pdf2image pypdf pyvespa vespacli requests numpy tqdm In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom io import BytesIO\nfrom colpali_engine.models import ColQwen2, ColQwen2Processor\n</pre> import torch from torch.utils.data import DataLoader from tqdm import tqdm from io import BytesIO from colpali_engine.models import ColQwen2, ColQwen2Processor In\u00a0[\u00a0]: Copied! <pre>model_name = \"vidore/colqwen2-v0.1\"\n\nmodel = ColQwen2.from_pretrained(\n    model_name, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\nprocessor = ColQwen2Processor.from_pretrained(model_name)\nmodel = model.eval()\n</pre> model_name = \"vidore/colqwen2-v0.1\"  model = ColQwen2.from_pretrained(     model_name, torch_dtype=torch.bfloat16, device_map=\"auto\" ) processor = ColQwen2Processor.from_pretrained(model_name) model = model.eval() In\u00a0[\u00a0]: Copied! <pre>import requests\nfrom pdf2image import convert_from_path\nfrom pypdf import PdfReader\n\n\ndef download_pdf(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return BytesIO(response.content)\n    else:\n        raise Exception(f\"Failed to download PDF: Status code {response.status_code}\")\n\n\ndef get_pdf_images(pdf_url):\n    # Download the PDF\n    pdf_file = download_pdf(pdf_url)\n    # Save the PDF temporarily to disk (pdf2image requires a file path)\n    temp_file = \"temp.pdf\"\n    with open(temp_file, \"wb\") as f:\n        f.write(pdf_file.read())\n    reader = PdfReader(temp_file)\n    page_texts = []\n    for page_number in range(len(reader.pages)):\n        page = reader.pages[page_number]\n        text = page.extract_text()\n        page_texts.append(text)\n    images = convert_from_path(temp_file)\n    assert len(images) == len(page_texts)\n    return (images, page_texts)\n</pre> import requests from pdf2image import convert_from_path from pypdf import PdfReader   def download_pdf(url):     response = requests.get(url)     if response.status_code == 200:         return BytesIO(response.content)     else:         raise Exception(f\"Failed to download PDF: Status code {response.status_code}\")   def get_pdf_images(pdf_url):     # Download the PDF     pdf_file = download_pdf(pdf_url)     # Save the PDF temporarily to disk (pdf2image requires a file path)     temp_file = \"temp.pdf\"     with open(temp_file, \"wb\") as f:         f.write(pdf_file.read())     reader = PdfReader(temp_file)     page_texts = []     for page_number in range(len(reader.pages)):         page = reader.pages[page_number]         text = page.extract_text()         page_texts.append(text)     images = convert_from_path(temp_file)     assert len(images) == len(page_texts)     return (images, page_texts) <p>We define a few sample PDFs to work with. The PDFs are discovered from this url.</p> In\u00a0[\u00a0]: Copied! <pre>sample_pdfs = [\n    {\n        \"title\": \"ConocoPhillips Sustainability Highlights - Nature (24-0976)\",\n        \"url\": \"https://static.conocophillips.com/files/resources/24-0976-sustainability-highlights_nature.pdf\",\n    },\n    {\n        \"title\": \"ConocoPhillips Managing Climate Related Risks\",\n        \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-managing-climate-related-risks.pdf\",\n    },\n    {\n        \"title\": \"ConocoPhillips 2023 Sustainability Report\",\n        \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-sustainability-report.pdf\",\n    },\n]\n</pre> sample_pdfs = [     {         \"title\": \"ConocoPhillips Sustainability Highlights - Nature (24-0976)\",         \"url\": \"https://static.conocophillips.com/files/resources/24-0976-sustainability-highlights_nature.pdf\",     },     {         \"title\": \"ConocoPhillips Managing Climate Related Risks\",         \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-managing-climate-related-risks.pdf\",     },     {         \"title\": \"ConocoPhillips 2023 Sustainability Report\",         \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-sustainability-report.pdf\",     }, ] <p>Now we can convert the PDFs to images and also extract the text content.</p> In\u00a0[\u00a0]: Copied! <pre>for pdf in sample_pdfs:\n    page_images, page_texts = get_pdf_images(pdf[\"url\"])\n    pdf[\"images\"] = page_images\n    pdf[\"texts\"] = page_texts\n</pre> for pdf in sample_pdfs:     page_images, page_texts = get_pdf_images(pdf[\"url\"])     pdf[\"images\"] = page_images     pdf[\"texts\"] = page_texts <p>Let us look at the extracted image of the first PDF page. This is the document side input to ColPali, one image per page.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display\n\n\ndef resize_image(image, max_height=800):\n    width, height = image.size\n    if height &gt; max_height:\n        ratio = max_height / height\n        new_width = int(width * ratio)\n        new_height = int(height * ratio)\n        return image.resize((new_width, new_height))\n    return image\n\n\ndisplay(resize_image(sample_pdfs[0][\"images\"][0]))\n</pre> from IPython.display import display   def resize_image(image, max_height=800):     width, height = image.size     if height &gt; max_height:         ratio = max_height / height         new_width = int(width * ratio)         new_height = int(height * ratio)         return image.resize((new_width, new_height))     return image   display(resize_image(sample_pdfs[0][\"images\"][0])) <p>Let us also look at the extracted text content of the first PDF page.</p> In\u00a0[\u00a0]: Copied! <pre>print(sample_pdfs[0][\"texts\"][0])\n</pre> print(sample_pdfs[0][\"texts\"][0]) <p>Notice how the layout and order of the text is different from the image representation. Note that</p> <ul> <li>The headlines NATURE and Sustainability have been combined into one word (NATURESustainability).</li> <li>The 0.03% has been converted to 0.03 and order is not preserved in the text representation.</li> <li>The data in the infographics is not represented in the text representation.</li> </ul> <p>Now we use the ColPali model to generate embeddings of the images.</p> In\u00a0[\u00a0]: Copied! <pre>for pdf in sample_pdfs:\n    page_embeddings = []\n    dataloader = DataLoader(\n        pdf[\"images\"],\n        batch_size=2,\n        shuffle=False,\n        collate_fn=lambda x: processor.process_images(x),\n    )\n\n    for batch_doc in tqdm(dataloader):\n        with torch.no_grad():\n            batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\n            embeddings_doc = model(**batch_doc)\n            page_embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n    pdf[\"embeddings\"] = page_embeddings\n</pre> for pdf in sample_pdfs:     page_embeddings = []     dataloader = DataLoader(         pdf[\"images\"],         batch_size=2,         shuffle=False,         collate_fn=lambda x: processor.process_images(x),     )      for batch_doc in tqdm(dataloader):         with torch.no_grad():             batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}             embeddings_doc = model(**batch_doc)             page_embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))     pdf[\"embeddings\"] = page_embeddings <p>Now we are done with the document side embeddings, we convert the embeddings to Vespa JSON format so we can store (and index) them in Vespa. Details in Vespa JSON feed format doc.</p> <p>We use binary quantization (BQ) of the page level ColPali vector embeddings to reduce their size by 32x.</p> <p>Read more about binarization of multi-vector representations in the colbert blog post.</p> <p>The binarization step maps 128 dimensional floats to 128 bits, or 16 bytes per vector.</p> <p>Reducing the size by 32x. On the DocVQA benchmark, binarization results in a small drop in ranking accuracy.</p> <p>We also demonstrate how to store the image data in Vespa using the raw type for binary data. To encode the binary data in JSON, we use base64 encoding.</p> In\u00a0[\u00a0]: Copied! <pre>import base64\n\n\ndef get_base64_image(image):\n    buffered = BytesIO()\n    image.save(buffered, format=\"JPEG\")\n    return str(base64.b64encode(buffered.getvalue()), \"utf-8\")\n</pre> import base64   def get_base64_image(image):     buffered = BytesIO()     image.save(buffered, format=\"JPEG\")     return str(base64.b64encode(buffered.getvalue()), \"utf-8\") In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nvespa_feed = []\nfor pdf in sample_pdfs:\n    url = pdf[\"url\"]\n    title = pdf[\"title\"]\n    for page_number, (page_text, embedding, image) in enumerate(\n        zip(pdf[\"texts\"], pdf[\"embeddings\"], pdf[\"images\"])\n    ):\n        base_64_image = get_base64_image(resize_image(image, 640))\n        embedding_dict = dict()\n        for idx, patch_embedding in enumerate(embedding):\n            binary_vector = (\n                np.packbits(np.where(patch_embedding &gt; 0, 1, 0))\n                .astype(np.int8)\n                .tobytes()\n                .hex()\n            )\n            embedding_dict[idx] = binary_vector\n        page = {\n            \"id\": hash(url + str(page_number)),\n            \"url\": url,\n            \"title\": title,\n            \"page_number\": page_number,\n            \"image\": base_64_image,\n            \"text\": page_text,\n            \"embedding\": embedding_dict,\n        }\n        vespa_feed.append(page)\n</pre> import numpy as np  vespa_feed = [] for pdf in sample_pdfs:     url = pdf[\"url\"]     title = pdf[\"title\"]     for page_number, (page_text, embedding, image) in enumerate(         zip(pdf[\"texts\"], pdf[\"embeddings\"], pdf[\"images\"])     ):         base_64_image = get_base64_image(resize_image(image, 640))         embedding_dict = dict()         for idx, patch_embedding in enumerate(embedding):             binary_vector = (                 np.packbits(np.where(patch_embedding &gt; 0, 1, 0))                 .astype(np.int8)                 .tobytes()                 .hex()             )             embedding_dict[idx] = binary_vector         page = {             \"id\": hash(url + str(page_number)),             \"url\": url,             \"title\": title,             \"page_number\": page_number,             \"image\": base_64_image,             \"text\": page_text,             \"embedding\": embedding_dict,         }         vespa_feed.append(page) In\u00a0[\u00a0]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet, HNSW\n\ncolpali_schema = Schema(\n    name=\"pdf_page\",\n    document=Document(\n        fields=[\n            Field(\n                name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"], match=[\"word\"]\n            ),\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(name=\"page_number\", type=\"int\", indexing=[\"summary\", \"attribute\"]),\n            Field(name=\"image\", type=\"raw\", indexing=[\"summary\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"index\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;int8&gt;(patch{}, v[16])\",\n                indexing=[\n                    \"attribute\",\n                    \"index\",\n                ],  # adds HNSW index for candidate retrieval.\n                ann=HNSW(\n                    distance_metric=\"hamming\",\n                    max_links_per_node=32,\n                    neighbors_to_explore_at_insert=400,\n                ),\n            ),\n        ]\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"text\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet, HNSW  colpali_schema = Schema(     name=\"pdf_page\",     document=Document(         fields=[             Field(                 name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"], match=[\"word\"]             ),             Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(name=\"page_number\", type=\"int\", indexing=[\"summary\", \"attribute\"]),             Field(name=\"image\", type=\"raw\", indexing=[\"summary\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"index\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"embedding\",                 type=\"tensor(patch{}, v[16])\",                 indexing=[                     \"attribute\",                     \"index\",                 ],  # adds HNSW index for candidate retrieval.                 ann=HNSW(                     distance_metric=\"hamming\",                     max_links_per_node=32,                     neighbors_to_explore_at_insert=400,                 ),             ),         ]     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"text\"])], ) <p>Notice the <code>embedding</code> field which is a tensor field with the type <code>tensor&lt;int8&gt;(patch{}, v[16])</code>. This is the field we use to represent the page level patch embeddings from ColPali.</p> <p>We also enable HNSW indexing for this field to enable fast nearest neighbor search which is used for candidate retrieval.</p> <p>We use binary hamming distance as an approximation of the cosine similarity. Hamming distance is a good approximation for binary representations, and it is much faster to compute than cosine similarity/dot product.</p> <p>The <code>embedding</code> field is an example of a mixed tensor where we combine one mapped (sparse) dimensions with a dense dimension.</p> <p>Read more in Tensor guide. We also enable BM25 for the <code>title</code> and <code>texts</code>\u00a0fields. Notice that the <code>image</code> field use type <code>raw</code> to store the binary image data, encoded with as a base64 string.</p> <p>Create the Vespa application package:</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"visionrag6\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name, schema=[colpali_schema]\n)\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"visionrag6\" vespa_application_package = ApplicationPackage(     name=vespa_app_name, schema=[colpali_schema] ) <p>Now we define how we want to rank the pages for a query. We use Vespa's support for BM25 for the text, and late interaction with Max Sim for the image embeddings.</p> <p>This means that we use the the text representations as a candidate retrieval phase, then we use the ColPALI embeddings with MaxSim to rerank the pages.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\ncolpali_profile = RankProfile(\n    name=\"default\",\n    inputs=[(\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\")],\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(embedding)) , v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(name=\"bm25_score\", expression=\"bm25(title) + bm25(text)\"),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"bm25_score\"),\n    second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=100),\n)\ncolpali_schema.add_rank_profile(colpali_profile)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  colpali_profile = RankProfile(     name=\"default\",     inputs=[(\"query(qt)\", \"tensor(querytoken{}, v[128])\")],     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(embedding)) , v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(name=\"bm25_score\", expression=\"bm25(title) + bm25(text)\"),     ],     first_phase=FirstPhaseRanking(expression=\"bm25_score\"),     second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=100), ) colpali_schema.add_rank_profile(colpali_profile) <p>The first phase uses a linear combination of BM25 scores for the text fields, and the second phase uses the MaxSim function with the image embeddings. Notice that Vespa supports a <code>unpack_bits</code> function to convert the 16 compressed binary vectors to 128-dimensional floats for the MaxSim function. The query input tensor is not compressed and using full float resolution.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() In\u00a0[\u00a0]: Copied! <pre>print(\"Number of PDF pages:\", len(vespa_feed))\n</pre> print(\"Number of PDF pages:\", len(vespa_feed)) <p>Index the documents in Vespa using the Vespa HTTP API.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaResponse\n\nasync with app.asyncio(connections=1, timeout=180) as session:\n    for page in tqdm(vespa_feed):\n        response: VespaResponse = await session.feed_data_point(\n            data_id=page[\"id\"], fields=page, schema=\"pdf_page\"\n        )\n        if not response.is_successful():\n            print(response.json())\n</pre> from vespa.io import VespaResponse  async with app.asyncio(connections=1, timeout=180) as session:     for page in tqdm(vespa_feed):         response: VespaResponse = await session.feed_data_point(             data_id=page[\"id\"], fields=page, schema=\"pdf_page\"         )         if not response.is_successful():             print(response.json()) <p>Now we can query Vespa with the text query and rerank the results using the ColPali embeddings.</p> In\u00a0[\u00a0]: Copied! <pre>queries = [\n    \"Percentage of non-fresh water as source?\",\n    \"Policies related to nature risk?\",\n    \"How much of produced water is recycled?\",\n]\n</pre> queries = [     \"Percentage of non-fresh water as source?\",     \"Policies related to nature risk?\",     \"How much of produced water is recycled?\", ] <p>Obtain the query embeddings using the ColPali model:</p> In\u00a0[\u00a0]: Copied! <pre>dataloader = DataLoader(\n    queries,\n    batch_size=1,\n    shuffle=False,\n    collate_fn=lambda x: processor.process_queries(x),\n)\nqs = []\nfor batch_query in dataloader:\n    with torch.no_grad():\n        batch_query = {k: v.to(model.device) for k, v in batch_query.items()}\n        embeddings_query = model(**batch_query)\n        qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n</pre> dataloader = DataLoader(     queries,     batch_size=1,     shuffle=False,     collate_fn=lambda x: processor.process_queries(x), ) qs = [] for batch_query in dataloader:     with torch.no_grad():         batch_query = {k: v.to(model.device) for k, v in batch_query.items()}         embeddings_query = model(**batch_query)         qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\")))) <p>We create a simple routine to display the results. We render the image and the title of the retrieved page/document.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display, HTML\n\n\ndef display_query_results(query, response, hits=5):\n    query_time = response.json.get(\"timing\", {}).get(\"searchtime\", -1)\n    query_time = round(query_time, 2)\n    count = response.json.get(\"root\", {}).get(\"fields\", {}).get(\"totalCount\", 0)\n    html_content = f\"&lt;h3&gt;Query text: '{query}', query time {query_time}s, count={count}, top results:&lt;/h3&gt;\"\n\n    for i, hit in enumerate(response.hits[:hits]):\n        title = hit[\"fields\"][\"title\"]\n        url = hit[\"fields\"][\"url\"]\n        page = hit[\"fields\"][\"page_number\"]\n        image = hit[\"fields\"][\"image\"]\n        score = hit[\"relevance\"]\n\n        html_content += f\"&lt;h4&gt;PDF Result {i + 1}&lt;/h4&gt;\"\n        html_content += f'&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; &lt;a href=\"{url}\"&gt;{title}&lt;/a&gt;, page {page+1} with score {score:.2f}&lt;/p&gt;'\n        html_content += (\n            f'&lt;img src=\"data:image/png;base64,{image}\" style=\"max-width:100%;\"&gt;'\n        )\n\n    display(HTML(html_content))\n</pre> from IPython.display import display, HTML   def display_query_results(query, response, hits=5):     query_time = response.json.get(\"timing\", {}).get(\"searchtime\", -1)     query_time = round(query_time, 2)     count = response.json.get(\"root\", {}).get(\"fields\", {}).get(\"totalCount\", 0)     html_content = f\"Query text: '{query}', query time {query_time}s, count={count}, top results:\"      for i, hit in enumerate(response.hits[:hits]):         title = hit[\"fields\"][\"title\"]         url = hit[\"fields\"][\"url\"]         page = hit[\"fields\"][\"page_number\"]         image = hit[\"fields\"][\"image\"]         score = hit[\"relevance\"]          html_content += f\"PDF Result {i + 1}\"         html_content += f'<p>Title: {title}, page {page+1} with score {score:.2f}</p>'         html_content += (             f''         )      display(HTML(html_content)) <p>Query Vespa with the queries and display the results, here we are using the <code>default</code> rank profile.</p> <p>Note that we retrieve using textual representation with <code>userInput(@userQuery)</code>, this means that we use the BM25 ranking for the extracted text in the first ranking phase and then re-rank the top-k pages using the ColPali embeddings.</p> <p>Later in this notebook we will use Vespa's support for approximate nearest neighbor search (<code>nearestNeighbor</code>) to retrieve directly using the ColPali embeddings.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\n\nasync with app.asyncio(connections=1, timeout=120) as session:\n    for idx, query in enumerate(queries):\n        query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}\n        response: VespaQueryResponse = await session.query(\n            yql=\"select title,url,image,page_number from pdf_page where userInput(@userQuery)\",\n            ranking=\"default\",\n            userQuery=query,\n            timeout=120,\n            hits=3,\n            body={\"input.query(qt)\": query_embedding, \"presentation.timing\": True},\n        )\n        assert response.is_successful()\n        display_query_results(query, response)\n</pre> from vespa.io import VespaQueryResponse  async with app.asyncio(connections=1, timeout=120) as session:     for idx, query in enumerate(queries):         query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}         response: VespaQueryResponse = await session.query(             yql=\"select title,url,image,page_number from pdf_page where userInput(@userQuery)\",             ranking=\"default\",             userQuery=query,             timeout=120,             hits=3,             body={\"input.query(qt)\": query_embedding, \"presentation.timing\": True},         )         assert response.is_successful()         display_query_results(query, response) In\u00a0[\u00a0]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\ninput_query_tensors = []\nMAX_QUERY_TERMS = 64\nfor i in range(MAX_QUERY_TERMS):\n    input_query_tensors.append((f\"query(rq{i})\", \"tensor&lt;int8&gt;(v[16])\"))\n\ninput_query_tensors.append((\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\"))\ninput_query_tensors.append((\"query(qtb)\", \"tensor&lt;int8&gt;(querytoken{}, v[16])\"))\n\ncolpali_retrieval_profile = RankProfile(\n    name=\"retrieval-and-rerank\",\n    inputs=input_query_tensors,\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(embedding)) , v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(\n            name=\"max_sim_binary\",\n            expression=\"\"\"\n                sum(\n                  reduce(\n                    1/(1 + sum(\n                        hamming(query(qtb), attribute(embedding)) ,v)\n                    ),\n                    max,\n                    patch\n                  ),\n                  querytoken\n                )\n            \"\"\",\n        ),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim_binary\"),\n    second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10),\n)\ncolpali_schema.add_rank_profile(colpali_retrieval_profile)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  input_query_tensors = [] MAX_QUERY_TERMS = 64 for i in range(MAX_QUERY_TERMS):     input_query_tensors.append((f\"query(rq{i})\", \"tensor(v[16])\"))  input_query_tensors.append((\"query(qt)\", \"tensor(querytoken{}, v[128])\")) input_query_tensors.append((\"query(qtb)\", \"tensor(querytoken{}, v[16])\"))  colpali_retrieval_profile = RankProfile(     name=\"retrieval-and-rerank\",     inputs=input_query_tensors,     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(embedding)) , v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(             name=\"max_sim_binary\",             expression=\"\"\"                 sum(                   reduce(                     1/(1 + sum(                         hamming(query(qtb), attribute(embedding)) ,v)                     ),                     max,                     patch                   ),                   querytoken                 )             \"\"\",         ),     ],     first_phase=FirstPhaseRanking(expression=\"max_sim_binary\"),     second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10), ) colpali_schema.add_rank_profile(colpali_retrieval_profile) <p>We define two functions, one for the first phase and one for the second phase. Instead of the float representations, we use the binary representations with inverted hamming distance in the first phase. Now, we need to re-deploy the application to Vespa Cloud.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <p>Now we can query Vespa with the text queries and use the <code>nearestNeighbor</code> operator to retrieve the most similar pages to the query and pass the different query tensors.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\n\ntarget_hits_per_query_tensor = (\n    20  # this is a hyper parameter that can be tuned for speed versus accuracy\n)\nasync with app.asyncio(connections=1, timeout=180) as session:\n    for idx, query in enumerate(queries):\n        float_query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}\n        binary_query_embeddings = dict()\n        for k, v in float_query_embedding.items():\n            binary_query_embeddings[k] = (\n                np.packbits(np.where(np.array(v) &gt; 0, 1, 0)).astype(np.int8).tolist()\n            )\n\n        # The mixed tensors used in MaxSim calculations\n        # We use both binary and float representations\n        query_tensors = {\n            \"input.query(qtb)\": binary_query_embeddings,\n            \"input.query(qt)\": float_query_embedding,\n        }\n        # The query tensors used in the nearest neighbor calculations\n        for i in range(0, len(binary_query_embeddings)):\n            query_tensors[f\"input.query(rq{i})\"] = binary_query_embeddings[i]\n        nn = []\n        for i in range(0, len(binary_query_embeddings)):\n            nn.append(\n                f\"({{targetHits:{target_hits_per_query_tensor}}}nearestNeighbor(embedding,rq{i}))\"\n            )\n        # We use a OR operator to combine the nearest neighbor operator\n        nn = \" OR \".join(nn)\n        response: VespaQueryResponse = await session.query(\n            yql=f\"select title, url, image, page_number from pdf_page where {nn}\",\n            ranking=\"retrieval-and-rerank\",\n            timeout=120,\n            hits=3,\n            body={**query_tensors, \"presentation.timing\": True},\n        )\n        assert response.is_successful()\n        display_query_results(query, response)\n</pre> from vespa.io import VespaQueryResponse  target_hits_per_query_tensor = (     20  # this is a hyper parameter that can be tuned for speed versus accuracy ) async with app.asyncio(connections=1, timeout=180) as session:     for idx, query in enumerate(queries):         float_query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}         binary_query_embeddings = dict()         for k, v in float_query_embedding.items():             binary_query_embeddings[k] = (                 np.packbits(np.where(np.array(v) &gt; 0, 1, 0)).astype(np.int8).tolist()             )          # The mixed tensors used in MaxSim calculations         # We use both binary and float representations         query_tensors = {             \"input.query(qtb)\": binary_query_embeddings,             \"input.query(qt)\": float_query_embedding,         }         # The query tensors used in the nearest neighbor calculations         for i in range(0, len(binary_query_embeddings)):             query_tensors[f\"input.query(rq{i})\"] = binary_query_embeddings[i]         nn = []         for i in range(0, len(binary_query_embeddings)):             nn.append(                 f\"({{targetHits:{target_hits_per_query_tensor}}}nearestNeighbor(embedding,rq{i}))\"             )         # We use a OR operator to combine the nearest neighbor operator         nn = \" OR \".join(nn)         response: VespaQueryResponse = await session.query(             yql=f\"select title, url, image, page_number from pdf_page where {nn}\",             ranking=\"retrieval-and-rerank\",             timeout=120,             hits=3,             body={**query_tensors, \"presentation.timing\": True},         )         assert response.is_successful()         display_query_results(query, response) <p>Depending on the scale, we can evaluate changing different number of targetHits per nearestNeighbor operator and the ranking depths in the two phases. We can also parallelize the ranking phases by using more threads per query request to reduce latency.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#pdf-retrieval-using-colqwen2-colpali-with-vespa","title":"PDF-Retrieval using ColQWen2 (ColPali) with Vespa\u00b6","text":"<p>This notebook is a continuation of our notebooks related to the ColPali models for complex document retrieval.</p> <p>This notebook demonstrates using the new ColQWen2 model checkpoint.</p> <p>ColQwen is a model based on a novel model architecture and training strategy based on Vision Language Models (VLMs) to efficiently index documents from their visual features. It is a Qwen2-VL-2B extension that generates ColBERT- style multi-vector representations of text and images. It was introduced in the paper ColPali: Efficient Document Retrieval with Vision Language Models and first released in this repository</p> <p>ColQWen2 is better than the previous ColPali model in the following ways:</p> <ul> <li>Its more accurate on the ViDoRe dataset (+5 nDCCG@5 points)</li> <li>It's permissive licensed as both the base model and adapter is using open-source licences (Apache 2.0 and MIT)</li> <li>It uses fewer patch embeddings than ColPaliGemma (from 1024 to 768), this reduces both compute and storage.</li> </ul> <p>See also Scaling ColPali to billions of PDFs with Vespa</p> <p>The TLDR; of this notebook:</p> <ul> <li>Generate an image per PDF page using pdf2image and also extract the text using pypdf.</li> <li>For each page image, use ColPali to obtain the visual multi-vector embeddings</li> </ul> <p>Then we store visual embeddings in Vespa as a <code>int8</code> tensor, where we use a binary compression technique to reduce the storage footprint by 32x compared to float representations. See Scaling ColPali to billions of PDFs with Vespa for details on binarization and using hamming distance for retrieval.</p> <p>During retrieval time, we use the same ColPali model to generate embeddings for the query and then use Vespa's <code>nearestNeighbor</code> query to retrieve the most similar documents per query vector token, using binary representation with hamming distance. Then we re-rank the results in two phases:</p> <ul> <li>In the 0-phase we use hamming distance to retrieve the k closest pages per query token vector representation, this is expressed by using multiple nearestNeighbor query operators in Vespa.</li> <li>The nearestNeighbor operators exposes pages to the first-phase ranking function, which uses an approximate MaxSim using inverted hamming distance insted of cosine similarity. This is done to reduce the number of pages that are re-ranked in the second phase.</li> <li>In the second phase, we perform the full MaxSim operation, using float representations of the embeddings to re-rank the top-k pages from the first phase.</li> </ul> <p>This allows us to scale ColPali to very large collections of PDF pages, while still providing accurate and fast retrieval.</p> <p>Let us get started.</p> <p></p> <p>Install dependencies:</p> <p>Note that the python pdf2image package requires poppler-utils, see other installation options here.</p> <p>For MacOs, the simplest install option is <code>brew install poppler</code> if you are using Homebrew.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#load-the-model","title":"Load the model\u00b6","text":"<p>We use device map auto to load the model on the available GPU if available, otherwise on the CPU or MPS if available.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#working-with-pdfs","title":"Working with pdfs\u00b6","text":"<p>We need to convert a PDF to an array of images. One image per page. We use the <code>pdf2image</code> library for this task. Secondary, we also extract the text contents of the PDF using <code>pypdf</code>.</p> <p>NOTE: This step requires that you have <code>poppler</code> installed on your system. Read more in pdf2image docs.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#configure-vespa","title":"Configure Vespa\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#querying-vespa","title":"Querying Vespa\u00b6","text":"<p>Ok, so now we have indexed the PDF pages in Vespa. Let us now obtain ColPali embeddings for a few text queries and use it during ranking of the indexed pdf pages.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#using-nearestneighbor-for-retrieval","title":"Using nearestNeighbor for retrieval\u00b6","text":"<p>In the above example, we used the ColPali embeddings in ranking, but using the text query for retrieval. This is a reasonable approach for text-heavy documents where the text representation is the most important and where ColPali embeddings are used to re-rank the top-k documents from the text retrieval phase.</p> <p>In some cases, the ColPali embeddings are the most important and we want to demonstrate how we can use HNSW indexing with binary hamming distance to retrieve the most similar pages to a query and then have two steps of re-ranking using the ColPali embeddings.</p> <p>All the phases here are executed locally inside the Vespa content node(s) so that no vector data needs to cross the network.</p> <p>Let us add a new rank-profile to the schema, the <code>nearestNeighbor</code> operator takes a query tensor and a field tensor as argument and we need to define the query tensors types in the rank-profile.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#summary","title":"Summary\u00b6","text":"<p>In this notebook, we have demonstrated how to represent the new ColQwen2 in Vespa. We have generated embeddings for images of PDF pages using ColQwen2 and stored the embeddings in Vespa using mixed tensors.</p> <p>We demonstrated how to store the base64 encoded image using the <code>raw</code> Vespa field type, plus meta data like title and url. We have demonstrated how to retrieve relevant pages for a query using the embeddings generated by ColPali.</p> <p>This notebook can be extended to include more complex ranking models, more complex queries, and more complex data structures, including metadata and other fields which can be filtered on or used for ranking.</p>"},{"location":"examples/pyvespa-examples.html","title":"Pyvespa examples","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Refer to troubleshooting, which also has utilies for debugging.</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install pyvespa\n</pre> !pip3 install pyvespa In\u00a0[14]: Copied! <pre>from vespa.package import ApplicationPackage, Field, RankProfile\nfrom vespa.deployment import VespaDocker\nfrom vespa.io import VespaResponse\n\napp_package = ApplicationPackage(name=\"neighbors\")\n\napp_package.schema.add_fields(\n    Field(name=\"point\", type=\"tensor&lt;float&gt;(d[3])\", indexing=[\"attribute\", \"summary\"])\n)\n\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"max_distance\",\n        inputs=[(\"query(qpoint)\", \"tensor&lt;float&gt;(d[3])\")],\n        first_phase=\"euclidean_distance(attribute(point), query(qpoint), d)\",\n    )\n)\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=app_package)\n</pre> from vespa.package import ApplicationPackage, Field, RankProfile from vespa.deployment import VespaDocker from vespa.io import VespaResponse  app_package = ApplicationPackage(name=\"neighbors\")  app_package.schema.add_fields(     Field(name=\"point\", type=\"tensor(d[3])\", indexing=[\"attribute\", \"summary\"]) )  app_package.schema.add_rank_profile(     RankProfile(         name=\"max_distance\",         inputs=[(\"query(qpoint)\", \"tensor(d[3])\")],         first_phase=\"euclidean_distance(attribute(point), query(qpoint), d)\",     ) )  vespa_docker = VespaDocker() app = vespa_docker.deploy(application_package=app_package) <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 15/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 20/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 25/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p>Feed points in 3d space using a 3-dimensional indexed tensor. Pyvespa feeds using the /document/v1/ API, refer to document format:</p> In\u00a0[15]: Copied! <pre>def get_feed(field_name):\n    return [\n        {\"id\": 0, \"fields\": {field_name: [0.0, 1.0, 2.0]}},\n        {\"id\": 1, \"fields\": {field_name: [1.0, 2.0, 3.0]}},\n        {\"id\": 2, \"fields\": {field_name: [2.0, 3.0, 4.0]}},\n    ]\n\n\nwith app.syncio(connections=1) as session:\n    for u in get_feed(\"point\"):\n        response: VespaResponse = session.update_data(\n            data_id=u[\"id\"], schema=\"neighbors\", fields=u[\"fields\"], create=True\n        )\n        if not response.is_successful():\n            print(\n                \"Update failed for document {}\".format(u[\"id\"])\n                + \" with status code {}\".format(response.status_code)\n                + \" with response {}\".format(response.get_json())\n            )\n</pre> def get_feed(field_name):     return [         {\"id\": 0, \"fields\": {field_name: [0.0, 1.0, 2.0]}},         {\"id\": 1, \"fields\": {field_name: [1.0, 2.0, 3.0]}},         {\"id\": 2, \"fields\": {field_name: [2.0, 3.0, 4.0]}},     ]   with app.syncio(connections=1) as session:     for u in get_feed(\"point\"):         response: VespaResponse = session.update_data(             data_id=u[\"id\"], schema=\"neighbors\", fields=u[\"fields\"], create=True         )         if not response.is_successful():             print(                 \"Update failed for document {}\".format(u[\"id\"])                 + \" with status code {}\".format(response.status_code)                 + \" with response {}\".format(response.get_json())             ) <p>Note:  The feed above uses create-if-nonexistent, i.e. update a document, create it if it does not exists. Later in this notebook we will add a field and update it, so using an update to feed data makes it easier.</p> <p>Query from origo using YQL. The rank profile will rank the most distant points highest, here <code>sqrt(2*2 + 3*3 + 4*4) = 5.385</code>:</p> In\u00a0[16]: Copied! <pre>import json\nfrom vespa.io import VespaQueryResponse\n\nresult: VespaQueryResponse = app.query(\n    body={\n        \"yql\": \"select point from neighbors where true\",\n        \"input.query(qpoint)\": \"[0.0, 0.0, 0.0]\",\n        \"ranking.profile\": \"max_distance\",\n        \"presentation.format.tensors\": \"short-value\",\n    }\n)\n\nif not response.is_successful():\n    print(\n        \"Query failed with status code {}\".format(response.status_code)\n        + \" with response {}\".format(response.get_json())\n    )\n    raise Exception(\"Query failed\")\nif len(result.hits) != 3:\n    raise Exception(\"Expected 3 hits, got {}\".format(len(result.hits)))\nprint(json.dumps(result.hits, indent=4))\n</pre> import json from vespa.io import VespaQueryResponse  result: VespaQueryResponse = app.query(     body={         \"yql\": \"select point from neighbors where true\",         \"input.query(qpoint)\": \"[0.0, 0.0, 0.0]\",         \"ranking.profile\": \"max_distance\",         \"presentation.format.tensors\": \"short-value\",     } )  if not response.is_successful():     print(         \"Query failed with status code {}\".format(response.status_code)         + \" with response {}\".format(response.get_json())     )     raise Exception(\"Query failed\") if len(result.hits) != 3:     raise Exception(\"Expected 3 hits, got {}\".format(len(result.hits))) print(json.dumps(result.hits, indent=4)) <pre>[\n    {\n        \"id\": \"index:neighbors_content/0/c81e728dfde15fa4e8dfb3d3\",\n        \"relevance\": 5.385164807134504,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                2.0,\n                3.0,\n                4.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/c4ca4238db266f395150e961\",\n        \"relevance\": 3.7416573867739413,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                1.0,\n                2.0,\n                3.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/cfcd20845b10b1420c6cdeca\",\n        \"relevance\": 2.23606797749979,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                0.0,\n                1.0,\n                2.0\n            ]\n        }\n    }\n]\n</pre> <p>Query from <code>[1.0, 2.0, 2.9]</code> - find that <code>[2.0, 3.0, 4.0]</code> is most distant:</p> In\u00a0[17]: Copied! <pre>result = app.query(\n    body={\n        \"yql\": \"select point from neighbors where true\",\n        \"input.query(qpoint)\": \"[1.0, 2.0, 2.9]\",\n        \"ranking.profile\": \"max_distance\",\n        \"presentation.format.tensors\": \"short-value\",\n    }\n)\nprint(json.dumps(result.hits, indent=4))\n</pre> result = app.query(     body={         \"yql\": \"select point from neighbors where true\",         \"input.query(qpoint)\": \"[1.0, 2.0, 2.9]\",         \"ranking.profile\": \"max_distance\",         \"presentation.format.tensors\": \"short-value\",     } ) print(json.dumps(result.hits, indent=4)) <pre>[\n    {\n        \"id\": \"index:neighbors_content/0/c81e728dfde15fa4e8dfb3d3\",\n        \"relevance\": 1.7916472308265357,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                2.0,\n                3.0,\n                4.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/cfcd20845b10b1420c6cdeca\",\n        \"relevance\": 1.6763055154708881,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                0.0,\n                1.0,\n                2.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/c4ca4238db266f395150e961\",\n        \"relevance\": 0.09999990575011103,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                1.0,\n                2.0,\n                3.0\n            ]\n        }\n    }\n]\n</pre> In\u00a0[18]: Copied! <pre>app_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"nearest_neighbor\",\n        inputs=[(\"query(qpoint)\", \"tensor&lt;float&gt;(d[3])\")],\n        first_phase=\"closeness(field, point)\",\n    )\n)\n\napp = vespa_docker.deploy(application_package=app_package)\n</pre> app_package.schema.add_rank_profile(     RankProfile(         name=\"nearest_neighbor\",         inputs=[(\"query(qpoint)\", \"tensor(d[3])\")],         first_phase=\"closeness(field, point)\",     ) )  app = vespa_docker.deploy(application_package=app_package) <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p>Read more in nearest neighbor search.</p> <p>Query using nearestNeighbor query operator:</p> In\u00a0[19]: Copied! <pre>result = app.query(\n    body={\n        \"yql\": \"select point from neighbors where {targetHits: 3}nearestNeighbor(point, qpoint)\",\n        \"input.query(qpoint)\": \"[1.0, 2.0, 2.9]\",\n        \"ranking.profile\": \"nearest_neighbor\",\n        \"presentation.format.tensors\": \"short-value\",\n    }\n)\nprint(json.dumps(result.hits, indent=4))\n</pre> result = app.query(     body={         \"yql\": \"select point from neighbors where {targetHits: 3}nearestNeighbor(point, qpoint)\",         \"input.query(qpoint)\": \"[1.0, 2.0, 2.9]\",         \"ranking.profile\": \"nearest_neighbor\",         \"presentation.format.tensors\": \"short-value\",     } ) print(json.dumps(result.hits, indent=4)) <pre>[\n    {\n        \"id\": \"index:neighbors_content/0/c4ca4238db266f395150e961\",\n        \"relevance\": 0.9090909879069752,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                1.0,\n                2.0,\n                3.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/cfcd20845b10b1420c6cdeca\",\n        \"relevance\": 0.37364941905256455,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                0.0,\n                1.0,\n                2.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/c81e728dfde15fa4e8dfb3d3\",\n        \"relevance\": 0.35821144946644456,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                2.0,\n                3.0,\n                4.0\n            ]\n        }\n    }\n]\n</pre> In\u00a0[20]: Copied! <pre>app_package.schema.add_fields(\n    Field(\n        name=\"point_angular\",\n        type=\"tensor&lt;float&gt;(d[3])\",\n        indexing=[\"attribute\", \"summary\"],\n        attribute=[\"distance-metric: angular\"],\n    )\n)\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"nearest_neighbor_angular\",\n        inputs=[(\"query(qpoint)\", \"tensor&lt;float&gt;(d[3])\")],\n        first_phase=\"closeness(field, point_angular)\",\n    )\n)\n\napp = vespa_docker.deploy(application_package=app_package)\n</pre> app_package.schema.add_fields(     Field(         name=\"point_angular\",         type=\"tensor(d[3])\",         indexing=[\"attribute\", \"summary\"],         attribute=[\"distance-metric: angular\"],     ) ) app_package.schema.add_rank_profile(     RankProfile(         name=\"nearest_neighbor_angular\",         inputs=[(\"query(qpoint)\", \"tensor(d[3])\")],         first_phase=\"closeness(field, point_angular)\",     ) )  app = vespa_docker.deploy(application_package=app_package) <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p>Feed the same data to the <code>point_angular</code> field:</p> In\u00a0[21]: Copied! <pre>for u in get_feed(\"point_angular\"):\n    response: VespaResponse = session.update_data(\n        data_id=u[\"id\"], schema=\"neighbors\", fields=u[\"fields\"]\n    )\n    if not response.is_successful():\n        print(\n            \"Update failed for document {}\".format(u[\"id\"])\n            + \" with status code {}\".format(response.status_code)\n            + \" with response {}\".format(response.get_json())\n        )\n</pre> for u in get_feed(\"point_angular\"):     response: VespaResponse = session.update_data(         data_id=u[\"id\"], schema=\"neighbors\", fields=u[\"fields\"]     )     if not response.is_successful():         print(             \"Update failed for document {}\".format(u[\"id\"])             + \" with status code {}\".format(response.status_code)             + \" with response {}\".format(response.get_json())         ) <p>Observe the documents now have two vectors</p> <p>Notice that we pass native Vespa document v1 api parameters to reduce the tensor verbosity.</p> In\u00a0[24]: Copied! <pre>from vespa.io import VespaResponse\n\nresponse: VespaResponse = app.get_data(\n    schema=\"neighbors\", data_id=0, **{\"format.tensors\": \"short-value\"}\n)\nprint(json.dumps(response.get_json(), indent=4))\n</pre> from vespa.io import VespaResponse  response: VespaResponse = app.get_data(     schema=\"neighbors\", data_id=0, **{\"format.tensors\": \"short-value\"} ) print(json.dumps(response.get_json(), indent=4)) <pre>{\n    \"pathId\": \"/document/v1/neighbors/neighbors/docid/0\",\n    \"id\": \"id:neighbors:neighbors::0\",\n    \"fields\": {\n        \"point\": [\n            0.0,\n            1.0,\n            2.0\n        ],\n        \"point_angular\": [\n            0.0,\n            1.0,\n            2.0\n        ]\n    }\n}\n</pre> In\u00a0[25]: Copied! <pre>result = app.query(\n    body={\n        \"yql\": \"select point_angular from neighbors where {targetHits: 3}nearestNeighbor(point_angular, qpoint)\",\n        \"input.query(qpoint)\": \"[1.0, 2.0, 2.9]\",\n        \"ranking.profile\": \"nearest_neighbor_angular\",\n        \"presentation.format.tensors\": \"short-value\",\n    }\n)\nprint(json.dumps(result.hits, indent=4))\n</pre> result = app.query(     body={         \"yql\": \"select point_angular from neighbors where {targetHits: 3}nearestNeighbor(point_angular, qpoint)\",         \"input.query(qpoint)\": \"[1.0, 2.0, 2.9]\",         \"ranking.profile\": \"nearest_neighbor_angular\",         \"presentation.format.tensors\": \"short-value\",     } ) print(json.dumps(result.hits, indent=4)) <pre>[\n    {\n        \"id\": \"index:neighbors_content/0/c4ca4238db266f395150e961\",\n        \"relevance\": 0.983943389010042,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point_angular\": [\n                1.0,\n                2.0,\n                3.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/c81e728dfde15fa4e8dfb3d3\",\n        \"relevance\": 0.9004871017951954,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point_angular\": [\n                2.0,\n                3.0,\n                4.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/cfcd20845b10b1420c6cdeca\",\n        \"relevance\": 0.7638041096953281,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point_angular\": [\n                0.0,\n                1.0,\n                2.0\n            ]\n        }\n    }\n]\n</pre> <p>In the output above, observe the different in \"relevance\", compared to the query using <code>'ranking.profile': 'nearest_neighbor'</code> above - this is the difference in <code>closeness()</code> using different distance metrics.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"examples/pyvespa-examples.html#pyvespa-examples","title":"Pyvespa examples\u00b6","text":"<p>This is a notebook with short examples one can build applications from.</p>"},{"location":"examples/pyvespa-examples.html#neighbors","title":"Neighbors\u00b6","text":"<p>Explore distance between points in 3D vector space.</p> <p>These are simple examples, feeding documents with a tensor representing a point in space, and a rank profile calculating the distance between a point in the query and the point in the documents.</p> <p>The examples start with using simple ranking expressions like euclidean-distance, then rank features like closeness()) and setting different distance-metrics.</p>"},{"location":"examples/pyvespa-examples.html#distant-neighbor","title":"Distant neighbor\u00b6","text":"<p>First, find the point that is  most  distant from a point in query - deploy the Application Package:</p>"},{"location":"examples/pyvespa-examples.html#nearest-neighbor","title":"Nearest neighbor\u00b6","text":"<p>The nearestNeighbor query operator calculates distances between points in vector space. Here, we are using the default distance metric (euclidean), as it is not specified. The closeness()) rank feature can be used to rank results - add a new rank profile:</p>"},{"location":"examples/pyvespa-examples.html#nearest-neighbor-angular","title":"Nearest neighbor - angular\u00b6","text":"<p>So far, we have used the default distance-metric which is euclidean - now try with another. Add new few field with \"angular\" distance metric:</p>"},{"location":"examples/pyvespa-examples.html#next-steps","title":"Next steps\u00b6","text":"<ul> <li>Try the multi-vector-indexing notebook to explore using an HNSW-index for approximate nearest neighbor search.</li> <li>Explore using the distance()) rank feature - this should give the same results as the ranking expressions using <code>euclidean-distance</code> above.</li> <li><code>label</code> is useful when having more vector fields - read more about the nearestNeighbor query operator.</li> </ul>"},{"location":"examples/pyvespa-examples.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html","title":"Scaling personal ai assistants with streaming mode cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa llama-index vespacli\n</pre> !pip3 install -U pyvespa llama-index vespacli In\u00a0[2]: Copied! <pre>from typing import List\n\n\ndef synthetic_mail_data_generator() -&gt; List[dict]:\n    synthetic_mails = [\n        {\n            \"id\": 1,\n            \"groupname\": \"bergum@vespa.ai\",\n            \"fields\": {\n                \"subject\": \"LlamaIndex news, 2023-11-14\",\n                \"to\": \"bergum@vespa.ai\",\n                \"body\": \"\"\"Hello Llama Friends \ud83e\udd99 LlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we're taking a stroll down memory \n                    lane on our blog with twelve milestones from our first year. Be sure to check it out.\"\"\",\n                \"from\": \"news@llamaindex.ai\",\n                \"display_date\": \"2023-11-15T09:00:00Z\",\n            },\n        },\n        {\n            \"id\": 2,\n            \"groupname\": \"bergum@vespa.ai\",\n            \"fields\": {\n                \"subject\": \"Dentist Appointment Reminder\",\n                \"to\": \"bergum@vespa.ai\",\n                \"body\": \"Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist\",\n                \"from\": \"dentist@dentist.no\",\n                \"display_date\": \"2023-11-15T15:30:00Z\",\n            },\n        },\n        {\n            \"id\": 1,\n            \"groupname\": \"giraffe@wildlife.ai\",\n            \"fields\": {\n                \"subject\": \"Wildlife Update: Giraffe Edition\",\n                \"to\": \"giraffe@wildlife.ai\",\n                \"body\": \"Dear Wildlife Enthusiasts \ud83e\udd92, We're thrilled to share the latest insights into giraffe behavior in the wild. Join us on an adventure as we explore their natural habitat and learn more about these majestic creatures.\",\n                \"from\": \"updates@wildlife.ai\",\n                \"display_date\": \"2023-11-12T14:30:00Z\",\n            },\n        },\n        {\n            \"id\": 1,\n            \"groupname\": \"penguin@antarctica.ai\",\n            \"fields\": {\n                \"subject\": \"Antarctica Expedition: Penguin Chronicles\",\n                \"to\": \"penguin@antarctica.ai\",\n                \"body\": \"Greetings Explorers \ud83d\udc27, Our team is embarking on an exciting expedition to Antarctica to study penguin colonies. Stay tuned for live updates and behind-the-scenes footage as we dive into the world of these fascinating birds.\",\n                \"from\": \"expedition@antarctica.ai\",\n                \"display_date\": \"2023-11-11T11:45:00Z\",\n            },\n        },\n        {\n            \"id\": 1,\n            \"groupname\": \"space@exploration.ai\",\n            \"fields\": {\n                \"subject\": \"Space Exploration News: November Edition\",\n                \"to\": \"space@exploration.ai\",\n                \"body\": \"Hello Space Enthusiasts \ud83d\ude80, Join us as we highlight the latest discoveries and breakthroughs in space exploration. From distant galaxies to new technologies, there's a lot to explore!\",\n                \"from\": \"news@exploration.ai\",\n                \"display_date\": \"2023-11-01T16:20:00Z\",\n            },\n        },\n        {\n            \"id\": 1,\n            \"groupname\": \"ocean@discovery.ai\",\n            \"fields\": {\n                \"subject\": \"Ocean Discovery: Hidden Treasures Unveiled\",\n                \"to\": \"ocean@discovery.ai\",\n                \"body\": \"Dear Ocean Explorers \ud83c\udf0a, Dive deep into the secrets of the ocean with our latest discoveries. From undiscovered species to underwater landscapes, our team is uncovering the wonders of the deep blue.\",\n                \"from\": \"discovery@ocean.ai\",\n                \"display_date\": \"2023-10-01T10:15:00Z\",\n            },\n        },\n    ]\n    for mail in synthetic_mails:\n        yield mail\n</pre> from typing import List   def synthetic_mail_data_generator() -&gt; List[dict]:     synthetic_mails = [         {             \"id\": 1,             \"groupname\": \"bergum@vespa.ai\",             \"fields\": {                 \"subject\": \"LlamaIndex news, 2023-11-14\",                 \"to\": \"bergum@vespa.ai\",                 \"body\": \"\"\"Hello Llama Friends \ud83e\udd99 LlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we're taking a stroll down memory                      lane on our blog with twelve milestones from our first year. Be sure to check it out.\"\"\",                 \"from\": \"news@llamaindex.ai\",                 \"display_date\": \"2023-11-15T09:00:00Z\",             },         },         {             \"id\": 2,             \"groupname\": \"bergum@vespa.ai\",             \"fields\": {                 \"subject\": \"Dentist Appointment Reminder\",                 \"to\": \"bergum@vespa.ai\",                 \"body\": \"Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist\",                 \"from\": \"dentist@dentist.no\",                 \"display_date\": \"2023-11-15T15:30:00Z\",             },         },         {             \"id\": 1,             \"groupname\": \"giraffe@wildlife.ai\",             \"fields\": {                 \"subject\": \"Wildlife Update: Giraffe Edition\",                 \"to\": \"giraffe@wildlife.ai\",                 \"body\": \"Dear Wildlife Enthusiasts \ud83e\udd92, We're thrilled to share the latest insights into giraffe behavior in the wild. Join us on an adventure as we explore their natural habitat and learn more about these majestic creatures.\",                 \"from\": \"updates@wildlife.ai\",                 \"display_date\": \"2023-11-12T14:30:00Z\",             },         },         {             \"id\": 1,             \"groupname\": \"penguin@antarctica.ai\",             \"fields\": {                 \"subject\": \"Antarctica Expedition: Penguin Chronicles\",                 \"to\": \"penguin@antarctica.ai\",                 \"body\": \"Greetings Explorers \ud83d\udc27, Our team is embarking on an exciting expedition to Antarctica to study penguin colonies. Stay tuned for live updates and behind-the-scenes footage as we dive into the world of these fascinating birds.\",                 \"from\": \"expedition@antarctica.ai\",                 \"display_date\": \"2023-11-11T11:45:00Z\",             },         },         {             \"id\": 1,             \"groupname\": \"space@exploration.ai\",             \"fields\": {                 \"subject\": \"Space Exploration News: November Edition\",                 \"to\": \"space@exploration.ai\",                 \"body\": \"Hello Space Enthusiasts \ud83d\ude80, Join us as we highlight the latest discoveries and breakthroughs in space exploration. From distant galaxies to new technologies, there's a lot to explore!\",                 \"from\": \"news@exploration.ai\",                 \"display_date\": \"2023-11-01T16:20:00Z\",             },         },         {             \"id\": 1,             \"groupname\": \"ocean@discovery.ai\",             \"fields\": {                 \"subject\": \"Ocean Discovery: Hidden Treasures Unveiled\",                 \"to\": \"ocean@discovery.ai\",                 \"body\": \"Dear Ocean Explorers \ud83c\udf0a, Dive deep into the secrets of the ocean with our latest discoveries. From undiscovered species to underwater landscapes, our team is uncovering the wonders of the deep blue.\",                 \"from\": \"discovery@ocean.ai\",                 \"display_date\": \"2023-10-01T10:15:00Z\",             },         },     ]     for mail in synthetic_mails:         yield mail In\u00a0[3]: Copied! <pre>from typing import List\n\n\ndef synthetic_calendar_data_generator() -&gt; List[dict]:\n    calendar_data = [\n        {\n            \"id\": 1,\n            \"groupname\": \"bergum@vespa.ai\",\n            \"fields\": {\n                \"subject\": \"Dentist Appointment\",\n                \"to\": \"bergum@vespa.ai\",\n                \"body\": \"Dentist appointment at 2023-12-04 at 09:30 - 1 hour duration\",\n                \"from\": \"dentist@dentist.no\",\n                \"display_date\": \"2023-11-15T15:30:00Z\",\n                \"duration\": 60,\n            },\n        },\n        {\n            \"id\": 2,\n            \"groupname\": \"bergum@vespa.ai\",\n            \"fields\": {\n                \"subject\": \"Public Cloud Platform Events\",\n                \"to\": \"bergum@vespa.ai\",\n                \"body\": \"The cloud team continues to push new features and improvements to the platform. Join us for a live demo of the latest updates\",\n                \"from\": \"public-cloud-platform-events\",\n                \"display_date\": \"2023-11-21T09:30:00Z\",\n                \"duration\": 60,\n            },\n        },\n    ]\n    for event in calendar_data:\n        yield event\n</pre> from typing import List   def synthetic_calendar_data_generator() -&gt; List[dict]:     calendar_data = [         {             \"id\": 1,             \"groupname\": \"bergum@vespa.ai\",             \"fields\": {                 \"subject\": \"Dentist Appointment\",                 \"to\": \"bergum@vespa.ai\",                 \"body\": \"Dentist appointment at 2023-12-04 at 09:30 - 1 hour duration\",                 \"from\": \"dentist@dentist.no\",                 \"display_date\": \"2023-11-15T15:30:00Z\",                 \"duration\": 60,             },         },         {             \"id\": 2,             \"groupname\": \"bergum@vespa.ai\",             \"fields\": {                 \"subject\": \"Public Cloud Platform Events\",                 \"to\": \"bergum@vespa.ai\",                 \"body\": \"The cloud team continues to push new features and improvements to the platform. Join us for a live demo of the latest updates\",                 \"from\": \"public-cloud-platform-events\",                 \"display_date\": \"2023-11-21T09:30:00Z\",                 \"duration\": 60,             },         },     ]     for event in calendar_data:         yield event In\u00a0[4]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet, HNSW\n\nmail_schema = Schema(\n    name=\"mail\",\n    mode=\"streaming\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"subject\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"to\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"from\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"body\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"display_date\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"timestamp\",\n                type=\"long\",\n                indexing=[\n                    \"input display_date\",\n                    \"to_epoch_second\",\n                    \"summary\",\n                    \"attribute\",\n                ],\n                is_document_field=False,\n            ),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;bfloat16&gt;(x[384])\",\n                indexing=[\n                    'input subject .\" \". input body',\n                    \"embed e5\",\n                    \"attribute\",\n                    \"index\",\n                ],\n                ann=HNSW(distance_metric=\"angular\"),\n                is_document_field=False,\n            ),\n        ],\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"subject\", \"body\", \"to\", \"from\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet, HNSW  mail_schema = Schema(     name=\"mail\",     mode=\"streaming\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"subject\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"to\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"from\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"body\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"display_date\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"timestamp\",                 type=\"long\",                 indexing=[                     \"input display_date\",                     \"to_epoch_second\",                     \"summary\",                     \"attribute\",                 ],                 is_document_field=False,             ),             Field(                 name=\"embedding\",                 type=\"tensor(x[384])\",                 indexing=[                     'input subject .\" \". input body',                     \"embed e5\",                     \"attribute\",                     \"index\",                 ],                 ann=HNSW(distance_metric=\"angular\"),                 is_document_field=False,             ),         ],     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"subject\", \"body\", \"to\", \"from\"])], ) <p>In the <code>mail</code> schema, we have six document fields; these are provided by us when we feed documents of type <code>mail</code> to this app. The fieldset defines which fields are matched against when we do not mention explicit field names when querying. We can add as many fieldsets as we like without duplicating content.</p> <p>In addition to the fields within the <code>document</code>, there are two synthetic fields in the schema, <code>timestamp</code> and <code>embedding</code>, using Vespa indexing expressions taking inputs from the document and performing conversions.</p> <ul> <li>the <code>timestamp</code> field takes the input <code>display_date</code> and uses the to_epoch_second converter to convert the display date into an epoch timestamp. This is useful because we can calculate the document's age and use the <code>freshness(timestamp)</code> rank feature during ranking phases.</li> <li>the <code>embedding</code> tensor field takes the subject and body as input and feeds that into an embed function that uses an embedding model to map the string input into an embedding vector representation using 384 dimensions with <code>bfloat16</code> precision. Vectors in Vespa are represented as Tensors.</li> </ul> In\u00a0[5]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet, HNSW\n\ncalendar_schema = Schema(\n    name=\"calendar\",\n    inherits=\"mail\",\n    mode=\"streaming\",\n    document=Document(\n        inherits=\"mail\",\n        fields=[\n            Field(name=\"duration\", type=\"int\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"guests\", type=\"array&lt;string&gt;\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"location\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"address\", type=\"string\", indexing=[\"summary\", \"index\"]),\n        ],\n    ),\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet, HNSW  calendar_schema = Schema(     name=\"calendar\",     inherits=\"mail\",     mode=\"streaming\",     document=Document(         inherits=\"mail\",         fields=[             Field(name=\"duration\", type=\"int\", indexing=[\"summary\", \"index\"]),             Field(name=\"guests\", type=\"array\", indexing=[\"summary\", \"index\"]),             Field(name=\"location\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"address\", type=\"string\", indexing=[\"summary\", \"index\"]),         ],     ), ) <p>The observant reader might have noticed the <code>e5</code> argument to the <code>embed</code> expression in the above <code>embedding</code> field. The <code>e5</code> argument references a component of the type hugging-face-embedder. We configure the application package and its name with the <code>mail</code> schema and the <code>e5</code> embedder component.</p> In\u00a0[6]: Copied! <pre>from vespa.package import ApplicationPackage, Component, Parameter\n\nvespa_app_name = \"assistant\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name,\n    schema=[mail_schema, calendar_schema],\n    components=[\n        Component(\n            id=\"e5\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    name=\"transformer-model\",\n                    args={\n                        \"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\"\n                    },\n                ),\n                Parameter(\n                    name=\"tokenizer-model\",\n                    args={\n                        \"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\"\n                    },\n                ),\n                Parameter(\n                    name=\"prepend\",\n                    args={},\n                    children=[\n                        Parameter(name=\"query\", args={}, children=\"query: \"),\n                        Parameter(name=\"document\", args={}, children=\"passage: \"),\n                    ],\n                ),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import ApplicationPackage, Component, Parameter  vespa_app_name = \"assistant\" vespa_application_package = ApplicationPackage(     name=vespa_app_name,     schema=[mail_schema, calendar_schema],     components=[         Component(             id=\"e5\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(                     name=\"transformer-model\",                     args={                         \"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\"                     },                 ),                 Parameter(                     name=\"tokenizer-model\",                     args={                         \"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\"                     },                 ),                 Parameter(                     name=\"prepend\",                     args={},                     children=[                         Parameter(name=\"query\", args={}, children=\"query: \"),                         Parameter(name=\"document\", args={}, children=\"passage: \"),                     ],                 ),             ],         )     ], ) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the mail schema.</p> <p>Vespa supports phased ranking and has a rich set of built-in rank-features.</p> <p>Users can also define custom functions with ranking expressions.</p> In\u00a0[7]: Copied! <pre>from vespa.package import RankProfile, Function, GlobalPhaseRanking, FirstPhaseRanking\n\nkeywords_and_freshness = RankProfile(\n    name=\"default\",\n    functions=[\n        Function(\n            name=\"my_function\",\n            expression=\"nativeRank(subject) + nativeRank(body) + freshness(timestamp)\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(expression=\"my_function\", rank_score_drop_limit=0.02),\n    match_features=[\n        \"nativeRank(subject)\",\n        \"nativeRank(body)\",\n        \"my_function\",\n        \"freshness(timestamp)\",\n    ],\n)\n\nsemantic = RankProfile(\n    name=\"semantic\",\n    functions=[\n        Function(name=\"cosine\", expression=\"max(0,cos(distance(field, embedding)))\")\n    ],\n    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\"), (\"query(threshold)\", \"\", \"0.75\")],\n    first_phase=FirstPhaseRanking(\n        expression=\"if(cosine &gt; query(threshold), cosine, -1)\",\n        rank_score_drop_limit=0.1,\n    ),\n    match_features=[\n        \"cosine\",\n        \"freshness(timestamp)\",\n        \"distance(field, embedding)\",\n        \"query(threshold)\",\n    ],\n)\n\nfusion = RankProfile(\n    name=\"fusion\",\n    inherits=\"semantic\",\n    functions=[\n        Function(\n            name=\"keywords_and_freshness\",\n            expression=\" nativeRank(subject) + nativeRank(body) + freshness(timestamp)\",\n        ),\n        Function(name=\"semantic\", expression=\"cos(distance(field,embedding))\"),\n    ],\n    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\"), (\"query(threshold)\", \"\", \"0.75\")],\n    first_phase=FirstPhaseRanking(\n        expression=\"if(cosine &gt; query(threshold), cosine, -1)\",\n        rank_score_drop_limit=0.1,\n    ),\n    match_features=[\n        \"nativeRank(subject)\",\n        \"keywords_and_freshness\",\n        \"freshness(timestamp)\",\n        \"cosine\",\n        \"query(threshold)\",\n    ],\n    global_phase=GlobalPhaseRanking(\n        rerank_count=1000,\n        expression=\"reciprocal_rank_fusion(semantic, keywords_and_freshness)\",\n    ),\n)\n</pre> from vespa.package import RankProfile, Function, GlobalPhaseRanking, FirstPhaseRanking  keywords_and_freshness = RankProfile(     name=\"default\",     functions=[         Function(             name=\"my_function\",             expression=\"nativeRank(subject) + nativeRank(body) + freshness(timestamp)\",         )     ],     first_phase=FirstPhaseRanking(expression=\"my_function\", rank_score_drop_limit=0.02),     match_features=[         \"nativeRank(subject)\",         \"nativeRank(body)\",         \"my_function\",         \"freshness(timestamp)\",     ], )  semantic = RankProfile(     name=\"semantic\",     functions=[         Function(name=\"cosine\", expression=\"max(0,cos(distance(field, embedding)))\")     ],     inputs=[(\"query(q)\", \"tensor(x[384])\"), (\"query(threshold)\", \"\", \"0.75\")],     first_phase=FirstPhaseRanking(         expression=\"if(cosine &gt; query(threshold), cosine, -1)\",         rank_score_drop_limit=0.1,     ),     match_features=[         \"cosine\",         \"freshness(timestamp)\",         \"distance(field, embedding)\",         \"query(threshold)\",     ], )  fusion = RankProfile(     name=\"fusion\",     inherits=\"semantic\",     functions=[         Function(             name=\"keywords_and_freshness\",             expression=\" nativeRank(subject) + nativeRank(body) + freshness(timestamp)\",         ),         Function(name=\"semantic\", expression=\"cos(distance(field,embedding))\"),     ],     inputs=[(\"query(q)\", \"tensor(x[384])\"), (\"query(threshold)\", \"\", \"0.75\")],     first_phase=FirstPhaseRanking(         expression=\"if(cosine &gt; query(threshold), cosine, -1)\",         rank_score_drop_limit=0.1,     ),     match_features=[         \"nativeRank(subject)\",         \"keywords_and_freshness\",         \"freshness(timestamp)\",         \"cosine\",         \"query(threshold)\",     ],     global_phase=GlobalPhaseRanking(         rerank_count=1000,         expression=\"reciprocal_rank_fusion(semantic, keywords_and_freshness)\",     ), ) <p>The <code>default</code> rank profile defines a custom function <code>my_function</code> that computes a linear combination of three different features:</p> <ul> <li><code>nativeRank(subject)</code> Is a text matching feature , scoped to the <code>subject</code> field.</li> <li><code>nativeRank(body)</code> Same, but scoped to the <code>body</code> field.</li> <li><code>freshness(timestamp)</code> This is a built-in rank-feature that returns a number that is close to 1 if the timestamp is recent compared to the current query time.</li> </ul> In\u00a0[8]: Copied! <pre>mail_schema.add_rank_profile(keywords_and_freshness)\nmail_schema.add_rank_profile(semantic)\nmail_schema.add_rank_profile(fusion)\ncalendar_schema.add_rank_profile(keywords_and_freshness)\ncalendar_schema.add_rank_profile(semantic)\ncalendar_schema.add_rank_profile(fusion)\n</pre> mail_schema.add_rank_profile(keywords_and_freshness) mail_schema.add_rank_profile(semantic) mail_schema.add_rank_profile(fusion) calendar_schema.add_rank_profile(keywords_and_freshness) calendar_schema.add_rank_profile(semantic) calendar_schema.add_rank_profile(fusion) <p>Finally, we have our basic Vespa schema and application package.</p> <p>We can serialize the representation to application package files. This is handy when we want to start working with production deployments and when we want to manage the application with version control.</p> In\u00a0[9]: Copied! <pre>import os\n\napplication_directory = \"my-assistant-vespa-app\"\nvespa_application_package.to_files(application_directory)\n\n\ndef print_files_in_directory(directory):\n    for root, _, files in os.walk(directory):\n        for file in files:\n            print(os.path.join(root, file))\n\n\nprint_files_in_directory(application_directory)\n</pre> import os  application_directory = \"my-assistant-vespa-app\" vespa_application_package.to_files(application_directory)   def print_files_in_directory(directory):     for root, _, files in os.walk(directory):         for file in files:             print(os.path.join(root, file))   print_files_in_directory(application_directory) <pre>my-assistant-vespa-app/services.xml\nmy-assistant-vespa-app/schemas/mail.sd\nmy-assistant-vespa-app/schemas/calendar.sd\nmy-assistant-vespa-app/search/query-profiles/default.xml\nmy-assistant-vespa-app/search/query-profiles/types/root.xml\n</pre> In\u00a0[15]: Copied! <pre>from vespa.deployment import VespaCloud\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone. The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy(disk_folder=application_directory)\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy(disk_folder=application_directory) In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Error {response.url} : {response.get_json()}\")\n    else:\n        print(f\"Success {response.url}\")\n\n\napp.feed_iterable(\n    synthetic_mail_data_generator(),\n    schema=\"mail\",\n    namespace=\"assistant\",\n    callback=callback,\n)\napp.feed_iterable(\n    synthetic_calendar_data_generator(),\n    schema=\"calendar\",\n    namespace=\"assistant\",\n    callback=callback,\n)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Error {response.url} : {response.get_json()}\")     else:         print(f\"Success {response.url}\")   app.feed_iterable(     synthetic_mail_data_generator(),     schema=\"mail\",     namespace=\"assistant\",     callback=callback, ) app.feed_iterable(     synthetic_calendar_data_generator(),     schema=\"calendar\",     namespace=\"assistant\",     callback=callback, ) In\u00a0[18]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select subject, display_date, to from sources mail where userQuery()\",\n    query=\"when is my dentist appointment\",\n    groupname=\"bergum@vespa.ai\",\n    ranking=\"default\",\n    timeout=\"2s\",\n)\nassert response.is_successful()\nprint(json.dumps(response.hits[0], indent=2))\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select subject, display_date, to from sources mail where userQuery()\",     query=\"when is my dentist appointment\",     groupname=\"bergum@vespa.ai\",     ranking=\"default\",     timeout=\"2s\", ) assert response.is_successful() print(json.dumps(response.hits[0], indent=2)) <pre>{\n  \"id\": \"id:assistant:mail:g=bergum@vespa.ai:2\",\n  \"relevance\": 1.134783932836458,\n  \"source\": \"assistant_content.mail\",\n  \"fields\": {\n    \"matchfeatures\": {\n      \"freshness(timestamp)\": 0.9232458847736625,\n      \"nativeRank(body)\": 0.09246780326887034,\n      \"nativeRank(subject)\": 0.11907024479392506,\n      \"my_function\": 1.134783932836458\n    },\n    \"subject\": \"Dentist Appointment Reminder\",\n    \"to\": \"bergum@vespa.ai\",\n    \"display_date\": \"2023-11-15T15:30:00Z\"\n  }\n}\n</pre> <p>For the above query request, Vespa searched the <code>default</code> fieldset which we defined in the schema to match against several fields including the body and the subject. The <code>default</code> rank-profile calculated the relevance score as the sum of three rank-features: <code>nativeRank(body)</code> + <code>nativeRank(subject)</code> + <code>freshness(</code>timestamp)<code>, and the result of this computation is the </code>relevance<code>score of the hit. In addition, we also asked for Vespa to return</code>matchfeatures<code>that are handy for debugging the final</code>relevance` score or for feature logging.</p> <p>Now, we can try the <code>semantic</code> ranking profile, using Vespa's support for nearestNeighbor search. This also exemplifies using the configured <code>e5</code> embedder to embed the user query into an embedding representation. See embedding a query text for more usage examples of using Vespa embedders.</p> In\u00a0[19]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select subject, display_date from mail where {targetHits:10}nearestNeighbor(embedding,q)\",\n    groupname=\"bergum@vespa.ai\",\n    ranking=\"semantic\",\n    body={\n        \"input.query(q)\": 'embed(e5, \"when is my dentist appointment\")',\n    },\n    timeout=\"2s\",\n)\nassert response.is_successful()\nprint(json.dumps(response.hits[0], indent=2))\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select subject, display_date from mail where {targetHits:10}nearestNeighbor(embedding,q)\",     groupname=\"bergum@vespa.ai\",     ranking=\"semantic\",     body={         \"input.query(q)\": 'embed(e5, \"when is my dentist appointment\")',     },     timeout=\"2s\", ) assert response.is_successful() print(json.dumps(response.hits[0], indent=2)) <pre>{\n  \"id\": \"id:assistant:mail:g=bergum@vespa.ai:2\",\n  \"relevance\": 0.9079386507883569,\n  \"source\": \"assistant_content.mail\",\n  \"fields\": {\n    \"matchfeatures\": {\n      \"distance(field,embedding)\": 0.4324572498488368,\n      \"freshness(timestamp)\": 0.9232457561728395,\n      \"query(threshold)\": 0.75,\n      \"cosine\": 0.9079386507883569\n    },\n    \"subject\": \"Dentist Appointment Reminder\",\n    \"display_date\": \"2023-11-15T15:30:00Z\"\n  }\n}\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.legacy.core.base_retriever import BaseRetriever\nfrom llama_index.legacy.schema import NodeWithScore, QueryBundle, TextNode\nfrom llama_index.legacy.callbacks.base import CallbackManager\n\nfrom vespa.application import Vespa\nfrom vespa.io import VespaQueryResponse\n\nfrom typing import List, Union, Optional\n\n\nclass PersonalAssistantVespaRetriever(BaseRetriever):\n    def __init__(\n        self,\n        app: Vespa,\n        user: str,\n        hits: int = 5,\n        vespa_rank_profile: str = \"default\",\n        vespa_score_cutoff: float = 0.70,\n        sources: List[str] = [\"mail\"],\n        fields: List[str] = [\"subject\", \"body\"],\n        callback_manager: Optional[CallbackManager] = None,\n    ) -&gt; None:\n        \"\"\"Sample Retriever for a personal assistant application.\n        Args:\n        param: app: Vespa application object\n        param: user: user id to retrieve documents for (used for Vespa streaming groupname)\n        param: hits: number of hits to retrieve from Vespa app\n        param: vespa_rank_profile: Vespa rank profile to use\n        param: vespa_score_cutoff: Vespa score cutoff to use during first-phase ranking\n        param: sources: sources to retrieve documents from\n        param: fields: fields to retrieve\n        \"\"\"\n\n        self.app = app\n        self.hits = hits\n        self.user = user\n        self.vespa_rank_profile = vespa_rank_profile\n        self.vespa_score_cutoff = vespa_score_cutoff\n        self.fields = fields\n        self.summary_fields = \",\".join(fields)\n        self.sources = \",\".join(sources)\n        super().__init__(callback_manager)\n\n    def _retrieve(self, query: Union[str, QueryBundle]) -&gt; List[NodeWithScore]:\n        \"\"\"Retrieve documents from Vespa application.\"\"\"\n        if isinstance(query, QueryBundle):\n            query = query.query_str\n\n        if self.vespa_rank_profile == \"default\":\n            yql: str = f\"select {self.summary_fields} from mail where userQuery()\"\n        else:\n            yql = f\"select {self.summary_fields} from sources {self.sources} where {{targetHits:10}}nearestNeighbor(embedding,q) or userQuery()\"\n        vespa_body_request = {\n            \"yql\": yql,\n            \"query\": query,\n            \"hits\": self.hits,\n            \"ranking.profile\": self.vespa_rank_profile,\n            \"timeout\": \"2s\",\n            \"input.query(threshold)\": self.vespa_score_cutoff,\n        }\n        if self.vespa_rank_profile != \"default\":\n            vespa_body_request[\"input.query(q)\"] = f'embed(e5, \"{query}\")'\n\n        with self.app.syncio(connections=1) as session:\n            response: VespaQueryResponse = session.query(\n                body=vespa_body_request, groupname=self.user\n            )\n            if not response.is_successful():\n                raise ValueError(\n                    f\"Query request failed: {response.status_code}, response payload: {response.get_json()}\"\n                )\n\n        nodes: List[NodeWithScore] = []\n        for hit in response.hits:\n            response_fields: dict = hit.get(\"fields\", {})\n            text: str = \"\"\n            for field in response_fields.keys():\n                if isinstance(response_fields[field], str) and field in self.fields:\n                    text += response_fields[field] + \" \"\n            id = hit[\"id\"]\n            #\n            doc = TextNode(\n                id_=id,\n                text=text,\n                metadata=response_fields,\n            )\n            nodes.append(NodeWithScore(node=doc, score=hit[\"relevance\"]))\n        return nodes\n</pre> from llama_index.legacy.core.base_retriever import BaseRetriever from llama_index.legacy.schema import NodeWithScore, QueryBundle, TextNode from llama_index.legacy.callbacks.base import CallbackManager  from vespa.application import Vespa from vespa.io import VespaQueryResponse  from typing import List, Union, Optional   class PersonalAssistantVespaRetriever(BaseRetriever):     def __init__(         self,         app: Vespa,         user: str,         hits: int = 5,         vespa_rank_profile: str = \"default\",         vespa_score_cutoff: float = 0.70,         sources: List[str] = [\"mail\"],         fields: List[str] = [\"subject\", \"body\"],         callback_manager: Optional[CallbackManager] = None,     ) -&gt; None:         \"\"\"Sample Retriever for a personal assistant application.         Args:         param: app: Vespa application object         param: user: user id to retrieve documents for (used for Vespa streaming groupname)         param: hits: number of hits to retrieve from Vespa app         param: vespa_rank_profile: Vespa rank profile to use         param: vespa_score_cutoff: Vespa score cutoff to use during first-phase ranking         param: sources: sources to retrieve documents from         param: fields: fields to retrieve         \"\"\"          self.app = app         self.hits = hits         self.user = user         self.vespa_rank_profile = vespa_rank_profile         self.vespa_score_cutoff = vespa_score_cutoff         self.fields = fields         self.summary_fields = \",\".join(fields)         self.sources = \",\".join(sources)         super().__init__(callback_manager)      def _retrieve(self, query: Union[str, QueryBundle]) -&gt; List[NodeWithScore]:         \"\"\"Retrieve documents from Vespa application.\"\"\"         if isinstance(query, QueryBundle):             query = query.query_str          if self.vespa_rank_profile == \"default\":             yql: str = f\"select {self.summary_fields} from mail where userQuery()\"         else:             yql = f\"select {self.summary_fields} from sources {self.sources} where {{targetHits:10}}nearestNeighbor(embedding,q) or userQuery()\"         vespa_body_request = {             \"yql\": yql,             \"query\": query,             \"hits\": self.hits,             \"ranking.profile\": self.vespa_rank_profile,             \"timeout\": \"2s\",             \"input.query(threshold)\": self.vespa_score_cutoff,         }         if self.vespa_rank_profile != \"default\":             vespa_body_request[\"input.query(q)\"] = f'embed(e5, \"{query}\")'          with self.app.syncio(connections=1) as session:             response: VespaQueryResponse = session.query(                 body=vespa_body_request, groupname=self.user             )             if not response.is_successful():                 raise ValueError(                     f\"Query request failed: {response.status_code}, response payload: {response.get_json()}\"                 )          nodes: List[NodeWithScore] = []         for hit in response.hits:             response_fields: dict = hit.get(\"fields\", {})             text: str = \"\"             for field in response_fields.keys():                 if isinstance(response_fields[field], str) and field in self.fields:                     text += response_fields[field] + \" \"             id = hit[\"id\"]             #             doc = TextNode(                 id_=id,                 text=text,                 metadata=response_fields,             )             nodes.append(NodeWithScore(node=doc, score=hit[\"relevance\"]))         return nodes <p>The above defines a <code>PersonalAssistantVespaRetriever</code> which accepts most importantly a pyvespa <code>Vespa</code> application instance.</p> <p>The YQL specifies a hybrid retrieval query that retrieves both using embedding-based retrieval (vector search) using Vespa's nearest neighbor search operator in combination with traditional keyword matching.</p> <p>With the above, we can connect to the running Vespa app and initialize the <code>PersonalAssistantVespaRetriever</code> for the user <code>bergum@vespa.ai</code>. The <code>user</code> argument maps to the streaming search groupname parameter.</p> In\u00a0[21]: Copied! <pre>retriever = PersonalAssistantVespaRetriever(\n    app=app, user=\"bergum@vespa.ai\", vespa_rank_profile=\"default\"\n)\nretriever.retrieve(\"When is my dentist appointment?\")\n</pre> retriever = PersonalAssistantVespaRetriever(     app=app, user=\"bergum@vespa.ai\", vespa_rank_profile=\"default\" ) retriever.retrieve(\"When is my dentist appointment?\") Out[21]: <pre>[NodeWithScore(node=TextNode(id_='id:assistant:mail:g=bergum@vespa.ai:2', embedding=None, metadata={'matchfeatures': {'freshness(timestamp)': 0.9232454989711935, 'nativeRank(body)': 0.09246780326887034, 'nativeRank(subject)': 0.11907024479392506, 'my_function': 1.1347835470339889}, 'subject': 'Dentist Appointment Reminder', 'body': 'Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='269fe208f8d43a967dc683e1c9b832b18ddfb0b2efd801ab7e428620c8163021', text='Dentist Appointment Reminder Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1.1347835470339889),\n NodeWithScore(node=TextNode(id_='id:assistant:mail:g=bergum@vespa.ai:1', embedding=None, metadata={'matchfeatures': {'freshness(timestamp)': 0.9202362397119341, 'nativeRank(body)': 0.02919821398130037, 'nativeRank(subject)': 1.3512214436142505e-38, 'my_function': 0.9494344536932345}, 'subject': 'LlamaIndex news, 2023-11-14', 'body': \"Hello Llama Friends \ud83e\udd99 LlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we're taking a stroll down memory \\n                    lane on our blog with twelve milestones from our first year. Be sure to check it out.\"}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='5e975eaece761d46956c9d301138f29b5c067d3da32fd013bb79c6ee9c033d3d', text=\"LlamaIndex news, 2023-11-14 Hello Llama Friends \ud83e\udd99 LlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we're taking a stroll down memory \\n                    lane on our blog with twelve milestones from our first year. Be sure to check it out. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.9494344536932345)]</pre> <p>These <code>NodeWithScore</code> retrieved <code>default</code> rank-profile can then be used for the next steps in a generative chain.</p> <p>We can also try the <code>semantic</code> rank-profile, which has rank-score-drop functionality, allowing us to have a per-query time threshold. Altering the threshold will remove context.</p> In\u00a0[22]: Copied! <pre>retriever = PersonalAssistantVespaRetriever(\n    app=app,\n    user=\"bergum@vespa.ai\",\n    vespa_rank_profile=\"semantic\",\n    vespa_score_cutoff=0.6,\n    hits=20,\n)\nretriever.retrieve(\"When is my dentist appointment?\")\n</pre> retriever = PersonalAssistantVespaRetriever(     app=app,     user=\"bergum@vespa.ai\",     vespa_rank_profile=\"semantic\",     vespa_score_cutoff=0.6,     hits=20, ) retriever.retrieve(\"When is my dentist appointment?\") Out[22]: <pre>[NodeWithScore(node=TextNode(id_='id:assistant:mail:g=bergum@vespa.ai:2', embedding=None, metadata={'matchfeatures': {'distance(field,embedding)': 0.43945494361938975, 'freshness(timestamp)': 0.9232453703703704, 'query(threshold)': 0.6, 'cosine': 0.9049836898369259}, 'subject': 'Dentist Appointment Reminder', 'body': 'Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='e89f669e6c9cf64ab6a856d9857915481396e2aa84154951327cd889c23f7c4f', text='Dentist Appointment Reminder Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.9049836898369259),\n NodeWithScore(node=TextNode(id_='id:assistant:mail:g=bergum@vespa.ai:1', embedding=None, metadata={'matchfeatures': {'distance(field,embedding)': 0.69930099954744, 'freshness(timestamp)': 0.9202361111111111, 'query(threshold)': 0.6, 'cosine': 0.7652923088511814}, 'subject': 'LlamaIndex news, 2023-11-14', 'body': \"Hello Llama Friends \ud83e\udd99 LlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we're taking a stroll down memory \\n                    lane on our blog with twelve milestones from our first year. Be sure to check it out.\"}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='cb9b588e5b53dbdd0fbe6f7aadfa689d84a5bea23239293bd299347ee9ecd853', text=\"LlamaIndex news, 2023-11-14 Hello Llama Friends \ud83e\udd99 LlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we're taking a stroll down memory \\n                    lane on our blog with twelve milestones from our first year. Be sure to check it out. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7652923088511814)]</pre> <p>Create a new retriever with sources including both mail and calendar data:</p> In\u00a0[23]: Copied! <pre>retriever = PersonalAssistantVespaRetriever(\n    app=app,\n    user=\"bergum@vespa.ai\",\n    vespa_rank_profile=\"fusion\",\n    sources=[\"calendar\", \"mail\"],\n    vespa_score_cutoff=0.80,\n)\nretriever.retrieve(\"When is my dentist appointment?\")\n</pre> retriever = PersonalAssistantVespaRetriever(     app=app,     user=\"bergum@vespa.ai\",     vespa_rank_profile=\"fusion\",     sources=[\"calendar\", \"mail\"],     vespa_score_cutoff=0.80, ) retriever.retrieve(\"When is my dentist appointment?\") Out[23]: <pre>[NodeWithScore(node=TextNode(id_='id:assistant:calendar:g=bergum@vespa.ai:1', embedding=None, metadata={'matchfeatures': {'freshness(timestamp)': 0.9232447273662552, 'nativeRank(subject)': 0.11907024479392506, 'query(threshold)': 0.8, 'cosine': 0.8872983644178517, 'keywords_and_freshness': 1.1606592237923947}, 'subject': 'Dentist Appointment', 'body': 'Dentist appointment at 2023-12-04 at 09:30 - 1 hour duration'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='b30948011cbe9bbf29135384efbc72f85a6eb65113be0eb9762315a022f11ba1', text='Dentist Appointment Dentist appointment at 2023-12-04 at 09:30 - 1 hour duration ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.03278688524590164),\n NodeWithScore(node=TextNode(id_='id:assistant:mail:g=bergum@vespa.ai:2', embedding=None, metadata={'matchfeatures': {'freshness(timestamp)': 0.9232447273662552, 'nativeRank(subject)': 0.11907024479392506, 'query(threshold)': 0.8, 'cosine': 0.9049836898369259, 'keywords_and_freshness': 1.1347827754290507}, 'subject': 'Dentist Appointment Reminder', 'body': 'Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='21c501ccdc6e4b33d388eefa244c5039a0e1ed4b81e4f038916765e22be24705', text='Dentist Appointment Reminder Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.03278688524590164)]</pre> In\u00a0[24]: Copied! <pre>app.query(\n    yql=\"select subject, display_date from calendar where duration &gt; 0\",\n    ranking=\"default\",\n    groupname=\"bergum@vespa.ai\",\n    timeout=\"2s\",\n).json\n</pre> app.query(     yql=\"select subject, display_date from calendar where duration &gt; 0\",     ranking=\"default\",     groupname=\"bergum@vespa.ai\",     timeout=\"2s\", ).json Out[24]: <pre>{'root': {'id': 'toplevel',\n  'relevance': 1.0,\n  'fields': {'totalCount': 2},\n  'coverage': {'coverage': 100,\n   'documents': 2,\n   'full': True,\n   'nodes': 1,\n   'results': 1,\n   'resultsFull': 1},\n  'children': [{'id': 'id:assistant:calendar:g=bergum@vespa.ai:2',\n    'relevance': 0.987133487654321,\n    'source': 'assistant_content.calendar',\n    'fields': {'matchfeatures': {'freshness(timestamp)': 0.987133487654321,\n      'nativeRank(body)': 0.0,\n      'nativeRank(subject)': 0.0,\n      'my_function': 0.987133487654321},\n     'subject': 'Public Cloud Platform Events',\n     'display_date': '2023-11-21T09:30:00Z'}},\n   {'id': 'id:assistant:calendar:g=bergum@vespa.ai:1',\n    'relevance': 0.9232445987654321,\n    'source': 'assistant_content.calendar',\n    'fields': {'matchfeatures': {'freshness(timestamp)': 0.9232445987654321,\n      'nativeRank(body)': 0.0,\n      'nativeRank(subject)': 0.0,\n      'my_function': 0.9232445987654321},\n     'subject': 'Dentist Appointment',\n     'display_date': '2023-11-15T15:30:00Z'}}]}}</pre> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#building-cost-efficient-retrieval-augmented-personal-ai-assistants","title":"Building cost-efficient retrieval-augmented personal AI assistants\u00b6","text":"<p>This notebook demonstrates how to use Vespa streaming mode for cost-efficient retrieval for applications that store and retrieve personal data. You can read more about Vespa vector streaming search in these two blog posts:</p> <ul> <li>Announcing vector streaming search: AI assistants at scale without breaking the bank</li> <li>Yahoo Mail turns to Vespa to do RAG at scale</li> </ul>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#a-summary-of-vespa-streaming-mode","title":"A summary of Vespa streaming mode\u00b6","text":"<p>Vespa\u2019s streaming search solution lets you make the user id a part of the document ID so that Vespa can use it to co-locate the data of each user on a small set of nodes and the same chunk of disk. This allows you to do searches over a user\u2019s data with low latency without keeping any user\u2019s data in memory or paying the cost of managing indexes.</p> <ul> <li>There is no accuracy drop for vector search as it uses exact vector search</li> <li>Several orders of magnitude higher throughput (No expensive index builds to support approximate search)</li> <li>Documents (including vector data) are disk-based.</li> <li>Ultra-low memory requirements (fixed per document)</li> </ul> <p>This notebook connects a custom LlamaIndex Retriever with a Vespa app using streaming mode to retrieve personal data. The focus is on how to use the streaming mode feature.</p> <p></p> <p>First, install dependencies:</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#synthetic-mail-calendar-data","title":"Synthetic Mail &amp; Calendar Data\u00b6","text":"<p>There are few public email datasets because people care about their privacy, so this notebook uses synthetic data to examine how to use Vespa streaming mode. We create two generator functions that returns Python <code>dict</code>s with synthetic mail and calendar data.</p> <p>Notice that the dict has three keys:</p> <ul> <li><code>id</code></li> <li><code>groupname</code></li> <li><code>fields</code></li> </ul> <p>This is the expected feed format for PyVespa feed operations and where PyVespa will use these to build a Vespa document v1 API request(s). The <code>groupname</code> key is only to be used when using streaming mode.</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#defining-a-vespa-application","title":"Defining a Vespa application\u00b6","text":"<p>PyVespa help us build the Vespa application package. A Vespa application package consists of configuration files.</p> <p>First, we define a Vespa schema. PyVespa offers a programmatic API for creating the schema. In the end, it is serialized to a file (<code>&lt;schema&gt;.sd</code>) before it can be deployed to Vespa.</p> <p>Vespa is statically typed, so we need to define the fields and their type in the schema before we can start feeding documents. Note that we set <code>mode</code> to <code>streaming</code> which enables Vespa streaming mode for this schema. Other valid modes are <code>indexed</code> and <code>store-only</code>.</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#feeding-data-to-vespa","title":"Feeding data to Vespa\u00b6","text":"<p>With the app up and running in Vespa Cloud, we can start feeding and querying our data.</p> <p>We use the feed_iterable API of pyvespa with a custom <code>callback</code> that prints the URL and an error if the operation fails.</p> <p>We pass the <code>synthetic_*generator()</code> and call <code>feed_iterable</code> with the specific <code>schema</code> and <code>namespace</code>.</p> <p>Read more about Vespa document IDs.</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Now, we can also query our data. With streaming mode, we must pass the <code>groupname</code> parameter, or the request will fail with an error.</p> <p>The query request uses the Vespa Query API and the <code>Vespa.query()</code> function supports passing any of the Vespa query API parameters.</p> <p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> </ul> <p>Sample query request for <code>when is my dentist appointment</code> for the user <code>bergum@vespa.ai</code>:</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#llamaindex-retrievers-introduction","title":"LlamaIndex Retrievers Introduction\u00b6","text":"<p>Now, we have a basic Vespa app using streaming mode. We likely want to use an LLM framework like\u00a0LangChain or LLamaIndex to build an end-to-end assistant. In this example notebook, we use LLamaIndex retrievers.</p> <p>LlamaIndex retriever abstraction allows developers to add custom retrievers that retrieve information in Retrieval Augmented Generation (RAG) pipelines.</p> <p>For an excellent introduction to LLamaIndex and its concepts, see LLamaIndex High-Level Concepts.</p> <p>To create a custom LlamaIndex retrieve, we implement a class that inherits from <code>llama_index.retrievers.BaseRetriever.BaseRetriever</code> and which implements <code>_retrieve(query)</code>.</p> <p>A simple <code>PersonalAssistantVespaRetriever</code> could look like the following:</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook, we have demonstrated:</p> <ul> <li>Configuring and using Vespa's streaming mode</li> <li>Using multiple document types and schema to organize our data</li> <li>Running embedding inference in Vespa</li> <li>Hybrid retrieval techniques - combined with score thresholding to filter irrelevant contexts</li> <li>Creating a custom LLamaIndex retriever and connecting it with our Vespa app</li> <li>Vespa Cloud deployments to sandbox/dev zone</li> </ul> <p>We can now delete the cloud instance:</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html","title":"simplified retrieval with colpali vlm Vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y\n</pre> !sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y <p>Now install the required python packages:</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install colpali-engine==0.3.1 vidore_benchmark==4.0.0 pdf2image pypdf==5.0.1 pyvespa vespacli requests numpy\n</pre> !pip3 install colpali-engine==0.3.1 vidore_benchmark==4.0.0 pdf2image pypdf==5.0.1 pyvespa vespacli requests numpy In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom io import BytesIO\n\nfrom colpali_engine.models import ColPali, ColPaliProcessor\nfrom colpali_engine.utils.torch_utils import get_torch_device\nfrom vidore_benchmark.utils.image_utils import scale_image, get_base64_image\n</pre> import torch from torch.utils.data import DataLoader from tqdm import tqdm from io import BytesIO  from colpali_engine.models import ColPali, ColPaliProcessor from colpali_engine.utils.torch_utils import get_torch_device from vidore_benchmark.utils.image_utils import scale_image, get_base64_image <p>Choose the right device to run the model.</p> In\u00a0[\u00a0]: Copied! <pre>device = get_torch_device()\nif device == \"cuda\":\n    dtype = torch.bfloat16\nelse:\n    dtype = torch.float32\n</pre> device = get_torch_device() if device == \"cuda\":     dtype = torch.bfloat16 else:     dtype = torch.float32 <p>Load the model and the processor.</p> In\u00a0[\u00a0]: Copied! <pre>model_name = \"vidore/colpali-v1.2\"\nmodel = ColPali.from_pretrained(model_name, torch_dtype=dtype, device_map=device).eval()\nprocessor = ColPaliProcessor.from_pretrained(model_name)\n</pre> model_name = \"vidore/colpali-v1.2\" model = ColPali.from_pretrained(model_name, torch_dtype=dtype, device_map=device).eval() processor = ColPaliProcessor.from_pretrained(model_name) In\u00a0[\u00a0]: Copied! <pre>import requests\nfrom pdf2image import convert_from_path\nfrom pypdf import PdfReader\n\n\ndef download_pdf(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return BytesIO(response.content)\n    else:\n        raise Exception(f\"Failed to download PDF: Status code {response.status_code}\")\n\n\ndef get_pdf_images(pdf_url):\n    # Download the PDF\n    pdf_file = download_pdf(pdf_url)\n    # Save the PDF temporarily to disk (pdf2image requires a file path)\n    temp_file = \"temp.pdf\"\n    with open(temp_file, \"wb\") as f:\n        f.write(pdf_file.read())\n    reader = PdfReader(temp_file)\n    page_texts = []\n    for page_number in range(len(reader.pages)):\n        page = reader.pages[page_number]\n        text = page.extract_text()\n        page_texts.append(text)\n    images = convert_from_path(temp_file)\n    assert len(images) == len(page_texts)\n    return (images, page_texts)\n</pre> import requests from pdf2image import convert_from_path from pypdf import PdfReader   def download_pdf(url):     response = requests.get(url)     if response.status_code == 200:         return BytesIO(response.content)     else:         raise Exception(f\"Failed to download PDF: Status code {response.status_code}\")   def get_pdf_images(pdf_url):     # Download the PDF     pdf_file = download_pdf(pdf_url)     # Save the PDF temporarily to disk (pdf2image requires a file path)     temp_file = \"temp.pdf\"     with open(temp_file, \"wb\") as f:         f.write(pdf_file.read())     reader = PdfReader(temp_file)     page_texts = []     for page_number in range(len(reader.pages)):         page = reader.pages[page_number]         text = page.extract_text()         page_texts.append(text)     images = convert_from_path(temp_file)     assert len(images) == len(page_texts)     return (images, page_texts) <p>We define a few sample PDFs to work with. The PDFs are discovered from this url.</p> In\u00a0[\u00a0]: Copied! <pre>sample_pdfs = [\n    {\n        \"title\": \"ConocoPhillips Sustainability Highlights - Nature (24-0976)\",\n        \"url\": \"https://static.conocophillips.com/files/resources/24-0976-sustainability-highlights_nature.pdf\",\n    },\n    {\n        \"title\": \"ConocoPhillips Managing Climate Related Risks\",\n        \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-managing-climate-related-risks.pdf\",\n    },\n    {\n        \"title\": \"ConocoPhillips 2023 Sustainability Report\",\n        \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-sustainability-report.pdf\",\n    },\n]\n</pre> sample_pdfs = [     {         \"title\": \"ConocoPhillips Sustainability Highlights - Nature (24-0976)\",         \"url\": \"https://static.conocophillips.com/files/resources/24-0976-sustainability-highlights_nature.pdf\",     },     {         \"title\": \"ConocoPhillips Managing Climate Related Risks\",         \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-managing-climate-related-risks.pdf\",     },     {         \"title\": \"ConocoPhillips 2023 Sustainability Report\",         \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-sustainability-report.pdf\",     }, ] <p>Now we can convert the PDFs to images and also extract the text content.</p> In\u00a0[\u00a0]: Copied! <pre>for pdf in sample_pdfs:\n    page_images, page_texts = get_pdf_images(pdf[\"url\"])\n\n    pdf[\"images\"] = page_images\n    pdf[\"texts\"] = page_texts\n</pre> for pdf in sample_pdfs:     page_images, page_texts = get_pdf_images(pdf[\"url\"])      pdf[\"images\"] = page_images     pdf[\"texts\"] = page_texts <p>Let us look at the extracted image of the first PDF page. This is the document side input to ColPali, one image per page.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display\n\ndisplay(scale_image(sample_pdfs[0][\"images\"][0], 720))\n</pre> from IPython.display import display  display(scale_image(sample_pdfs[0][\"images\"][0], 720)) <p>Let us also look at the extracted text content of the first PDF page.</p> In\u00a0[\u00a0]: Copied! <pre>print(sample_pdfs[0][\"texts\"][0])\n</pre> print(sample_pdfs[0][\"texts\"][0]) <p>Notice how the layout and order of the text is different from the image representation. Note that</p> <ul> <li>The headlines NATURE and Sustainability have been combined into one word (NATURESustainability).</li> <li>The 0.03% has been converted to 0.03 and order is not preserved in the text representation.</li> <li>The data in the infographics is not represented in the text representation. For example the source water distribution in the infographics is not represented in the text representation.</li> </ul> <p>Now we use the ColPali model to generate embeddings of the images.</p> In\u00a0[\u00a0]: Copied! <pre>for pdf in sample_pdfs:\n    page_embeddings = []\n    dataloader = DataLoader(\n        pdf[\"images\"],\n        batch_size=2,\n        shuffle=False,\n        collate_fn=lambda x: processor.process_images(x),\n    )\n    for batch_doc in tqdm(dataloader):\n        with torch.no_grad():\n            batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\n            embeddings_doc = model(**batch_doc)\n            if model.device == \"cuda\":\n                embeddings_doc = embeddings_doc.float()\n            page_embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n    pdf[\"embeddings\"] = page_embeddings\n</pre> for pdf in sample_pdfs:     page_embeddings = []     dataloader = DataLoader(         pdf[\"images\"],         batch_size=2,         shuffle=False,         collate_fn=lambda x: processor.process_images(x),     )     for batch_doc in tqdm(dataloader):         with torch.no_grad():             batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}             embeddings_doc = model(**batch_doc)             if model.device == \"cuda\":                 embeddings_doc = embeddings_doc.float()             page_embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))     pdf[\"embeddings\"] = page_embeddings <p>Now we are done with the document side embeddings, we now convert the embeddings to Vespa JSON format so we can store (and index) them in Vespa. Details in Vespa JSON feed format doc.</p> <p>We use binary quantization (BQ) of the page level ColPali vector embeddings to reduce their size by 32x.</p> <p>Read more about binarization of multi-vector representations in the colbert blog post.</p> <p>The binarization step maps 128 dimensional floats to 128 bits, or 16 bytes per vector. Reducing the size by 32x. On the DocVQA benchmark, binarization results in a small drop in ranking accuracy.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nvespa_feed = []\nfor pdf in sample_pdfs:\n    url = pdf[\"url\"]\n    title = pdf[\"title\"]\n    for page_number, (page_text, embedding, image) in enumerate(\n        zip(pdf[\"texts\"], pdf[\"embeddings\"], pdf[\"images\"])\n    ):\n        base_64_image = get_base64_image(scale_image(image, 640), add_url_prefix=False)\n        embedding_dict = dict()\n        for idx, patch_embedding in enumerate(embedding):\n            binary_vector = (\n                np.packbits(np.where(patch_embedding &gt; 0, 1, 0))\n                .astype(np.int8)\n                .tobytes()\n                .hex()\n            )\n            embedding_dict[idx] = binary_vector\n        page = {\n            \"id\": hash(url + str(page_number)),\n            \"fields\": {\n                \"url\": url,\n                \"title\": title,\n                \"page_number\": page_number,\n                \"image\": base_64_image,\n                \"text\": page_text,\n                \"embedding\": embedding_dict,\n            },\n        }\n        vespa_feed.append(page)\n</pre> import numpy as np  vespa_feed = [] for pdf in sample_pdfs:     url = pdf[\"url\"]     title = pdf[\"title\"]     for page_number, (page_text, embedding, image) in enumerate(         zip(pdf[\"texts\"], pdf[\"embeddings\"], pdf[\"images\"])     ):         base_64_image = get_base64_image(scale_image(image, 640), add_url_prefix=False)         embedding_dict = dict()         for idx, patch_embedding in enumerate(embedding):             binary_vector = (                 np.packbits(np.where(patch_embedding &gt; 0, 1, 0))                 .astype(np.int8)                 .tobytes()                 .hex()             )             embedding_dict[idx] = binary_vector         page = {             \"id\": hash(url + str(page_number)),             \"fields\": {                 \"url\": url,                 \"title\": title,                 \"page_number\": page_number,                 \"image\": base_64_image,                 \"text\": page_text,                 \"embedding\": embedding_dict,             },         }         vespa_feed.append(page) In\u00a0[\u00a0]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet, HNSW\n\ncolpali_schema = Schema(\n    name=\"pdf_page\",\n    document=Document(\n        fields=[\n            Field(\n                name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"], match=[\"word\"]\n            ),\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(name=\"page_number\", type=\"int\", indexing=[\"summary\", \"attribute\"]),\n            Field(name=\"image\", type=\"raw\", indexing=[\"summary\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"index\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;int8&gt;(patch{}, v[16])\",\n                indexing=[\n                    \"attribute\",\n                    \"index\",\n                ],  # adds HNSW index for candidate retrieval.\n                ann=HNSW(\n                    distance_metric=\"hamming\",\n                    max_links_per_node=32,\n                    neighbors_to_explore_at_insert=400,\n                ),\n            ),\n        ]\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"text\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet, HNSW  colpali_schema = Schema(     name=\"pdf_page\",     document=Document(         fields=[             Field(                 name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"], match=[\"word\"]             ),             Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(name=\"page_number\", type=\"int\", indexing=[\"summary\", \"attribute\"]),             Field(name=\"image\", type=\"raw\", indexing=[\"summary\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"index\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"embedding\",                 type=\"tensor(patch{}, v[16])\",                 indexing=[                     \"attribute\",                     \"index\",                 ],  # adds HNSW index for candidate retrieval.                 ann=HNSW(                     distance_metric=\"hamming\",                     max_links_per_node=32,                     neighbors_to_explore_at_insert=400,                 ),             ),         ]     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"text\"])], ) <p>Notice the <code>embedding</code> field which is a tensor field with the type <code>tensor&lt;int8&gt;(patch{}, v[16])</code>. This is the field we use to represent the patch embeddings from ColPali.</p> <p>We also enable HNSW indexing for this field to enable fast nearest neighbor search which is used for candidate retrieval.</p> <p>We use binary hamming distance as an approximation of the cosine similarity. Hamming distance is a good approximation for binary representations, and it is much faster to compute than cosine similarity/dot product.</p> <p>The <code>embedding</code> field is an example of a mixed tensor where we combine one mapped (sparse) dimensions with a dense dimension.</p> <p>Read more in Tensor guide. We also enable BM25 for the <code>title</code> and <code>texts</code>\u00a0fields. Notice that the <code>image</code> field use type <code>raw</code> to store the binary image data, encoded with as a base64 string.</p> <p>Create the Vespa application package:</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"visionrag5\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name, schema=[colpali_schema]\n)\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"visionrag5\" vespa_application_package = ApplicationPackage(     name=vespa_app_name, schema=[colpali_schema] ) <p>Now we define how we want to rank the pages for a query. We use Vespa's support for BM25 for the text, and late interaction with Max Sim for the image embeddings.</p> <p>This means that we use the the text representations as a candidate retrieval phase,  then we use the ColPALI embeddings with MaxSim to rerank the pages.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\ncolpali_profile = RankProfile(\n    name=\"default\",\n    inputs=[(\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\")],\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(embedding)) , v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(name=\"bm25_score\", expression=\"bm25(title) + bm25(text)\"),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"bm25_score\"),\n    second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=100),\n)\ncolpali_schema.add_rank_profile(colpali_profile)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  colpali_profile = RankProfile(     name=\"default\",     inputs=[(\"query(qt)\", \"tensor(querytoken{}, v[128])\")],     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(embedding)) , v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(name=\"bm25_score\", expression=\"bm25(title) + bm25(text)\"),     ],     first_phase=FirstPhaseRanking(expression=\"bm25_score\"),     second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=100), ) colpali_schema.add_rank_profile(colpali_profile) <p>The first phase uses a linear combination of BM25 scores for the text fields, and the second phase uses the MaxSim function with the image embeddings. Notice that Vespa supports a <code>unpack_bits</code> function to convert the 16 compressed binary vectors to 128-dimensional floats for the MaxSim function. The query input tensor is not compressed and using full float resolution.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() In\u00a0[\u00a0]: Copied! <pre>print(\"Number of PDF pages:\", len(vespa_feed))\n</pre> print(\"Number of PDF pages:\", len(vespa_feed)) <p>Index the documents in Vespa using the Vespa HTTP API.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n\n\n# Feed data into Vespa synchronously\napp.feed_iterable(vespa_feed, schema=\"pdf_page\", callback=callback)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         )   # Feed data into Vespa synchronously app.feed_iterable(vespa_feed, schema=\"pdf_page\", callback=callback) <p>Now we can query Vespa with the text query and rerank the results using the ColPali embeddings.</p> In\u00a0[\u00a0]: Copied! <pre>queries = [\n    \"Percentage of non-fresh water as source?\",\n    \"Policies related to nature risk?\",\n    \"How much of produced water is recycled?\",\n]\n</pre> queries = [     \"Percentage of non-fresh water as source?\",     \"Policies related to nature risk?\",     \"How much of produced water is recycled?\", ] <p>Obtain the query embeddings using the ColPali model:</p> In\u00a0[\u00a0]: Copied! <pre>dataloader = DataLoader(\n    queries,\n    batch_size=1,\n    shuffle=False,\n    collate_fn=lambda x: processor.process_queries(x),\n)\nqs = []\nfor batch_query in dataloader:\n    with torch.no_grad():\n        batch_query = {k: v.to(model.device) for k, v in batch_query.items()}\n        embeddings_query = model(**batch_query)\n        if model.device == \"cuda\":\n            embeddings_query = embeddings_query.float()\n        qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n</pre> dataloader = DataLoader(     queries,     batch_size=1,     shuffle=False,     collate_fn=lambda x: processor.process_queries(x), ) qs = [] for batch_query in dataloader:     with torch.no_grad():         batch_query = {k: v.to(model.device) for k, v in batch_query.items()}         embeddings_query = model(**batch_query)         if model.device == \"cuda\":             embeddings_query = embeddings_query.float()         qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\")))) <p>We create a simple routine to display the results. We render the image and the title of the retrieved page/document.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display, HTML\n\n\ndef display_query_results(query, response, hits=5):\n    query_time = response.json.get(\"timing\", {}).get(\"searchtime\", -1)\n    query_time = round(query_time, 2)\n    count = response.json.get(\"root\", {}).get(\"fields\", {}).get(\"totalCount\", 0)\n    html_content = f\"&lt;h3&gt;Query text: '{query}', query time {query_time}s, count={count}, top results:&lt;/h3&gt;\"\n\n    for i, hit in enumerate(response.hits[:hits]):\n        title = hit[\"fields\"][\"title\"]\n        url = hit[\"fields\"][\"url\"]\n        page = hit[\"fields\"][\"page_number\"]\n        image = hit[\"fields\"][\"image\"]\n        score = hit[\"relevance\"]\n\n        html_content += f\"&lt;h4&gt;PDF Result {i + 1}&lt;/h4&gt;\"\n        html_content += f'&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; &lt;a href=\"{url}\"&gt;{title}&lt;/a&gt;, page {page+1} with score {score:.2f}&lt;/p&gt;'\n        html_content += (\n            f'&lt;img src=\"data:image/png;base64,{image}\" style=\"max-width:100%;\"&gt;'\n        )\n\n    display(HTML(html_content))\n</pre> from IPython.display import display, HTML   def display_query_results(query, response, hits=5):     query_time = response.json.get(\"timing\", {}).get(\"searchtime\", -1)     query_time = round(query_time, 2)     count = response.json.get(\"root\", {}).get(\"fields\", {}).get(\"totalCount\", 0)     html_content = f\"Query text: '{query}', query time {query_time}s, count={count}, top results:\"      for i, hit in enumerate(response.hits[:hits]):         title = hit[\"fields\"][\"title\"]         url = hit[\"fields\"][\"url\"]         page = hit[\"fields\"][\"page_number\"]         image = hit[\"fields\"][\"image\"]         score = hit[\"relevance\"]          html_content += f\"PDF Result {i + 1}\"         html_content += f'<p>Title: {title}, page {page+1} with score {score:.2f}</p>'         html_content += (             f''         )      display(HTML(html_content)) <p>Query Vespa with the queries and display the results, here we are using the <code>default</code> rank profile.</p> <p>Note that we retrieve using textual representation with <code>userInput(@userQuery)</code>, this means that we use the BM25 ranking for the extracted text in the first ranking phase and then re-rank the top-k pages using the ColPali embeddings.</p> <p>Later in this notebook we will use Vespa's support for approximate nearest neighbor search (<code>nearestNeighbor</code>) to retrieve directly using the ColPali embeddings.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\n\nasync with app.asyncio(connections=1, timeout=120) as session:\n    for idx, query in enumerate(queries):\n        query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}\n        response: VespaQueryResponse = await session.query(\n            yql=\"select title,url,image,page_number from pdf_page where userInput(@userQuery)\",\n            ranking=\"default\",\n            userQuery=query,\n            timeout=120,\n            hits=3,\n            body={\"input.query(qt)\": query_embedding, \"presentation.timing\": True},\n        )\n        assert response.is_successful()\n        display_query_results(query, response)\n</pre> from vespa.io import VespaQueryResponse  async with app.asyncio(connections=1, timeout=120) as session:     for idx, query in enumerate(queries):         query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}         response: VespaQueryResponse = await session.query(             yql=\"select title,url,image,page_number from pdf_page where userInput(@userQuery)\",             ranking=\"default\",             userQuery=query,             timeout=120,             hits=3,             body={\"input.query(qt)\": query_embedding, \"presentation.timing\": True},         )         assert response.is_successful()         display_query_results(query, response) In\u00a0[\u00a0]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\ninput_query_tensors = []\nMAX_QUERY_TERMS = 64\nfor i in range(MAX_QUERY_TERMS):\n    input_query_tensors.append((f\"query(rq{i})\", \"tensor&lt;int8&gt;(v[16])\"))\n\ninput_query_tensors.append((\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\"))\ninput_query_tensors.append((\"query(qtb)\", \"tensor&lt;int8&gt;(querytoken{}, v[16])\"))\n\ncolpali_retrieval_profile = RankProfile(\n    name=\"retrieval-and-rerank\",\n    inputs=input_query_tensors,\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(embedding)) , v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(\n            name=\"max_sim_binary\",\n            expression=\"\"\"\n                sum(\n                  reduce(\n                    1/(1 + sum(\n                        hamming(query(qtb), attribute(embedding)) ,v)\n                    ),\n                    max,\n                    patch\n                  ),\n                  querytoken\n                )\n            \"\"\",\n        ),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim_binary\"),\n    second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10),\n)\ncolpali_schema.add_rank_profile(colpali_retrieval_profile)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  input_query_tensors = [] MAX_QUERY_TERMS = 64 for i in range(MAX_QUERY_TERMS):     input_query_tensors.append((f\"query(rq{i})\", \"tensor(v[16])\"))  input_query_tensors.append((\"query(qt)\", \"tensor(querytoken{}, v[128])\")) input_query_tensors.append((\"query(qtb)\", \"tensor(querytoken{}, v[16])\"))  colpali_retrieval_profile = RankProfile(     name=\"retrieval-and-rerank\",     inputs=input_query_tensors,     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(embedding)) , v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(             name=\"max_sim_binary\",             expression=\"\"\"                 sum(                   reduce(                     1/(1 + sum(                         hamming(query(qtb), attribute(embedding)) ,v)                     ),                     max,                     patch                   ),                   querytoken                 )             \"\"\",         ),     ],     first_phase=FirstPhaseRanking(expression=\"max_sim_binary\"),     second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10), ) colpali_schema.add_rank_profile(colpali_retrieval_profile) <p>We define two functions, one for the first phase and one for the second phase. Instead of the float representations, we use the binary representations with inverted hamming distance in the first phase. Now, we need to re-deploy the application to Vespa Cloud.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <p>Now we can query Vespa with the text queries and use the <code>nearestNeighbor</code> operator to retrieve the most similar pages to the query and pass the different query tensors.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\n\ntarget_hits_per_query_tensor = (\n    20  # this is a hyper parameter that can be tuned for speed versus accuracy\n)\nasync with app.asyncio(connections=1, timeout=180) as session:\n    for idx, query in enumerate(queries):\n        float_query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}\n        binary_query_embeddings = dict()\n        for k, v in float_query_embedding.items():\n            binary_query_embeddings[k] = (\n                np.packbits(np.where(np.array(v) &gt; 0, 1, 0)).astype(np.int8).tolist()\n            )\n\n        # The mixed tensors used in MaxSim calculations\n        # We use both binary and float representations\n        query_tensors = {\n            \"input.query(qtb)\": binary_query_embeddings,\n            \"input.query(qt)\": float_query_embedding,\n        }\n        # The query tensors used in the nearest neighbor calculations\n        for i in range(0, len(binary_query_embeddings)):\n            query_tensors[f\"input.query(rq{i})\"] = binary_query_embeddings[i]\n        nn = []\n        for i in range(0, len(binary_query_embeddings)):\n            nn.append(\n                f\"({{targetHits:{target_hits_per_query_tensor}}}nearestNeighbor(embedding,rq{i}))\"\n            )\n        # We use a OR operator to combine the nearest neighbor operator\n        nn = \" OR \".join(nn)\n        response: VespaQueryResponse = await session.query(\n            yql=f\"select title, url, image, page_number from pdf_page where {nn}\",\n            ranking=\"retrieval-and-rerank\",\n            timeout=120,\n            hits=3,\n            body={**query_tensors, \"presentation.timing\": True},\n        )\n        assert response.is_successful()\n        display_query_results(query, response)\n</pre> from vespa.io import VespaQueryResponse  target_hits_per_query_tensor = (     20  # this is a hyper parameter that can be tuned for speed versus accuracy ) async with app.asyncio(connections=1, timeout=180) as session:     for idx, query in enumerate(queries):         float_query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}         binary_query_embeddings = dict()         for k, v in float_query_embedding.items():             binary_query_embeddings[k] = (                 np.packbits(np.where(np.array(v) &gt; 0, 1, 0)).astype(np.int8).tolist()             )          # The mixed tensors used in MaxSim calculations         # We use both binary and float representations         query_tensors = {             \"input.query(qtb)\": binary_query_embeddings,             \"input.query(qt)\": float_query_embedding,         }         # The query tensors used in the nearest neighbor calculations         for i in range(0, len(binary_query_embeddings)):             query_tensors[f\"input.query(rq{i})\"] = binary_query_embeddings[i]         nn = []         for i in range(0, len(binary_query_embeddings)):             nn.append(                 f\"({{targetHits:{target_hits_per_query_tensor}}}nearestNeighbor(embedding,rq{i}))\"             )         # We use a OR operator to combine the nearest neighbor operator         nn = \" OR \".join(nn)         response: VespaQueryResponse = await session.query(             yql=f\"select title, url, image, page_number from pdf_page where {nn}\",             ranking=\"retrieval-and-rerank\",             timeout=120,             hits=3,             body={**query_tensors, \"presentation.timing\": True},         )         assert response.is_successful()         display_query_results(query, response) <p>Depending on the scale, we can evaluate changing different number of targetHits per nearestNeighbor operator and the ranking depths in the two phases. We can also parallelize the ranking phases by using more threads per query request to reduce latency.</p> In\u00a0[\u00a0]: Copied! <pre>if os.getenv(\"CI\", \"false\") == \"true\":\n    vespa_cloud.delete()\n</pre> if os.getenv(\"CI\", \"false\") == \"true\":     vespa_cloud.delete()"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#scaling-colpali-vlm-retrieval","title":"Scaling ColPALI (VLM) Retrieval\u00b6","text":"<p>This notebook demonstrates how to represent ColPali in Vespa and to scale to large collections. Also see the blog post: Scaling ColPali to billions of PDFs with Vespa</p> <p>Consider following the ColQWen2 notebook instead as it use a better model with improved performance (Both accuracy and speed).</p> <p>ColPali is a powerful visual language model that can generate embeddings for images (screenshots of PDF pages) and text queries.</p> <p>In this notebook, we will use ColPali to generate embeddings for images of PDF pages and store the embeddings in Vespa. We will also store the base64 encoded image of the PDF page and meta data like title and url.</p> <p>We demonstrate how to retrieve relevant pages for a query using the embeddings generated by ColPali.</p> <p>The TLDR of this notebook:</p> <ul> <li>Generate an image per PDF page using pdf2image and also extract the text using pypdf.</li> <li>For each page image, use ColPali to obtain the visual multi-vector embeddings</li> </ul> <p>Then we store visual embeddings in Vespa as a <code>int8</code> tensor, where we use a binary compression technique to reduce the storage footprint by 32x compared to float representations. See Scaling ColPali to billions of PDFs with Vespa for details on binarization and using hamming distance for retrieval.</p> <p>During retrieval time, we use the same ColPali model to generate embeddings for the query and then use Vespa's <code>nearestNeighbor</code> query to retrieve the most similar documents per query vector token, using binary representation with hamming distance. Then we re-rank the results in two phases:</p> <ul> <li>In the 0-phase we use hamming distance to retrieve the k closest pages per query token vector representation, this is expressed by using multiple nearestNeighbor query operators in Vespa.</li> <li>The nearestNeighbor operators exposes pages to the first-phase ranking function, which uses an approximate MaxSim using inverted hamming distance insted of cosine similarity. This is done to reduce the number of pages that are re-ranked in the second phase.</li> <li>In the second phase, we perform the full MaxSim operation, using float representations of the embeddings to re-rank the top-k pages from the first phase.</li> </ul> <p>This allows us to scale ColPali to very large collections of PDF pages, while still providing accurate and fast retrieval.</p> <p>Let us get started.</p> <p></p> <p>Install dependencies:</p> <p>Note that the python pdf2image package requires poppler-utils, see other installation options here.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#load-the-model","title":"Load the model\u00b6","text":"<p>This requires that the HF_TOKEN environment variable is set as the underlaying PaliGemma model is hosted on Hugging Face and has a restricive licence that requires authentication.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#working-with-pdfs","title":"Working with pdfs\u00b6","text":"<p>We need to convert a PDF to an array of images. One image per page. We use the <code>pdf2image</code> library for this task. Secondary, we also extract the text contents of the PDF using <code>pypdf</code>.</p> <p>NOTE: This step requires that you have <code>poppler</code> installed on your system. Read more in pdf2image docs.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#configure-vespa","title":"Configure Vespa\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#querying-vespa","title":"Querying Vespa\u00b6","text":"<p>Ok, so now we have indexed the PDF pages in Vespa. Let us now obtain ColPali embeddings for a few text queries and use it during ranking of the indexed pdf pages.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#using-nearestneighbor-for-retrieval","title":"Using nearestNeighbor for retrieval\u00b6","text":"<p>In the above example, we used the ColPali embeddings in ranking, but using the text query for retrieval. This is a reasonable approach for text-heavy documents where the text representation is the most important and where ColPali embeddings are used to re-rank the top-k documents from the text retrieval phase.</p> <p>In some cases, the ColPali embeddings are the most important and we want to demonstrate how we can use HNSW indexing with binary hamming distance to retrieve the most similar pages to a query and then have two steps of re-ranking using the ColPali embeddings.</p> <p>All the phases here are executed locally inside the Vespa content node(s) so that no vector data needs to cross the network.</p> <p>Let us add a new rank-profile to the schema, the <code>nearestNeighbor</code> operator takes a query tensor and a field tensor as argument and we need to define the query tensors types in the rank-profile.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#summary","title":"Summary\u00b6","text":"<p>In this notebook, we have demonstrated how to represent ColPali in Vespa. We have generated embeddings for images of PDF pages using ColPali and stored the embeddings in Vespa using mixed tensors.</p> <p>We demonstrated how to store the base64 encoded image using the <code>raw</code> Vespa field type, plus meta data like title and url. We have demonstrated how to retrieve relevant pages for a query using the embeddings generated by ColPali.</p> <p>This notebook can be extended to include more complex ranking models, more complex queries, and more complex data structures, including metadata and other fields which can be filtered on or used for ranking.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#cleanup","title":"Cleanup\u00b6","text":"<p>When this notebook is running in CI, we want to delete the application.</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html","title":"Turbocharge rag with langchain and vespa streaming mode cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa langchain langchain-community pypdf==5.0.1 openai\n</pre> !pip3 install -U pyvespa langchain langchain-community pypdf==5.0.1 openai In\u00a0[1]: Copied! <pre>def sample_pdfs():\n    return [\n        {\n            \"title\": \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\",\n            \"url\": \"https://arxiv.org/pdf/2112.01488.pdf\",\n            \"authors\": \"Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia\",\n        },\n        {\n            \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",\n            \"url\": \"https://arxiv.org/pdf/2004.12832.pdf\",\n            \"authors\": \"Omar Khattab, Matei Zaharia\",\n        },\n        {\n            \"title\": \"On Approximate Nearest Neighbour Selection for Multi-Stage Dense Retrieval\",\n            \"url\": \"https://arxiv.org/pdf/2108.11480.pdf\",\n            \"authors\": \"Craig Macdonald, Nicola Tonellotto\",\n        },\n        {\n            \"title\": \"A Study on Token Pruning for ColBERT\",\n            \"url\": \"https://arxiv.org/pdf/2112.06540.pdf\",\n            \"authors\": \"Carlos Lassance, Maroua Maachou, Joohee Park, St\u00e9phane Clinchant\",\n        },\n        {\n            \"title\": \"Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval\",\n            \"url\": \"https://arxiv.org/pdf/2106.11251.pdf\",\n            \"authors\": \"Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis\",\n        },\n    ]\n</pre> def sample_pdfs():     return [         {             \"title\": \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\",             \"url\": \"https://arxiv.org/pdf/2112.01488.pdf\",             \"authors\": \"Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia\",         },         {             \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",             \"url\": \"https://arxiv.org/pdf/2004.12832.pdf\",             \"authors\": \"Omar Khattab, Matei Zaharia\",         },         {             \"title\": \"On Approximate Nearest Neighbour Selection for Multi-Stage Dense Retrieval\",             \"url\": \"https://arxiv.org/pdf/2108.11480.pdf\",             \"authors\": \"Craig Macdonald, Nicola Tonellotto\",         },         {             \"title\": \"A Study on Token Pruning for ColBERT\",             \"url\": \"https://arxiv.org/pdf/2112.06540.pdf\",             \"authors\": \"Carlos Lassance, Maroua Maachou, Joohee Park, St\u00e9phane Clinchant\",         },         {             \"title\": \"Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval\",             \"url\": \"https://arxiv.org/pdf/2106.11251.pdf\",             \"authors\": \"Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis\",         },     ] In\u00a0[2]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet, HNSW\n\npdf_schema = Schema(\n    name=\"pdf\",\n    mode=\"streaming\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"title\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"authors\", type=\"array&lt;string&gt;\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"page\", type=\"int\", indexing=[\"summary\", \"index\"]),\n            Field(\n                name=\"metadata\",\n                type=\"map&lt;string,string&gt;\",\n                indexing=[\"summary\", \"index\"],\n            ),\n            Field(name=\"chunks\", type=\"array&lt;string&gt;\", indexing=[\"summary\", \"index\"]),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;bfloat16&gt;(chunk{}, x[384])\",\n                indexing=[\"input chunks\", \"embed e5\", \"attribute\", \"index\"],\n                ann=HNSW(distance_metric=\"angular\"),\n                is_document_field=False,\n            ),\n        ],\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"chunks\", \"title\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet, HNSW  pdf_schema = Schema(     name=\"pdf\",     mode=\"streaming\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"title\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"authors\", type=\"array\", indexing=[\"summary\", \"index\"]),             Field(name=\"page\", type=\"int\", indexing=[\"summary\", \"index\"]),             Field(                 name=\"metadata\",                 type=\"map\",                 indexing=[\"summary\", \"index\"],             ),             Field(name=\"chunks\", type=\"array\", indexing=[\"summary\", \"index\"]),             Field(                 name=\"embedding\",                 type=\"tensor(chunk{}, x[384])\",                 indexing=[\"input chunks\", \"embed e5\", \"attribute\", \"index\"],                 ann=HNSW(distance_metric=\"angular\"),                 is_document_field=False,             ),         ],     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"chunks\", \"title\"])], ) <p>The above defines our <code>pdf</code> schema using mode <code>streaming</code>. Most fields are straightforward, but take a note of:</p> <ul> <li><code>metadata</code> using <code>map&lt;string,string&gt;</code> - here we can store and match over page level metadata extracted by the PDF parser.</li> <li><code>chunks</code> using <code>array&lt;string&gt;</code>, these are the text chunks that we use langchain document transformers for</li> <li>The <code>embedding</code> field of type <code>tensor&lt;bfloat16&gt;(chunk{},x[384])</code> allows us to store and search the 384-dimensional embeddings per chunk in the same document</li> </ul> <p>The observant reader might have noticed the <code>e5</code> argument to the <code>embed</code> expression in the above <code>embedding</code> field. The <code>e5</code> argument references a component of the type hugging-face-embedder. We configure the application package and its name with the <code>pdf</code> schema and the <code>e5</code> embedder component.</p> In\u00a0[3]: Copied! <pre>from vespa.package import ApplicationPackage, Component, Parameter\n\nvespa_app_name = \"ragpdfs\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name,\n    schema=[pdf_schema],\n    components=[\n        Component(\n            id=\"e5\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    \"transformer-model\",\n                    {\n                        \"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\"\n                    },\n                ),\n                Parameter(\n                    \"tokenizer-model\",\n                    {\n                        \"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\"\n                    },\n                ),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import ApplicationPackage, Component, Parameter  vespa_app_name = \"ragpdfs\" vespa_application_package = ApplicationPackage(     name=vespa_app_name,     schema=[pdf_schema],     components=[         Component(             id=\"e5\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(                     \"transformer-model\",                     {                         \"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\"                     },                 ),                 Parameter(                     \"tokenizer-model\",                     {                         \"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\"                     },                 ),             ],         )     ], ) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p>Vespa supports phased ranking and has a rich set of built-in rank-features, including many text-matching features such as:</p> <ul> <li>BM25.</li> <li>nativeRank and many more.</li> </ul> <p>Users can also define custom functions using ranking expressions. The following defines a <code>hybrid</code> Vespa ranking profile.</p> In\u00a0[4]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking\n\n\nsemantic = RankProfile(\n    name=\"hybrid\",\n    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n    functions=[\n        Function(\n            name=\"similarities\",\n            expression=\"cosine_similarity(query(q), attribute(embedding),x)\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(\n        expression=\"nativeRank(title) + nativeRank(chunks) + reduce(similarities, max, chunk)\",\n        rank_score_drop_limit=0.0,\n    ),\n    match_features=[\n        \"closest(embedding)\",\n        \"similarities\",\n        \"nativeRank(chunks)\",\n        \"nativeRank(title)\",\n        \"elementSimilarity(chunks)\",\n    ],\n)\npdf_schema.add_rank_profile(semantic)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking   semantic = RankProfile(     name=\"hybrid\",     inputs=[(\"query(q)\", \"tensor(x[384])\")],     functions=[         Function(             name=\"similarities\",             expression=\"cosine_similarity(query(q), attribute(embedding),x)\",         )     ],     first_phase=FirstPhaseRanking(         expression=\"nativeRank(title) + nativeRank(chunks) + reduce(similarities, max, chunk)\",         rank_score_drop_limit=0.0,     ),     match_features=[         \"closest(embedding)\",         \"similarities\",         \"nativeRank(chunks)\",         \"nativeRank(title)\",         \"elementSimilarity(chunks)\",     ], ) pdf_schema.add_rank_profile(semantic) <p>The <code>hybrid</code> rank-profile above defines the query input embedding type and a similarities function that uses a Vespa tensor compute function that calculates the cosine similarity between all the chunk embeddings and the query embedding.</p> <p>The profile only defines a single ranking phase, using a linear combination of multiple features.</p> <p>Using match-features, Vespa returns selected features along with the hit in the SERP (result page).</p> In\u00a0[8]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[18]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 2 of dev-aws-us-east-1c for samples.pdfs. This may take a few minutes the first time.\nINFO    [17:23:35]  Deploying platform version 8.270.8 and application dev build 2 for dev-aws-us-east-1c of default ...\nINFO    [17:23:35]  Using CA signed certificate version 0\nWARNING [17:23:35]  For schema 'pdf', field 'page': Changed to attribute because numerical indexes (field has type int) is not currently supported. Index-only settings may fail. Ignore this warning for streaming search.\nINFO    [17:23:35]  Using 1 nodes in container cluster 'pdfs_container'\nWARNING [17:23:36]  For streaming search cluster 'pdfs_content.pdf', SD field 'embedding': hnsw index is not relevant and not supported, ignoring setting\nWARNING [17:23:36]  For streaming search cluster 'pdfs_content.pdf', SD field 'embedding': hnsw index is not relevant and not supported, ignoring setting\nINFO    [17:23:38]  Deployment successful.\nINFO    [17:23:38]  Session 3239 for tenant 'samples' prepared and activated.\nINFO    [17:23:38]  ######## Details for all nodes ########\nINFO    [17:23:38]  h88963a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [17:23:38]  --- platform vespa/cloud-tenant-rhel8:8.270.8\nINFO    [17:23:38]  --- storagenode on port 19102 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  --- searchnode on port 19107 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  --- distributor on port 19111 has config generation 3238, wanted is 3239\nINFO    [17:23:38]  --- metricsproxy-container on port 19092 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  h88969g.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [17:23:38]  --- platform vespa/cloud-tenant-rhel8:8.270.8\nINFO    [17:23:38]  --- logserver-container on port 4080 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  --- metricsproxy-container on port 19092 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  h88972i.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [17:23:38]  --- platform vespa/cloud-tenant-rhel8:8.270.8\nINFO    [17:23:38]  --- container-clustercontroller on port 19050 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  --- metricsproxy-container on port 19092 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  h89461a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [17:23:38]  --- platform vespa/cloud-tenant-rhel8:8.270.8\nINFO    [17:23:38]  --- container on port 4080 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  --- metricsproxy-container on port 19092 has config generation 3239, wanted is 3239\nINFO    [17:23:51]  Found endpoints:\nINFO    [17:23:51]  - dev.aws-us-east-1c\nINFO    [17:23:51]   |-- https://c4f42a1b.bfbdb4fd.z.vespa-app.cloud/ (cluster 'pdfs_container')\nINFO    [17:23:52]  Installation succeeded!\nUsing mTLS (key,cert) Authentication against endpoint https://c4f42a1b.bfbdb4fd.z.vespa-app.cloud//ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[10]: Copied! <pre>from langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1024,  # chars, not llm tokens\n    chunk_overlap=0,\n    length_function=len,\n    is_separator_regex=False,\n)\n</pre> from langchain_community.document_loaders import PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter  text_splitter = RecursiveCharacterTextSplitter(     chunk_size=1024,  # chars, not llm tokens     chunk_overlap=0,     length_function=len,     is_separator_regex=False, ) <p>The following iterates over the <code>sample_pdfs</code> and performs the following:</p> <ul> <li>Load the URL and extract the text into pages. A page is the retrievable unit we will use in Vespa</li> <li>For each page, use the text splitter to split the text into chunks. The chunks are represented as an <code>array&lt;string&gt;</code> in the Vespa schema</li> <li>Create the page level Vespa <code>fields</code>, note that we duplicate some content like the title and URL into the page level representation.</li> </ul> In\u00a0[11]: Copied! <pre>import hashlib\nimport unicodedata\n\n\ndef remove_control_characters(s):\n    return \"\".join(ch for ch in s if unicodedata.category(ch)[0] != \"C\")\n\n\nmy_docs_to_feed = []\nfor pdf in sample_pdfs():\n    url = pdf[\"url\"]\n    loader = PyPDFLoader(url)\n    pages = loader.load_and_split()\n    for index, page in enumerate(pages):\n        source = page.metadata[\"source\"]\n        chunks = text_splitter.transform_documents([page])\n        text_chunks = [chunk.page_content for chunk in chunks]\n        text_chunks = [remove_control_characters(chunk) for chunk in text_chunks]\n        page_number = index + 1\n        vespa_id = f\"{url}#{page_number}\"\n        hash_value = hashlib.sha1(vespa_id.encode()).hexdigest()\n        fields = {\n            \"title\": pdf[\"title\"],\n            \"url\": url,\n            \"page\": page_number,\n            \"id\": hash_value,\n            \"authors\": [a.strip() for a in pdf[\"authors\"].split(\",\")],\n            \"chunks\": text_chunks,\n            \"metadata\": page.metadata,\n        }\n        my_docs_to_feed.append(fields)\n</pre> import hashlib import unicodedata   def remove_control_characters(s):     return \"\".join(ch for ch in s if unicodedata.category(ch)[0] != \"C\")   my_docs_to_feed = [] for pdf in sample_pdfs():     url = pdf[\"url\"]     loader = PyPDFLoader(url)     pages = loader.load_and_split()     for index, page in enumerate(pages):         source = page.metadata[\"source\"]         chunks = text_splitter.transform_documents([page])         text_chunks = [chunk.page_content for chunk in chunks]         text_chunks = [remove_control_characters(chunk) for chunk in text_chunks]         page_number = index + 1         vespa_id = f\"{url}#{page_number}\"         hash_value = hashlib.sha1(vespa_id.encode()).hexdigest()         fields = {             \"title\": pdf[\"title\"],             \"url\": url,             \"page\": page_number,             \"id\": hash_value,             \"authors\": [a.strip() for a in pdf[\"authors\"].split(\",\")],             \"chunks\": text_chunks,             \"metadata\": page.metadata,         }         my_docs_to_feed.append(fields) <p>Now that we have parsed the input PDFs and created a list of pages that we want to add to Vespa, we must format the list into the format that PyVespa accepts. Notice the <code>fields</code>, <code>id</code> and <code>groupname</code> keys. The <code>groupname</code> is the key that is used to shard and co-locate the data and is only relevant when using Vespa with streaming mode.</p> In\u00a0[12]: Copied! <pre>from typing import Iterable\n\n\ndef vespa_feed(user: str) -&gt; Iterable[dict]:\n    for doc in my_docs_to_feed:\n        yield {\"fields\": doc, \"id\": doc[\"id\"], \"groupname\": user}\n</pre> from typing import Iterable   def vespa_feed(user: str) -&gt; Iterable[dict]:     for doc in my_docs_to_feed:         yield {\"fields\": doc, \"id\": doc[\"id\"], \"groupname\": user} <p>Now, we can feed to the Vespa instance (<code>app</code>), using the <code>feed_iterable</code> API, using the generator function above as input with a custom <code>callback</code> function. Vespa also performs embedding inference during this step using the built-in Vespa embedding functionality.</p> In\u00a0[13]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Document {id} failed to feed with status code {response.status_code}, url={response.url} response={response.json}\"\n        )\n\n\napp.feed_iterable(\n    schema=\"pdf\", iter=vespa_feed(\"jo-bergum\"), namespace=\"personal\", callback=callback\n)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Document {id} failed to feed with status code {response.status_code}, url={response.url} response={response.json}\"         )   app.feed_iterable(     schema=\"pdf\", iter=vespa_feed(\"jo-bergum\"), namespace=\"personal\", callback=callback ) <p>Notice the <code>schema</code> and <code>namespace</code> arguments. PyVespa transforms the input operations to Vespa document v1 requests.</p> <p></p> In\u00a0[15]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select id,title,page,chunks from pdf where userQuery() or ({targetHits:10}nearestNeighbor(embedding,q))\",\n    groupname=\"jo-bergum\",\n    ranking=\"hybrid\",\n    query=\"why is colbert effective?\",\n    body={\n        \"presentation.format.tensors\": \"short-value\",\n        \"input.query(q)\": 'embed(e5, \"why is colbert effective?\")',\n    },\n    timeout=\"2s\",\n)\nassert response.is_successful()\nprint(json.dumps(response.hits[0], indent=2))\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select id,title,page,chunks from pdf where userQuery() or ({targetHits:10}nearestNeighbor(embedding,q))\",     groupname=\"jo-bergum\",     ranking=\"hybrid\",     query=\"why is colbert effective?\",     body={         \"presentation.format.tensors\": \"short-value\",         \"input.query(q)\": 'embed(e5, \"why is colbert effective?\")',     },     timeout=\"2s\", ) assert response.is_successful() print(json.dumps(response.hits[0], indent=2)) <pre>{\n  \"id\": \"id:personal:pdf:g=jo-bergum:a4b2ced87807ee9cb0325b7a1c64a070d05a31f7\",\n  \"relevance\": 1.1412738851962692,\n  \"source\": \"pdfs_content.pdf\",\n  \"fields\": {\n    \"matchfeatures\": {\n      \"closest(embedding)\": {\n        \"0\": 1.0\n      },\n      \"elementSimilarity(chunks)\": 0.5006379585326953,\n      \"nativeRank(chunks)\": 0.15642522855051508,\n      \"nativeRank(title)\": 0.1341324233922751,\n      \"similarities\": {\n        \"1\": 0.7731813192367554,\n        \"2\": 0.8196794986724854,\n        \"3\": 0.796222984790802,\n        \"4\": 0.7699441909790039,\n        \"0\": 0.850716233253479\n      }\n    },\n    \"id\": \"a4b2ced87807ee9cb0325b7a1c64a070d05a31f7\",\n    \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",\n    \"page\": 9,\n    \"chunks\": [\n      \"Sq,d:=\\u00d5i\\u2208[|Eq|]maxj\\u2208[|Ed|]Eqi\\u00b7ETdj(3)ColBERT is di\\ufb00erentiable end-to-end. We /f_ine-tune the BERTencoders and train from scratch the additional parameters (i.e., thelinear layer and the [Q] and [D] markers\\u2019 embeddings) using theAdam [ 16] optimizer. Notice that our interaction mechanism hasno trainable parameters. Given a triple \\u27e8q,d+,d\\u2212\\u27e9with query q,positive document d+and negative document d\\u2212, ColBERT is usedto produce a score for each document individually and is optimizedvia pairwise so/f_tmax cross-entropy loss over the computed scoresofd+andd\\u2212.3.4 O\\ufb00line Indexing: Computing &amp; StoringDocument EmbeddingsBy design, ColBERT isolates almost all of the computations betweenqueries and documents, largely to enable pre-computing documentrepresentations o\\ufb04ine. At a high level, our indexing procedure isstraight-forward: we proceed over the documents in the collectionin batches, running our document encoder fDon each batch andstoring the output embeddings per document. Although indexing\",\n      \"a set of documents is an o\\ufb04ine process, we incorporate a fewsimple optimizations for enhancing the throughput of indexing. Aswe show in \\u00a74.5, these optimizations can considerably reduce theo\\ufb04ine cost of indexing.To begin with, we exploit multiple GPUs, if available, for fasterencoding of batches of documents in parallel. When batching, wepad all documents to the maximum length of a document withinthe batch.3To make capping the sequence length on a per-batchbasis more e\\ufb00ective, our indexer proceeds through documents ingroups of B(e.g., B=100,000) documents. It sorts these documentsby length and then feeds batches of b(e.g., b=128) documents ofcomparable length through our encoder. /T_his length-based bucket-ing is sometimes refered to as a BucketIterator in some libraries(e.g., allenNLP). Lastly, while most computations occur on the GPU,we found that a non-trivial portion of the indexing time is spent onpre-processing the text sequences, primarily BERT\\u2019s WordPiece to-\",\n      \"kenization. Exploiting that these operations are independent acrossdocuments in a batch, we parallelize the pre-processing across theavailable CPU cores.Once the document representations are produced, they are savedto disk using 32-bit or 16-bit values to represent each dimension.As we describe in \\u00a73.5 and 3.6, these representations are eithersimply loaded from disk for ranking or are subsequently indexedfor vector-similarity search, respectively.3.5 Top- kRe-ranking with ColBERTRecall that ColBERT can be used for re-ranking the output of an-other retrieval model, typically a term-based model, or directlyfor end-to-end retrieval from a document collection. In this sec-tion, we discuss how we use ColBERT for ranking a small set ofk(e.g., k=1000) documents given a query q. Since kis small, werely on batch computations to exhaustively score each document\",\n      \"3/T_he public BERT implementations we saw simply pad to a pre-de/f_ined length.(unlike our approach in \\u00a73.6). To begin with, our query serving sub-system loads the indexed documents representations into memory,representing each document as a matrix of embeddings.Given a query q, we compute its bag of contextualized embed-dings Eq(Equation 1) and, concurrently, gather the document repre-sentations into a 3-dimensional tensor Dconsisting of kdocumentmatrices. We pad the kdocuments to their maximum length tofacilitate batched operations, and move the tensor Dto the GPU\\u2019smemory. On the GPU, we compute a batch dot-product of EqandD, possibly over multiple mini-batches. /T_he output materializes a3-dimensional tensor that is a collection of cross-match matricesbetween qand each document. To compute the score of each docu-ment, we reduce its matrix across document terms via a max-pool(i.e., representing an exhaustive implementation of our MaxSim\",\n      \"computation) and reduce across query terms via a summation. Fi-nally, we sort the kdocuments by their total scores.\"\n    ]\n  }\n}\n</pre> <p>Notice the <code>matchfeatures</code> that returns the configured match-features from the rank-profile, including all the chunk similarities.</p> In\u00a0[19]: Copied! <pre>from langchain_core.documents import Document\nfrom langchain_core.retrievers import BaseRetriever\nfrom typing import List\n\n\nclass VespaStreamingHybridRetriever(BaseRetriever):\n    app: Vespa\n    user: str\n    pages: int = 5\n    chunks_per_page: int = 3\n    chunk_similarity_threshold: float = 0.8\n\n    def _get_relevant_documents(self, query: str) -&gt; List[Document]:\n        response: VespaQueryResponse = self.app.query(\n            yql=\"select id, url, title, page, authors, chunks from pdf where userQuery() or ({targetHits:20}nearestNeighbor(embedding,q))\",\n            groupname=self.user,\n            ranking=\"hybrid\",\n            query=query,\n            hits=self.pages,\n            body={\n                \"presentation.format.tensors\": \"short-value\",\n                \"input.query(q)\": f'embed(e5, \"query: {query} \")',\n            },\n            timeout=\"2s\",\n        )\n        if not response.is_successful():\n            raise ValueError(\n                f\"Query failed with status code {response.status_code}, url={response.url} response={response.json}\"\n            )\n        return self._parse_response(response)\n\n    def _parse_response(self, response: VespaQueryResponse) -&gt; List[Document]:\n        documents: List[Document] = []\n        for hit in response.hits:\n            fields = hit[\"fields\"]\n            chunks_with_scores = self._get_chunk_similarities(fields)\n            ## Best k chunks from each page\n            best_chunks_on_page = \" ### \".join(\n                [\n                    chunk\n                    for chunk, score in chunks_with_scores[0 : self.chunks_per_page]\n                    if score &gt; self.chunk_similarity_threshold\n                ]\n            )\n            documents.append(\n                Document(\n                    id=fields[\"id\"],\n                    page_content=best_chunks_on_page,\n                    title=fields[\"title\"],\n                    metadata={\n                        \"title\": fields[\"title\"],\n                        \"url\": fields[\"url\"],\n                        \"page\": fields[\"page\"],\n                        \"authors\": fields[\"authors\"],\n                        \"features\": fields[\"matchfeatures\"],\n                    },\n                )\n            )\n        return documents\n\n    def _get_chunk_similarities(self, hit_fields: dict) -&gt; List[tuple]:\n        match_features = hit_fields[\"matchfeatures\"]\n        similarities = match_features[\"similarities\"]\n        chunk_scores = []\n        for i in range(0, len(similarities)):\n            chunk_scores.append(similarities.get(str(i), 0))\n        chunks = hit_fields[\"chunks\"]\n        chunks_with_scores = list(zip(chunks, chunk_scores))\n        return sorted(chunks_with_scores, key=lambda x: x[1], reverse=True)\n</pre> from langchain_core.documents import Document from langchain_core.retrievers import BaseRetriever from typing import List   class VespaStreamingHybridRetriever(BaseRetriever):     app: Vespa     user: str     pages: int = 5     chunks_per_page: int = 3     chunk_similarity_threshold: float = 0.8      def _get_relevant_documents(self, query: str) -&gt; List[Document]:         response: VespaQueryResponse = self.app.query(             yql=\"select id, url, title, page, authors, chunks from pdf where userQuery() or ({targetHits:20}nearestNeighbor(embedding,q))\",             groupname=self.user,             ranking=\"hybrid\",             query=query,             hits=self.pages,             body={                 \"presentation.format.tensors\": \"short-value\",                 \"input.query(q)\": f'embed(e5, \"query: {query} \")',             },             timeout=\"2s\",         )         if not response.is_successful():             raise ValueError(                 f\"Query failed with status code {response.status_code}, url={response.url} response={response.json}\"             )         return self._parse_response(response)      def _parse_response(self, response: VespaQueryResponse) -&gt; List[Document]:         documents: List[Document] = []         for hit in response.hits:             fields = hit[\"fields\"]             chunks_with_scores = self._get_chunk_similarities(fields)             ## Best k chunks from each page             best_chunks_on_page = \" ### \".join(                 [                     chunk                     for chunk, score in chunks_with_scores[0 : self.chunks_per_page]                     if score &gt; self.chunk_similarity_threshold                 ]             )             documents.append(                 Document(                     id=fields[\"id\"],                     page_content=best_chunks_on_page,                     title=fields[\"title\"],                     metadata={                         \"title\": fields[\"title\"],                         \"url\": fields[\"url\"],                         \"page\": fields[\"page\"],                         \"authors\": fields[\"authors\"],                         \"features\": fields[\"matchfeatures\"],                     },                 )             )         return documents      def _get_chunk_similarities(self, hit_fields: dict) -&gt; List[tuple]:         match_features = hit_fields[\"matchfeatures\"]         similarities = match_features[\"similarities\"]         chunk_scores = []         for i in range(0, len(similarities)):             chunk_scores.append(similarities.get(str(i), 0))         chunks = hit_fields[\"chunks\"]         chunks_with_scores = list(zip(chunks, chunk_scores))         return sorted(chunks_with_scores, key=lambda x: x[1], reverse=True) <p>That's it! We can give our newborn retriever a spin for the user\u00a0<code>jo-bergum</code> by</p> In\u00a0[20]: Copied! <pre>vespa_hybrid_retriever = VespaStreamingHybridRetriever(\n    app=app, user=\"jo-bergum\", pages=1, chunks_per_page=1\n)\n</pre> vespa_hybrid_retriever = VespaStreamingHybridRetriever(     app=app, user=\"jo-bergum\", pages=1, chunks_per_page=1 ) In\u00a0[21]: Copied! <pre>vespa_hybrid_retriever.get_relevant_documents(\"what is the maxsim operator in colbert?\")\n</pre> vespa_hybrid_retriever.get_relevant_documents(\"what is the maxsim operator in colbert?\") Out[21]: <pre>[Document(page_content='ture that precisely does so. As illustrated, every query embeddinginteracts with all document embeddings via a MaxSim operator,which computes maximum similarity (e.g., cosine similarity), andthe scalar outputs of these operators are summed across queryterms. /T_his paradigm allows ColBERT to exploit deep LM-basedrepresentations while shi/f_ting the cost of encoding documents of-/f_line and amortizing the cost of encoding the query once acrossall ranked documents. Additionally, it enables ColBERT to lever-age vector-similarity search indexes (e.g., [ 1,15]) to retrieve thetop-kresults directly from a large document collection, substan-tially improving recall over models that only re-rank the output ofterm-based retrieval.As Figure 1 illustrates, ColBERT can serve queries in tens orfew hundreds of milliseconds. For instance, when used for re-ranking as in \u201cColBERT (re-rank)\u201d, it delivers over 170 \u00d7speedup(and requires 14,000 \u00d7fewer FLOPs) relative to existing BERT-based', metadata={'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'url': 'https://arxiv.org/pdf/2004.12832.pdf', 'page': 4, 'authors': ['Omar Khattab', 'Matei Zaharia'], 'features': {'closest(embedding)': {'0': 1.0}, 'elementSimilarity(chunks)': 0.41768707482993195, 'nativeRank(chunks)': 0.1401101487033024, 'nativeRank(title)': 0.0520403737720047, 'similarities': {'1': 0.8369992971420288, '0': 0.8730311393737793}}})]</pre> <p>Finally, we can connect our custom retriever with the complete flexibility and power of the [LangChain] LLM framework. The following uses LangChain Expression Language, or LCEL, a declarative way to compose chains.</p> <p>We have several steps composed into a chain:</p> <ul> <li>The prompt template and LLM model, in this case using OpenAI</li> <li>The retriever that provides the retrieved context for the question</li> <li>The formatting of the retrieved context</li> </ul> In\u00a0[22]: Copied! <pre>vespa_hybrid_retriever = VespaStreamingHybridRetriever(\n    app=app, user=\"jo-bergum\", pages=3, chunks_per_page=3\n)\n</pre> vespa_hybrid_retriever = VespaStreamingHybridRetriever(     app=app, user=\"jo-bergum\", pages=3, chunks_per_page=3 ) In\u00a0[25]: Copied! <pre>from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema import StrOutputParser\nfrom langchain.schema.runnable import RunnablePassthrough\n\nprompt_template = \"\"\"\nAnswer the question based only on the following context. \nCite the page number and the url of the document you are citing.\n\n{context}\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(prompt_template)\nmodel = ChatOpenAI()\n\n\ndef format_prompt_context(docs) -&gt; str:\n    context = []\n    for d in docs:\n        context.append(f\"{d.metadata['title']} by {d.metadata['authors']}\\n\")\n        context.append(f\"url: {d.metadata['url']}\\n\")\n        context.append(f\"page: {d.metadata['page']}\\n\")\n        context.append(f\"{d.page_content}\\n\\n\")\n    return \"\".join(context)\n\n\nchain = (\n    {\n        \"context\": vespa_hybrid_retriever | format_prompt_context,\n        \"question\": RunnablePassthrough(),\n    }\n    | prompt\n    | model\n    | StrOutputParser()\n)\n</pre> from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough  prompt_template = \"\"\" Answer the question based only on the following context.  Cite the page number and the url of the document you are citing.  {context} Question: {question} \"\"\" prompt = ChatPromptTemplate.from_template(prompt_template) model = ChatOpenAI()   def format_prompt_context(docs) -&gt; str:     context = []     for d in docs:         context.append(f\"{d.metadata['title']} by {d.metadata['authors']}\\n\")         context.append(f\"url: {d.metadata['url']}\\n\")         context.append(f\"page: {d.metadata['page']}\\n\")         context.append(f\"{d.page_content}\\n\\n\")     return \"\".join(context)   chain = (     {         \"context\": vespa_hybrid_retriever | format_prompt_context,         \"question\": RunnablePassthrough(),     }     | prompt     | model     | StrOutputParser() ) In\u00a0[26]: Copied! <pre>chain.invoke(\"what is colbert?\")\n</pre> chain.invoke(\"what is colbert?\") Out[26]: <pre>'ColBERT is a ranking model that adapts deep language models, specifically BERT, for efficient retrieval. It introduces a late interaction architecture that independently encodes queries and documents using BERT and then uses a cheap yet powerful interaction step to model their fine-grained similarity. This allows ColBERT to leverage the expressiveness of deep language models while also being able to pre-compute document representations offline, significantly speeding up query processing. ColBERT can be used for re-ranking documents retrieved by a traditional model or for end-to-end retrieval directly from a large document collection. It has been shown to be effective and efficient compared to existing models. (source: ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT by Omar Khattab, Matei Zaharia, page 1, url: https://arxiv.org/pdf/2004.12832.pdf)'</pre> In\u00a0[27]: Copied! <pre>chain.invoke(\"what is the colbert maxsim operator\")\n</pre> chain.invoke(\"what is the colbert maxsim operator\") Out[27]: <pre>\"The ColBERT model utilizes the MaxSim operator, which computes the maximum similarity (e.g., cosine similarity) between query embeddings and document embeddings. The scalar outputs of these operators are summed across query terms, allowing ColBERT to exploit deep LM-based representations while reducing the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents.\\n\\nSource: \\nColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT by ['Omar Khattab', 'Matei Zaharia']\\nURL: https://arxiv.org/pdf/2004.12832.pdf\\nPage: 4\"</pre> In\u00a0[28]: Copied! <pre>chain.invoke(\n    \"What is the difference between colbert and single vector representational models?\"\n)\n</pre> chain.invoke(     \"What is the difference between colbert and single vector representational models?\" ) Out[28]: <pre>'The difference between ColBERT and single vector representational models is that ColBERT utilizes a late interaction architecture that independently encodes the query and the document using BERT, while single vector models use a single embedding vector for both the query and the document. This late interaction mechanism in ColBERT allows for fine-grained similarity estimation, which leads to more effective retrieval. (Source: ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT by Omar Khattab and Matei Zaharia, page 17, url: https://arxiv.org/pdf/2004.12832.pdf)'</pre> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#turbocharge-rag-with-langchain-and-vespa-streaming-mode-for-partitioned-data","title":"Turbocharge RAG with LangChain and Vespa Streaming Mode for Partitioned Data\u00b6","text":"<p>This notebook illustrates using Vespa streaming mode to build cost-efficient RAG applications over naturally sharded data.</p> <p>You can read more about Vespa vector streaming search in these blog posts:</p> <ul> <li>Announcing vector streaming search: AI assistants at scale without breaking the bank</li> <li>Yahoo Mail turns to Vespa to do RAG at scale</li> <li>Hands-On RAG guide for personal data with Vespa and LLamaIndex</li> </ul> <p>This notebook is also available in blog form: Turbocharge RAG with LangChain and Vespa Streaming Mode for Sharded Data</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#tldr-vespa-streaming-mode-for-partitioned-data","title":"TLDR; Vespa streaming mode for partitioned data\u00b6","text":"<p>Vespa's streaming search solution enables you to integrate a user ID (or any sharding key) into the Vespa document ID. This setup allows Vespa to efficiently group each user's data on a small set of nodes and the same disk chunk. Streaming mode enables low latency searches on a user's data without keeping data in memory.</p> <p>The key benefits of streaming mode:</p> <ul> <li>Eliminating compromises in precision introduced by approximate algorithms</li> <li>Achieve significantly higher write throughput, thanks to the absence of index builds required for supporting approximate search.</li> <li>Optimize efficiency by storing documents, including tensors and data, on disk, benefiting from the cost-effective economics of storage tiers.</li> <li>Storage cost is the primary cost driver of Vespa streaming mode; no data is in memory. Avoiding memory usage lowers deployment costs significantly.</li> </ul>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#connecting-langchain-retriever-with-vespa-for-context-retrieval-from-pdf-documents","title":"Connecting LangChain Retriever with Vespa for Context Retrieval from PDF Documents\u00b6","text":"<p>In this notebook, we seamlessly integrate a custom LangChain retriever with a Vespa app, leveraging Vespa's streaming mode to extract meaningful context from PDF documents.</p> <p>The workflow</p> <ul> <li>Define and deploy a Vespa application package using PyVespa.</li> <li>Utilize LangChain PDF Loaders to download and parse PDF files.</li> <li>Leverage LangChain Document Transformers to convert each PDF page into multiple text chunks.</li> <li>Feed the transformer representation to the running Vespa instance</li> <li>Employ Vespa's built-in embedder functionality (using an open-source embedding model) for embedding the text chunks per page, resulting in a multi-vector representation.</li> <li>Develop a custom Retriever to enable seamless retrieval for any unstructured text query.</li> </ul> <p></p> <p></p> <p>Let's get started! First, install dependencies:</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#sample-data","title":"Sample data\u00b6","text":"<p>We love ColBERT, so we'll use a few COlBERT related papers as examples of PDFs in this notebook.</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#processing-pdfs-with-langchain","title":"Processing PDFs with LangChain\u00b6","text":"<p>LangChain has a rich set of document loaders that can be used to load and process various file formats. In this notebook, we use the PyPDFLoader.</p> <p>We also want to split the extracted text into chunks using a text splitter. Most text embedding models have limited input lengths (typically less than 512 language model tokens, so splitting the text into multiple chunks that fits into the context limit of the embedding model is a common strategy.</p> <p>For embedding text data, models based on the Transformer architecture have become the de facto standard. A challenge with Transformer-based models is their input length limitation due to the quadratic self-attention computational complexity. For example, a popular open-source text embedding model like e5 has an absolute maximum input length of 512 wordpiece tokens. In addition to the technical limitation, trying to fit more tokens than used during fine-tuning of the model will impact the quality of the vector representation.</p> <p>One can view text embedding encoding as a lossy compression technique, where variable-length texts are compressed into a fixed dimensional vector representation.</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Now, we can also query our data. With streaming mode, we must pass the <code>groupname</code> parameter, or the request will fail with an error.</p> <p>The query request uses the Vespa Query API and the <code>Vespa.query()</code> function supports passing any of the Vespa query API parameters.</p> <p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> </ul> <p>Sample query request for <code>why is colbert effective?</code> for the user <code>bergum@vespa.ai</code>:</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#langchain-retriever","title":"LangChain Retriever\u00b6","text":"<p>We use the LangChain Retriever interface so that we can connect our Vespa app with the flexibility and power of the LangChain LLM framework.</p> <p>A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.</p> <p>The retriever interface fits perfectly with Vespa, as Vespa can support a wide range of features and ways to retrieve and rank content. The following implements a custom retriever <code>VespaStreamingHybridRetriever</code> that takes the following arguments:</p> <ul> <li><code>app:Vespa</code> The Vespa application we retrieve from. This could be a Vespa Cloud instance or a local instance, for example running on a laptop.</li> <li><code>user:str</code> The user that that we want to retrieve for, this argument maps to the Vespa streaming mode groupname parameter</li> <li><code>pages:int</code> The target number of PDF pages we want to retrieve for a given query</li> <li><code>chunks_per_page</code> The is the target number of relevant text chunks that are associated with the page</li> <li><code>chunk_similarity_threshold</code> - The chunk similarity threshold, only chunks with a similarity above this threshold</li> </ul> <p>The core idea is to retrieve pages using maximum chunk similarity as the initial scoring function, then consider other chunks on the same page potentially relevant.</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#rag","title":"RAG\u00b6","text":""},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#interact-with-the-chain","title":"Interact with the chain\u00b6","text":"<p>Now, we can start asking questions using the <code>chain</code> define above.</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#summary","title":"Summary\u00b6","text":"<p>Vespa\u2019s streaming mode is a game-changer, enabling the creation of highly cost-effective RAG applications for naturally partitioned data.</p> <p>In this notebook, we delved into the hands-on application of LangChain, leveraging document loaders and transformers. Finally, we showcased a custom LangChain retriever that connected all the functionality of LangChain with Vespa.</p> <p>For those interested in learning more about Vespa, join the Vespa community on Slack to exchange ideas, seek assistance, or stay in the loop on the latest Vespa developments.</p> <p>We can now delete the cloud instance:</p>"},{"location":"examples/video_search_twelvelabs_cloud.html","title":"Video search twelvelabs cloud","text":"In\u00a0[15]: Copied! <pre>!python --version\n</pre> !python --version <pre>Python 3.11.11\n</pre> In\u00a0[16]: Copied! <pre>!pip3 install pyvespa vespacli twelvelabs pandas\n</pre> !pip3 install pyvespa vespacli twelvelabs pandas <pre>Requirement already satisfied: pyvespa in /usr/local/lib/python3.11/dist-packages (0.53.0)\nRequirement already satisfied: vespacli in /usr/local/lib/python3.11/dist-packages (8.478.26)\nRequirement already satisfied: twelvelabs in /usr/local/lib/python3.11/dist-packages (0.4.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pyvespa) (2.32.3)\nRequirement already satisfied: requests_toolbelt in /usr/local/lib/python3.11/dist-packages (from pyvespa) (1.0.0)\nRequirement already satisfied: docker in /usr/local/lib/python3.11/dist-packages (from pyvespa) (7.1.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pyvespa) (3.1.5)\nRequirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from pyvespa) (43.0.3)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from pyvespa) (3.11.12)\nRequirement already satisfied: httpx[http2] in /usr/local/lib/python3.11/dist-packages (from pyvespa) (0.28.1)\nRequirement already satisfied: tenacity&gt;=8.4.1 in /usr/local/lib/python3.11/dist-packages (from pyvespa) (9.0.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from pyvespa) (4.12.2)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from pyvespa) (2.8.2)\nRequirement already satisfied: fastcore&gt;=1.7.8 in /usr/local/lib/python3.11/dist-packages (from pyvespa) (1.7.29)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pyvespa) (5.3.1)\nRequirement already satisfied: pydantic&gt;=2.4.2 in /usr/local/lib/python3.11/dist-packages (from twelvelabs) (2.10.6)\nRequirement already satisfied: numpy&gt;=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastcore&gt;=1.7.8-&gt;pyvespa) (24.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx[http2]-&gt;pyvespa) (3.7.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx[http2]-&gt;pyvespa) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx[http2]-&gt;pyvespa) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx[http2]-&gt;pyvespa) (3.10)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*-&gt;httpx[http2]-&gt;pyvespa) (0.14.0)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic&gt;=2.4.2-&gt;twelvelabs) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic&gt;=2.4.2-&gt;twelvelabs) (2.27.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil-&gt;pyvespa) (1.17.0)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;pyvespa) (2.4.6)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;pyvespa) (1.3.2)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;pyvespa) (25.1.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;pyvespa) (1.5.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;pyvespa) (6.1.0)\nRequirement already satisfied: propcache&gt;=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;pyvespa) (0.2.1)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;pyvespa) (1.18.3)\nRequirement already satisfied: cffi&gt;=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography-&gt;pyvespa) (1.17.1)\nRequirement already satisfied: urllib3&gt;=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker-&gt;pyvespa) (2.3.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests-&gt;pyvespa) (3.4.1)\nRequirement already satisfied: h2&lt;5,&gt;=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]-&gt;pyvespa) (4.2.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2-&gt;pyvespa) (3.0.2)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi&gt;=1.12-&gt;cryptography-&gt;pyvespa) (2.22)\nRequirement already satisfied: hyperframe&lt;7,&gt;=6.1 in /usr/local/lib/python3.11/dist-packages (from h2&lt;5,&gt;=3-&gt;httpx[http2]-&gt;pyvespa) (6.1.0)\nRequirement already satisfied: hpack&lt;5,&gt;=4.1 in /usr/local/lib/python3.11/dist-packages (from h2&lt;5,&gt;=3-&gt;httpx[http2]-&gt;pyvespa) (4.1.0)\nRequirement already satisfied: sniffio&gt;=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio-&gt;httpx[http2]-&gt;pyvespa) (1.3.1)\n</pre> <p>Import all the required packages in this notebook.</p> In\u00a0[17]: Copied! <pre>import os\nimport hashlib\nimport json\n\nfrom vespa.package import (\n    ApplicationPackage,\n    Field,\n    Schema,\n    Document,\n    HNSW,\n    RankProfile,\n    FieldSet,\n    SecondPhaseRanking,\n    Function,\n)\n\nfrom vespa.deployment import VespaCloud\nfrom vespa.io import VespaResponse, VespaQueryResponse\n\nfrom twelvelabs import TwelveLabs\nfrom twelvelabs.models.embed import EmbeddingsTask\n\nimport pandas as pd\n\nfrom datetime import datetime\n</pre> import os import hashlib import json  from vespa.package import (     ApplicationPackage,     Field,     Schema,     Document,     HNSW,     RankProfile,     FieldSet,     SecondPhaseRanking,     Function, )  from vespa.deployment import VespaCloud from vespa.io import VespaResponse, VespaQueryResponse  from twelvelabs import TwelveLabs from twelvelabs.models.embed import EmbeddingsTask  import pandas as pd  from datetime import datetime In\u00a0[\u00a0]: Copied! <pre>TL_API_KEY = os.getenv(\"TL_API_KEY\") or input(\"Enter your TL_API key: \")\n</pre> TL_API_KEY = os.getenv(\"TL_API_KEY\") or input(\"Enter your TL_API key: \") In\u00a0[6]: Copied! <pre># Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n# Replace with your application name (does not need to exist yet)\napplication = \"videosearch\"\n</pre> # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\" # Replace with your application name (does not need to exist yet) application = \"videosearch\" In\u00a0[19]: Copied! <pre>VIDEO_URLs = [\n    \"https://ia801503.us.archive.org/27/items/hide-and-seek-with-giant-jenny/HnVideoEditor_2022_10_29_205557707.ia.mp4\",\n    \"https://ia601401.us.archive.org/1/items/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net.mp4\",\n    \"https://dn720401.ca.archive.org/0/items/mr-bean-the-animated-series-holiday-for-teddy/S2E12.ia.mp4\",\n]\n</pre> VIDEO_URLs = [     \"https://ia801503.us.archive.org/27/items/hide-and-seek-with-giant-jenny/HnVideoEditor_2022_10_29_205557707.ia.mp4\",     \"https://ia601401.us.archive.org/1/items/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net.mp4\",     \"https://dn720401.ca.archive.org/0/items/mr-bean-the-animated-series-holiday-for-teddy/S2E12.ia.mp4\", ] <p>In order to generate text on the videos, the prerequisite is to upload the videos and index them. Let's first create an index below:</p> In\u00a0[20]: Copied! <pre># Spin-up session\nclient = TwelveLabs(api_key=TL_API_KEY)\n\n# Generating Index Name\ntimestamp = int(datetime.now().timestamp())\nindex_name = \"Vespa_\" + str(timestamp)\n\n# Create Index\nprint(\"Creating Index:\" + index_name)\nindex = client.index.create(\n    name=index_name,\n    models=[\n        {\n            \"name\": \"pegasus1.2\",\n            \"options\": [\"visual\", \"audio\"],\n        }\n    ],\n    addons=[\"thumbnail\"],  # Optional\n)\nprint(f\"Created index: id={index.id} name={index.name} models={index.models}\")\n</pre> # Spin-up session client = TwelveLabs(api_key=TL_API_KEY)  # Generating Index Name timestamp = int(datetime.now().timestamp()) index_name = \"Vespa_\" + str(timestamp)  # Create Index print(\"Creating Index:\" + index_name) index = client.index.create(     name=index_name,     models=[         {             \"name\": \"pegasus1.2\",             \"options\": [\"visual\", \"audio\"],         }     ],     addons=[\"thumbnail\"],  # Optional ) print(f\"Created index: id={index.id} name={index.name} models={index.models}\") <pre>Creating Index:Vespa_1739986655\nCreated index: id=67b616dfc82670193cb59a05 name=Vespa_1739986655 models=root=[Model(name='pegasus1.2', options=['visual', 'audio'], addons=None, finetuned=False)]\n</pre> <p>We can now upload the videos:</p> In\u00a0[21]: Copied! <pre># Capturing index id for upload\nindex_id = index.id\n\n\ndef on_task_update(task: EmbeddingsTask):\n    print(f\"  Status={task.status}\")\n\n\nfor video_url in VIDEO_URLs:\n    # Create a video indexing task\n    task = client.task.create(index_id=index_id, url=video_url)\n    print(f\"Task created successfully! Task ID: {task.id}\")\n    status = task.wait_for_done(sleep_interval=10, callback=on_task_update)\n    print(f\"Indexing done: {status}\")\n    if task.status != \"ready\":\n        raise RuntimeError(f\"Indexing failed with status {task.status}\")\n    print(\n        f\"Uploaded {video_url}. The unique identifer of your video is {task.video_id}.\"\n    )\n</pre> # Capturing index id for upload index_id = index.id   def on_task_update(task: EmbeddingsTask):     print(f\"  Status={task.status}\")   for video_url in VIDEO_URLs:     # Create a video indexing task     task = client.task.create(index_id=index_id, url=video_url)     print(f\"Task created successfully! Task ID: {task.id}\")     status = task.wait_for_done(sleep_interval=10, callback=on_task_update)     print(f\"Indexing done: {status}\")     if task.status != \"ready\":         raise RuntimeError(f\"Indexing failed with status {task.status}\")     print(         f\"Uploaded {video_url}. The unique identifer of your video is {task.video_id}.\"     ) <pre>Task created successfully! Task ID: 67b616e4c82670193cb59a06\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=ready\nIndexing done: Task(id='67b616e4c82670193cb59a06', created_at='2025-02-19T17:37:40.286Z', updated_at='2025-02-19T17:37:40.286Z', index_id='67b616dfc82670193cb59a05', video_id='67b616e551e07a2910a9b956', status='ready', system_metadata={'filename': 'HnVideoEditor_2022_10_29_205557707.ia', 'duration': 221.9666671, 'width': 854, 'height': 480}, hls=None)\nUploaded https://ia801503.us.archive.org/27/items/hide-and-seek-with-giant-jenny/HnVideoEditor_2022_10_29_205557707.ia.mp4. The unique identifer of your video is 67b616e551e07a2910a9b956.\nTask created successfully! Task ID: 67b6179dc82670193cb59a0a\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=ready\nIndexing done: Task(id='67b6179dc82670193cb59a0a', created_at='2025-02-19T17:40:45.027Z', updated_at='2025-02-19T17:40:45.027Z', index_id='67b616dfc82670193cb59a05', video_id='67b617b6589f15770cd94602', status='ready', system_metadata={'filename': 'twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net', 'duration': 1448.8000001, 'width': 640, 'height': 480}, hls=None)\nUploaded https://ia601401.us.archive.org/1/items/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net.mp4. The unique identifer of your video is 67b617b6589f15770cd94602.\nTask created successfully! Task ID: 67b618c0c82670193cb59a10\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=ready\nIndexing done: Task(id='67b618c0c82670193cb59a10', created_at='2025-02-19T17:45:36.117Z', updated_at='2025-02-19T17:45:36.117Z', index_id='67b616dfc82670193cb59a05', video_id='67b618c2589f15770cd94603', status='ready', system_metadata={'filename': 'S2E12.ia', 'duration': 659.9200001, 'width': 854, 'height': 480}, hls=None)\nUploaded https://dn720401.ca.archive.org/0/items/mr-bean-the-animated-series-holiday-for-teddy/S2E12.ia.mp4. The unique identifer of your video is 67b618c2589f15770cd94603.\n</pre> <p>Now that the videos have been uploaded, we can generate the keywords, and summaries on the videos below. You will notice on the output that the video uploaded last is the one that is processed first in this stage. This matters since we store other attributes on the videos on arrays (eg URLs, Titles).</p> In\u00a0[22]: Copied! <pre>client = TwelveLabs(api_key=TL_API_KEY)\n\nsummaries = []\nkeywords_array = []\n\n# Get all videos in an Index\nvideos = client.index.video.list(index_id)\nfor video in videos:\n    print(f\"Generating text for {video.id}\")\n\n    res = client.generate.summarize(\n        video_id=video.id,\n        type=\"summary\",\n        prompt=\"Generate an abstract of the video serving as metadata on the video, up to five sentences.\",\n    )\n    print(f\"Summary: {res.summary}\")\n    summaries.append(res.summary)\n\n    keywords = client.generate.text(\n        video_id=video.id,\n        prompt=\"Based on this video, I want to generate five keywords for SEO (Search Engine Optimization). Provide just the keywords as a comma delimited list without any additional text.\",\n    )\n    print(f\"Open-ended Text: {keywords.data}\")\n    keywords_array.append(keywords.data)\n</pre> client = TwelveLabs(api_key=TL_API_KEY)  summaries = [] keywords_array = []  # Get all videos in an Index videos = client.index.video.list(index_id) for video in videos:     print(f\"Generating text for {video.id}\")      res = client.generate.summarize(         video_id=video.id,         type=\"summary\",         prompt=\"Generate an abstract of the video serving as metadata on the video, up to five sentences.\",     )     print(f\"Summary: {res.summary}\")     summaries.append(res.summary)      keywords = client.generate.text(         video_id=video.id,         prompt=\"Based on this video, I want to generate five keywords for SEO (Search Engine Optimization). Provide just the keywords as a comma delimited list without any additional text.\",     )     print(f\"Open-ended Text: {keywords.data}\")     keywords_array.append(keywords.data) <pre>Generating text for 67b618c2589f15770cd94603\nSummary: The video showcases a series of comedic scenes featuring Mr. Bean, who embarks on a holiday at Seaview Hotel. Throughout his stay, he engages in various mishaps, including accidentally starting a car and exchanging toys with a little girl. At the beach, he interacts with a family and participates in a dodgem car game, where he manages to take possession of a teddy bear, causing confusion and amusement. The video concludes with Mr. Bean back at his room, reflecting on his holiday adventures through photographs, highlighting his characteristic blend of humor and awkwardness.\nOpen-ended Text: MrBean, TeddyBear, Beach, BumperCars, Holiday\nGenerating text for 67b617b6589f15770cd94602\nSummary: The video is an animated adaptation of \"A Visit from St. Nicholas,\" commonly known as \"The Night Before Christmas,\" narrated and sung by Joel Grey. It features a town where the residents, including a clockmaker named Joshua Trundle and his family, are troubled by Santa's absence due to a critical letter published in the local newspaper. The story unfolds with the clockmaker's son, Albert, realizing his mistake and attempting to fix a malfunctioning clock in the town hall that was meant to welcome Santa. Despite initial setbacks, the community's efforts eventually lead to Santa's arrival, restoring joy and belief in the magic of Christmas. The video concludes with Santa Claus descending through chimneys to deliver gifts, symbolizing the triumph of hope and the spirit of the holiday.\nOpen-ended Text: Christmas Eve, Santa Claus, Clockmaker, Snowy Night, Mouse Characters\nGenerating text for 67b616e551e07a2910a9b956\nSummary: The video showcases a series of animated scenes featuring a panda and three cartoon wolves engaging in various activities, from watching TV to building a miniature cardboard town. The wolves, along with a pink dog and a green alien, encounter unexpected situations such as a giant fox descending upon them and a chase involving toy cars and a convertible. The narrative culminates in a heartwarming reunion under a bridge, where the characters express gratitude for their day's adventures. Throughout the video, the characters display a range of emotions and interactions, highlighting themes of friendship and teamwork.\nOpen-ended Text: Cartoon, Wolves, Hide-and-Seek, Cardboard City, Alien\n</pre> <p>We need to store the titles of the videos as an additional attribute.</p> In\u00a0[23]: Copied! <pre># Creating array with titles\ntitles = [\n    \"Mr. Bean the Animated Series Holiday for Teddy\",\n    \"Twas the night before Christmas\",\n    \"Hide and Seek with Giant Jenny\",\n]\n</pre> # Creating array with titles titles = [     \"Mr. Bean the Animated Series Holiday for Teddy\",     \"Twas the night before Christmas\",     \"Hide and Seek with Giant Jenny\", ] In\u00a0[24]: Copied! <pre>client = TwelveLabs(api_key=TL_API_KEY)\n\n# Initialize an array to store the task IDs as strings\ntask_ids = []\n\nfor url in VIDEO_URLs:\n    task = client.embed.task.create(model_name=\"Marengo-retrieval-2.7\", video_url=url)\n    print(\n        f\"Created task: id={task.id} model_name={task.model_name} status={task.status}\"\n    )\n    # Append the task ID to the array\n    task_ids.append(str(task.id))\n    status = task.wait_for_done(sleep_interval=10, callback=on_task_update)\n    print(f\"Embedding done: {status}\")\n    if task.status != \"ready\":\n        raise RuntimeError(f\"Embedding failed with status {task.status}\")\n</pre> client = TwelveLabs(api_key=TL_API_KEY)  # Initialize an array to store the task IDs as strings task_ids = []  for url in VIDEO_URLs:     task = client.embed.task.create(model_name=\"Marengo-retrieval-2.7\", video_url=url)     print(         f\"Created task: id={task.id} model_name={task.model_name} status={task.status}\"     )     # Append the task ID to the array     task_ids.append(str(task.id))     status = task.wait_for_done(sleep_interval=10, callback=on_task_update)     print(f\"Embedding done: {status}\")     if task.status != \"ready\":         raise RuntimeError(f\"Embedding failed with status {task.status}\") <pre>Created task: id=67b61b3567ce2ae76ec6b3eb model_name=Marengo-retrieval-2.7 status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=ready\nEmbedding done: ready\nCreated task: id=67b61b7e67ce2ae76ec6b3f4 model_name=Marengo-retrieval-2.7 status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=ready\nEmbedding done: ready\nCreated task: id=67b61c0c67ce2ae76ec6b3fa model_name=Marengo-retrieval-2.7 status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=ready\nEmbedding done: ready\n</pre> In\u00a0[25]: Copied! <pre># Spin-up session\nclient = TwelveLabs(api_key=TL_API_KEY)\n\n# Initialize an array to store the task objects directly\ntasks = []\n\nfor task_id in task_ids:\n    # Retrieve the task\n    task = client.embed.task.retrieve(task_id)\n    tasks.append(task)\n\n    # Print task details\n    print(f\"Task ID: {task.id}\")\n    print(f\"Status: {task.status}\")\n</pre> # Spin-up session client = TwelveLabs(api_key=TL_API_KEY)  # Initialize an array to store the task objects directly tasks = []  for task_id in task_ids:     # Retrieve the task     task = client.embed.task.retrieve(task_id)     tasks.append(task)      # Print task details     print(f\"Task ID: {task.id}\")     print(f\"Status: {task.status}\") <pre>Task ID: 67b61b3567ce2ae76ec6b3eb\nStatus: ready\nTask ID: 67b61b7e67ce2ae76ec6b3f4\nStatus: ready\nTask ID: 67b61c0c67ce2ae76ec6b3fa\nStatus: ready\n</pre> <p>We can now review the output structure of the first segment for each one of these videos. This output will help us define the schema to store the embeddings in Vespa in the second part of this notebook.</p> <p>From looking at this output, the video has been embedded into chunks of 6 seconds each (default configurable value in the Embed API). Each embedding has a float vector of dimension 1024.</p> <p>The number of segments generated vary per video, based on the length of the videos ranging from 37 to 242 segments.</p> In\u00a0[26]: Copied! <pre>for task in tasks:\n    print(task.id)\n    # Display data types of each field\n    for key, value in task.video_embedding.segments[0]:\n        if isinstance(value, list):\n            print(\n                f\"{key}: list of size {len(value)} (truncated to 5 items): {value[:5]} \"\n            )\n        else:\n            print(f\"{key}: {type(value).__name__} : {value}\")\n    print(f\"Total Number of segments: {len(task.video_embedding.segments)}\")\n</pre> for task in tasks:     print(task.id)     # Display data types of each field     for key, value in task.video_embedding.segments[0]:         if isinstance(value, list):             print(                 f\"{key}: list of size {len(value)} (truncated to 5 items): {value[:5]} \"             )         else:             print(f\"{key}: {type(value).__name__} : {value}\")     print(f\"Total Number of segments: {len(task.video_embedding.segments)}\") <pre>67b61b3567ce2ae76ec6b3eb\nstart_offset_sec: float : 0.0\nend_offset_sec: float : 6.0\nembedding_scope: str : clip\nembeddings_float: list of size 1024 (truncated to 5 items): [0.030361895, 0.008698823, -0.0048321243, -0.019013105, -0.011488311] \nTotal Number of segments: 37\n67b61b7e67ce2ae76ec6b3f4\nstart_offset_sec: float : 0.0\nend_offset_sec: float : 6.0\nembedding_scope: str : clip\nembeddings_float: list of size 1024 (truncated to 5 items): [0.024328815, -0.0035867887, 0.016065866, 0.02501548, 0.007778642] \nTotal Number of segments: 242\n67b61c0c67ce2ae76ec6b3fa\nstart_offset_sec: float : 0.0\nend_offset_sec: float : 6.0\nembedding_scope: str : clip\nembeddings_float: list of size 1024 (truncated to 5 items): [0.04080625, 0.0086980555, 0.00096186635, -0.00607, -0.020250283] \nTotal Number of segments: 110\n</pre> In\u00a0[27]: Copied! <pre>videos_schema = Schema(\n    name=\"videos\",\n    document=Document(\n        fields=[\n            Field(name=\"video_url\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"index\", \"summary\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"keywords\",\n                type=\"string\",\n                indexing=[\"index\", \"summary\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"video_summary\",\n                type=\"string\",\n                indexing=[\"index\", \"summary\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"embedding_scope\", type=\"string\", indexing=[\"attribute\", \"summary\"]\n            ),\n            Field(\n                name=\"start_offset_sec\",\n                type=\"array&lt;float&gt;\",\n                indexing=[\"attribute\", \"summary\"],\n            ),\n            Field(\n                name=\"end_offset_sec\",\n                type=\"array&lt;float&gt;\",\n                indexing=[\"attribute\", \"summary\"],\n            ),\n            Field(\n                name=\"embeddings\",\n                type=\"tensor&lt;float&gt;(p{},x[1024])\",\n                indexing=[\"index\", \"attribute\"],\n                ann=HNSW(distance_metric=\"angular\"),\n            ),\n        ]\n    ),\n)\n\nfieldsets = (\n    [\n        FieldSet(\n            name=\"default\",\n            fields=[\"title\", \"keywords\", \"video_summary\"],\n        ),\n    ],\n)\n\nmapfunctions = [\n    Function(\n        name=\"similarities\",\n        expression=\"\"\"\n                      sum(\n                          query(q) * attribute(embeddings), x\n                          )\n                      \"\"\",\n    ),\n    Function(\n        name=\"bm25_score\",\n        expression=\"bm25(title) + bm25(keywords) + bm25(video_summary)\",\n    ),\n]\n\nsemantic_rankprofile = RankProfile(\n    name=\"hybrid\",\n    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[1024])\")],\n    first_phase=\"bm25_score\",\n    second_phase=SecondPhaseRanking(\n        expression=\"closeness(field, embeddings)\", rerank_count=10\n    ),\n    match_features=[\"closest(embeddings)\"],\n    summary_features=[\"similarities\"],\n    functions=mapfunctions,\n)\n\nvideos_schema.add_rank_profile(semantic_rankprofile)\n</pre> videos_schema = Schema(     name=\"videos\",     document=Document(         fields=[             Field(name=\"video_url\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"index\", \"summary\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"keywords\",                 type=\"string\",                 indexing=[\"index\", \"summary\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"video_summary\",                 type=\"string\",                 indexing=[\"index\", \"summary\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"embedding_scope\", type=\"string\", indexing=[\"attribute\", \"summary\"]             ),             Field(                 name=\"start_offset_sec\",                 type=\"array\",                 indexing=[\"attribute\", \"summary\"],             ),             Field(                 name=\"end_offset_sec\",                 type=\"array\",                 indexing=[\"attribute\", \"summary\"],             ),             Field(                 name=\"embeddings\",                 type=\"tensor(p{},x[1024])\",                 indexing=[\"index\", \"attribute\"],                 ann=HNSW(distance_metric=\"angular\"),             ),         ]     ), )  fieldsets = (     [         FieldSet(             name=\"default\",             fields=[\"title\", \"keywords\", \"video_summary\"],         ),     ], )  mapfunctions = [     Function(         name=\"similarities\",         expression=\"\"\"                       sum(                           query(q) * attribute(embeddings), x                           )                       \"\"\",     ),     Function(         name=\"bm25_score\",         expression=\"bm25(title) + bm25(keywords) + bm25(video_summary)\",     ), ]  semantic_rankprofile = RankProfile(     name=\"hybrid\",     inputs=[(\"query(q)\", \"tensor(x[1024])\")],     first_phase=\"bm25_score\",     second_phase=SecondPhaseRanking(         expression=\"closeness(field, embeddings)\", rerank_count=10     ),     match_features=[\"closest(embeddings)\"],     summary_features=[\"similarities\"],     functions=mapfunctions, )  videos_schema.add_rank_profile(semantic_rankprofile) <p>We can now create the package based on the previous schema</p> In\u00a0[28]: Copied! <pre># Create the Vespa application package\npackage = ApplicationPackage(name=application, schema=[videos_schema])\n</pre> # Create the Vespa application package package = ApplicationPackage(name=application, schema=[videos_schema]) In\u00a0[29]: Copied! <pre>vespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=application,\n    application_package=package,\n    key_content=os.getenv(\"VESPA_TEAM_API_KEY\", None),\n)\n</pre> vespa_cloud = VespaCloud(     tenant=tenant_name,     application=application,     application_package=package,     key_content=os.getenv(\"VESPA_TEAM_API_KEY\", None), ) <pre>Setting application...\nRunning: vespa config set application vespa-presales.videosearch\nSetting target cloud...\nRunning: vespa config set target cloud\n\nNo api-key found for control plane access. Using access token.\nNo auth.json found. Please authenticate.\nYour Device Confirmation code is: QVNV-TRSK\nAutomatically open confirmation page in your default browser? [Y/n] Y\nY\nOpened link in your browser: https://login.console.vespa-cloud.com/activate?user_code=QVNV-TRSK\n/usr/bin/xdg-open: 882: www-browser: not found\n/usr/bin/xdg-open: 882: links2: not found\n/usr/bin/xdg-open: 882: elinks: not found\n/usr/bin/xdg-open: 882: links: not found\n/usr/bin/xdg-open: 882: lynx: not found\n/usr/bin/xdg-open: 882: w3m: not found\nxdg-open: no method available for opening 'https://login.console.vespa-cloud.com/activate?user_code=QVNV-TRSK'\nCouldn't open the URL, please do it manually\nWaiting for login to complete in browser ... done\nWarning: Could not store the refresh token locally. You may need to login again once your access token expires\nSuccess: Logged in\n auth.json created at /root/.vespa/auth.json\nSuccessfully obtained access token for control plane access.\nCertificate and key not found in /content/.vespa or /root/.vespa/vespa-presales.videosearch.default: Creating new cert/key pair with vespa CLI.\nGenerating certificate and key...\nRunning: vespa auth cert -N\nSuccess: Certificate written to '/root/.vespa/vespa-presales.videosearch.default/data-plane-public-cert.pem'\nSuccess: Private key written to '/root/.vespa/vespa-presales.videosearch.default/data-plane-private-key.pem'\n\n</pre> In\u00a0[30]: Copied! <pre>app = vespa_cloud.deploy()\n</pre> app = vespa_cloud.deploy() <pre>Deployment started in run 1 of dev-aws-us-east-1c for vespa-presales.videosearch. This may take a few minutes the first time.\nINFO    [18:05:07]  Deploying platform version 8.482.31 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [18:05:08]  Using CA signed certificate version 1\nINFO    [18:05:08]  Using 1 nodes in container cluster 'videosearch_container'\nINFO    [18:05:11]  Session 2384 for tenant 'vespa-presales' prepared and activated.\nINFO    [18:05:33]  ######## Details for all nodes ########\nINFO    [18:05:42]  h113694g.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [18:05:42]  --- platform vespa/cloud-tenant-rhel8:8.482.31\nINFO    [18:05:42]  --- container-clustercontroller on port 19050 has not started \nINFO    [18:05:42]  --- metricsproxy-container on port 19092 has not started \nINFO    [18:05:42]  h113694b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [18:05:42]  --- platform vespa/cloud-tenant-rhel8:8.482.31\nINFO    [18:05:42]  --- logserver-container on port 4080 has not started \nINFO    [18:05:42]  --- metricsproxy-container on port 19092 has not started \nINFO    [18:05:42]  h113669a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [18:05:42]  --- platform vespa/cloud-tenant-rhel8:8.482.31\nINFO    [18:05:42]  --- storagenode on port 19102 has not started \nINFO    [18:05:42]  --- searchnode on port 19107 has not started \nINFO    [18:05:42]  --- distributor on port 19111 has not started \nINFO    [18:05:42]  --- metricsproxy-container on port 19092 has not started \nINFO    [18:05:42]  h113963a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [18:05:42]  --- platform vespa/cloud-tenant-rhel8:8.482.31\nINFO    [18:05:42]  --- container on port 4080 has not started \nINFO    [18:05:42]  --- metricsproxy-container on port 19092 has not started \nINFO    [18:07:10]  Waiting for convergence of 10 services across 4 nodes\nINFO    [18:07:10]  3 nodes booting\nINFO    [18:07:10]  10 application services still deploying\nDEBUG   [18:07:17]  h113694b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nDEBUG   [18:07:17]  --- platform vespa/cloud-tenant-rhel8:8.482.31\nDEBUG   [18:07:17]  --- logserver-container on port 4080 has not started \nDEBUG   [18:07:17]  --- metricsproxy-container on port 19092 has not started \nDEBUG   [18:07:17]  h113669a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nDEBUG   [18:07:17]  --- platform vespa/cloud-tenant-rhel8:8.482.31\nDEBUG   [18:07:17]  --- storagenode on port 19102 has not started \nDEBUG   [18:07:17]  --- searchnode on port 19107 has not started \nDEBUG   [18:07:17]  --- distributor on port 19111 has not started \nDEBUG   [18:07:17]  --- metricsproxy-container on port 19092 has not started \nDEBUG   [18:07:17]  h113963a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nDEBUG   [18:07:17]  --- platform vespa/cloud-tenant-rhel8:8.482.31\nDEBUG   [18:07:17]  --- container on port 4080 has not started \nDEBUG   [18:07:17]  --- metricsproxy-container on port 19092 has not started \nDEBUG   [18:07:17]  h113694g.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nDEBUG   [18:07:17]  --- platform vespa/cloud-tenant-rhel8:8.482.31\nDEBUG   [18:07:17]  --- container-clustercontroller on port 19050 has not started \nDEBUG   [18:07:17]  --- metricsproxy-container on port 19092 has not started \nINFO    [18:08:06]  Found endpoints:\nINFO    [18:08:06]  - dev.aws-us-east-1c\nINFO    [18:08:06]   |-- https://aefbf207.f5d60452.z.vespa-app.cloud/ (cluster 'videosearch_container')\nINFO    [18:08:22]  Deployment complete!\nOnly region: aws-us-east-1c available in dev environment.\nFound mtls endpoint for videosearch_container\nURL: https://aefbf207.f5d60452.z.vespa-app.cloud/\nApplication is up!\n</pre> In\u00a0[31]: Copied! <pre># Initialize a list to store Vespa feed documents\nvespa_feed = []\n\n# Need to reverse VIDEO_URLS as keywords/summaries generated in reverse order\nVIDEO_URLs.reverse()\n\n# Iterate through each task and corresponding metadata\nfor i, task in enumerate(tasks):\n    video_url = VIDEO_URLs[i]\n    title = titles[i]\n    keywords = keywords_array[i]\n    summary = summaries[i]\n\n    start_offsets = []  # Reset for each video\n    end_offsets = []  # Reset for each video\n    embeddings = {}  # Reset for each video\n\n    # Iterate through the video embedding segments\n    for index, segment in enumerate(task.video_embedding.segments):\n        # Append start and end offsets as floats\n        start_offsets.append(float(segment.start_offset_sec))\n        end_offsets.append(float(segment.end_offset_sec))\n\n        # Add embedding to a multi-dimensional dictionary with index as the key\n        embeddings[str(index)] = list(map(float, segment.embeddings_float))\n\n    # Create Vespa document for each task\n    for segment in task.video_embedding.segments:\n        start_offset_sec = segment.start_offset_sec\n        end_offset_sec = segment.end_offset_sec\n        embedding = list(map(float, segment.embeddings_float))\n\n        # Create a unique ID by hashing the URL and segment index\n        id_hash = hashlib.md5(f\"{video_url}_{index}\".encode()).hexdigest()\n\n        document = {\n            \"id\": id_hash,\n            \"fields\": {\n                \"video_url\": video_url,\n                \"title\": title,\n                \"keywords\": keywords,\n                \"video_summary\": summary,\n                \"embedding_scope\": segment.embedding_scope,\n                \"start_offset_sec\": start_offsets,\n                \"end_offset_sec\": end_offsets,\n                \"embeddings\": embeddings,\n            },\n        }\n    vespa_feed.append(document)\n</pre> # Initialize a list to store Vespa feed documents vespa_feed = []  # Need to reverse VIDEO_URLS as keywords/summaries generated in reverse order VIDEO_URLs.reverse()  # Iterate through each task and corresponding metadata for i, task in enumerate(tasks):     video_url = VIDEO_URLs[i]     title = titles[i]     keywords = keywords_array[i]     summary = summaries[i]      start_offsets = []  # Reset for each video     end_offsets = []  # Reset for each video     embeddings = {}  # Reset for each video      # Iterate through the video embedding segments     for index, segment in enumerate(task.video_embedding.segments):         # Append start and end offsets as floats         start_offsets.append(float(segment.start_offset_sec))         end_offsets.append(float(segment.end_offset_sec))          # Add embedding to a multi-dimensional dictionary with index as the key         embeddings[str(index)] = list(map(float, segment.embeddings_float))      # Create Vespa document for each task     for segment in task.video_embedding.segments:         start_offset_sec = segment.start_offset_sec         end_offset_sec = segment.end_offset_sec         embedding = list(map(float, segment.embeddings_float))          # Create a unique ID by hashing the URL and segment index         id_hash = hashlib.md5(f\"{video_url}_{index}\".encode()).hexdigest()          document = {             \"id\": id_hash,             \"fields\": {                 \"video_url\": video_url,                 \"title\": title,                 \"keywords\": keywords,                 \"video_summary\": summary,                 \"embedding_scope\": segment.embedding_scope,                 \"start_offset_sec\": start_offsets,                 \"end_offset_sec\": end_offsets,                 \"embeddings\": embeddings,             },         }     vespa_feed.append(document) <p>We can quickly validate the number of the number of documents created (one for each video), and visually check the first record.</p> In\u00a0[32]: Copied! <pre># Print Vespa feed size and an example\nprint(f\"Total documents created: {len(vespa_feed)}\")\n</pre> # Print Vespa feed size and an example print(f\"Total documents created: {len(vespa_feed)}\") <pre>Total documents created: 3\n</pre> In\u00a0[33]: Copied! <pre># The positional index of the document\ni = 0\n\n# Iterate through the first 3 embeddings in vespa_feed\nfor i in range(\n    min(3, len(vespa_feed))\n):  # Ensure we don't exceed the length of vespa_feed\n    # Limit the embedding to the first 3 keys and first 5 values for each key\n    embedding = vespa_feed[i][\"fields\"][\"embeddings\"]\n    embedding_sample = {key: values[:3] for key, values in list(embedding.items())[:3]}\n\n# Beautify and print the first document with only the first 5 embedding values\npretty_json = json.dumps(\n    {\n        \"id\": vespa_feed[i][\"id\"],\n        \"fields\": {\n            \"video_url\": vespa_feed[i][\"fields\"][\"video_url\"],\n            \"title\": vespa_feed[i][\"fields\"][\"title\"],\n            \"keywords\": vespa_feed[i][\"fields\"][\"keywords\"],\n            \"video_summary\": vespa_feed[i][\"fields\"][\"video_summary\"],\n            \"embedding_scope\": vespa_feed[i][\"fields\"][\"embedding_scope\"],\n            \"start_offset_sec\": vespa_feed[i][\"fields\"][\"start_offset_sec\"][:3],\n            \"end_offset_sec\": vespa_feed[i][\"fields\"][\"end_offset_sec\"][:3],\n            \"embedding\": embedding_sample,\n        },\n    },\n    indent=4,\n)\n\nprint(pretty_json)\n</pre> # The positional index of the document i = 0  # Iterate through the first 3 embeddings in vespa_feed for i in range(     min(3, len(vespa_feed)) ):  # Ensure we don't exceed the length of vespa_feed     # Limit the embedding to the first 3 keys and first 5 values for each key     embedding = vespa_feed[i][\"fields\"][\"embeddings\"]     embedding_sample = {key: values[:3] for key, values in list(embedding.items())[:3]}  # Beautify and print the first document with only the first 5 embedding values pretty_json = json.dumps(     {         \"id\": vespa_feed[i][\"id\"],         \"fields\": {             \"video_url\": vespa_feed[i][\"fields\"][\"video_url\"],             \"title\": vespa_feed[i][\"fields\"][\"title\"],             \"keywords\": vespa_feed[i][\"fields\"][\"keywords\"],             \"video_summary\": vespa_feed[i][\"fields\"][\"video_summary\"],             \"embedding_scope\": vespa_feed[i][\"fields\"][\"embedding_scope\"],             \"start_offset_sec\": vespa_feed[i][\"fields\"][\"start_offset_sec\"][:3],             \"end_offset_sec\": vespa_feed[i][\"fields\"][\"end_offset_sec\"][:3],             \"embedding\": embedding_sample,         },     },     indent=4, )  print(pretty_json) <pre>{\n    \"id\": \"0b1fc68a17391fb58102a539ed290d27\",\n    \"fields\": {\n        \"video_url\": \"https://ia801503.us.archive.org/27/items/hide-and-seek-with-giant-jenny/HnVideoEditor_2022_10_29_205557707.ia.mp4\",\n        \"title\": \"Hide and Seek with Giant Jenny\",\n        \"keywords\": \"Cartoon, Wolves, Hide-and-Seek, Cardboard City, Alien\",\n        \"video_summary\": \"The video showcases a series of animated scenes featuring a panda and three cartoon wolves engaging in various activities, from watching TV to building a miniature cardboard town. The wolves, along with a pink dog and a green alien, encounter unexpected situations such as a giant fox descending upon them and a chase involving toy cars and a convertible. The narrative culminates in a heartwarming reunion under a bridge, where the characters express gratitude for their day's adventures. Throughout the video, the characters display a range of emotions and interactions, highlighting themes of friendship and teamwork.\",\n        \"embedding_scope\": \"clip\",\n        \"start_offset_sec\": [\n            0.0,\n            6.0,\n            12.0\n        ],\n        \"end_offset_sec\": [\n            6.0,\n            12.0,\n            18.0\n        ],\n        \"embedding\": {\n            \"0\": [\n                0.04080625,\n                0.0086980555,\n                0.00096186635\n            ],\n            \"1\": [\n                0.05161131,\n                -0.0063618324,\n                -0.008135624\n            ],\n            \"2\": [\n                0.050463274,\n                0.0006376326,\n                -0.010785032\n            ]\n        }\n    }\n}\n</pre> <p>Now we can feed to Vespa using <code>feed_iterable</code> which accepts any <code>Iterable</code> and an optional callback function where we can check the outcome of each operation.</p> In\u00a0[34]: Copied! <pre>def callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n\n\n# Feed data into Vespa synchronously\napp.feed_iterable(vespa_feed, schema=\"videos\", callback=callback)\n</pre> def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         )   # Feed data into Vespa synchronously app.feed_iterable(vespa_feed, schema=\"videos\", callback=callback) In\u00a0[47]: Copied! <pre>client = TwelveLabs(api_key=TL_API_KEY)\nuser_query = \"Santa Claus on his sleigh\"\n\nres = client.embed.create(\n    model_name=\"Marengo-retrieval-2.7\",\n    text=user_query,\n)\n\nprint(\"Created a text embedding\")\nprint(f\" Model: {res.model_name}\")\nif res.text_embedding is not None and res.text_embedding.segments is not None:\n    q_embedding = res.text_embedding.segments[0].embeddings_float\n    print(f\" Embedding Dimension: {len(q_embedding)}\")\n    print(f\" Sample 5 values from array: {q_embedding[:5]}\")\n</pre> client = TwelveLabs(api_key=TL_API_KEY) user_query = \"Santa Claus on his sleigh\"  res = client.embed.create(     model_name=\"Marengo-retrieval-2.7\",     text=user_query, )  print(\"Created a text embedding\") print(f\" Model: {res.model_name}\") if res.text_embedding is not None and res.text_embedding.segments is not None:     q_embedding = res.text_embedding.segments[0].embeddings_float     print(f\" Embedding Dimension: {len(q_embedding)}\")     print(f\" Sample 5 values from array: {q_embedding[:5]}\") <pre>Created a text embedding\n Model: Marengo-retrieval-2.7\n Embedding Dimension: 1024\n Sample 5 values from array: [-0.018066406, -0.0065307617, 0.05859375, -0.033447266, -0.02368164]\n</pre> <p>The following uses dense vector representations of the query embedding obtained previously and document and matching is performed and accelerated by Vespa's support for approximate nearest neighbor search.</p> <p>The output is limited to the top 1 hit, as we only have a sample of 3 videos. The top hit returned was based on a hybrid ranking based on a bm25 ranking based on a lexical search on the text, keywords and summary of the video, performed as a first phase, and similarity search on the embeddings.</p> <p>We can see as part of the <code>match-features</code>, the segment 212 in the video was the one providing the highest match.</p> <p>We also calculate the similarities as part of the <code>summary-features</code> for the rest of the segments so we can look for top N segments within a video, optionally.</p> In\u00a0[\u00a0]: Copied! <pre>with app.syncio(connections=1) as session:\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from videos where userQuery() OR ({targetHits:100}nearestNeighbor(embeddings,q))\",\n        query=user_query,\n        ranking=\"hybrid\",\n        hits=1,\n        body={\"input.query(q)\": q_embedding},\n    )\n    assert response.is_successful()\n\nfor hit in response.hits:\n    print(json.dumps(hit, indent=4))\n\n# response.get_json()\n</pre> with app.syncio(connections=1) as session:     response: VespaQueryResponse = session.query(         yql=\"select * from videos where userQuery() OR ({targetHits:100}nearestNeighbor(embeddings,q))\",         query=user_query,         ranking=\"hybrid\",         hits=1,         body={\"input.query(q)\": q_embedding},     )     assert response.is_successful()  for hit in response.hits:     print(json.dumps(hit, indent=4))  # response.get_json() <p>You should see output similar to this:</p> <pre>{\n    \"id\": \"id:videos:videos::13bcb994b389c9d925993e611877e40b\",\n    \"relevance\": 0.47162757625475055,\n    \"source\": \"videosearch_content\",\n    \"fields\": {\n        \"matchfeatures\": {\n            \"closest(embeddings)\": {\n                \"type\": \"tensor&lt;float&gt;(p{})\",\n                \"cells\": {\n                    \"212\": 1.0\n                }\n            }\n        },\n        \"sddocname\": \"videos\",\n        \"documentid\": \"id:videos:videos::13bcb994b389c9d925993e611877e40b\",\n        \"video_url\": \"https://ia601401.us.archive.org/1/items/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net.mp4\",\n        \"title\": \"Twas the night before Christmas\",\n        \"keywords\": \"Christmas Eve, Santa Claus, Clockmaker, Snowy Night, Mouse Characters\",\n        \"video_summary\": \"The video is an animated adaptation of \\\"A Visit from St. Nicholas,\\\" commonly known as \\\"The Night Before Christmas,\\\" narrated and sung by Joel Grey. It features a town where the residents, including a clockmaker named Joshua Trundle and his family, are troubled by Santa's absence due to a critical letter published in the local newspaper. The story unfolds with the clockmaker's son, Albert, realizing his mistake and attempting to fix a malfunctioning clock in the town hall that was meant to welcome Santa. Despite initial setbacks, the community's efforts eventually lead to Santa's arrival, restoring joy and belief in the magic of Christmas. The video concludes with Santa Claus descending through chimneys to deliver gifts, symbolizing the triumph of hope and the spirit of the holiday.\",\n        \"embedding_scope\": \"clip\",\n        \"start_offset_sec\": [\n            0.0,\n            6.0,\n            12.0,\n            18.0,\n        ],\n    }\n}\n</pre> <p>In order to process the results above in a more consumable format and sort out the top N segments based on similarities, we can do this more conveniently in a pandas dataframe below:</p> In\u00a0[37]: Copied! <pre>def get_top_n_similarity_matches(data, N=5):\n    \"\"\"\n    Function to extract the top N similarity scores and their corresponding start and end offsets.\n\n    Args:\n    - data (dict): Input JSON-like structure containing similarities and offsets.\n    - N (int): The number of top similarity scores to return.\n\n    Returns:\n    - pd.DataFrame: A DataFrame with the top N similarity scores and their corresponding offsets.\n    \"\"\"\n    # Extract relevant fields\n    similarities = data[\"fields\"][\"summaryfeatures\"][\"similarities\"][\"cells\"]\n    start_offset_sec = data[\"fields\"][\"start_offset_sec\"]\n    end_offset_sec = data[\"fields\"][\"end_offset_sec\"]\n\n    # Convert similarity scores to a list of tuples (index, similarity_score) and sort by similarity score\n    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n\n    # Extract top N similarity scores\n    top_n_similarities = sorted_similarities[:N]\n\n    # Prepare results\n    results = []\n    for index_str, score in top_n_similarities:\n        index = int(index_str)\n        if index &lt; len(start_offset_sec):\n            result = {\n                \"index\": index,\n                \"similarity_score\": score,\n                \"start_offset_sec\": start_offset_sec[index],\n                \"end_offset_sec\": end_offset_sec[index],\n            }\n        else:\n            result = {\n                \"index\": index,\n                \"similarity_score\": score,\n                \"start_offset_sec\": None,\n                \"end_offset_sec\": None,\n            }\n        results.append(result)\n\n    # Convert results to a DataFrame\n    df = pd.DataFrame(results)\n    return df\n</pre> def get_top_n_similarity_matches(data, N=5):     \"\"\"     Function to extract the top N similarity scores and their corresponding start and end offsets.      Args:     - data (dict): Input JSON-like structure containing similarities and offsets.     - N (int): The number of top similarity scores to return.      Returns:     - pd.DataFrame: A DataFrame with the top N similarity scores and their corresponding offsets.     \"\"\"     # Extract relevant fields     similarities = data[\"fields\"][\"summaryfeatures\"][\"similarities\"][\"cells\"]     start_offset_sec = data[\"fields\"][\"start_offset_sec\"]     end_offset_sec = data[\"fields\"][\"end_offset_sec\"]      # Convert similarity scores to a list of tuples (index, similarity_score) and sort by similarity score     sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)      # Extract top N similarity scores     top_n_similarities = sorted_similarities[:N]      # Prepare results     results = []     for index_str, score in top_n_similarities:         index = int(index_str)         if index &lt; len(start_offset_sec):             result = {                 \"index\": index,                 \"similarity_score\": score,                 \"start_offset_sec\": start_offset_sec[index],                 \"end_offset_sec\": end_offset_sec[index],             }         else:             result = {                 \"index\": index,                 \"similarity_score\": score,                 \"start_offset_sec\": None,                 \"end_offset_sec\": None,             }         results.append(result)      # Convert results to a DataFrame     df = pd.DataFrame(results)     return df In\u00a0[38]: Copied! <pre>df_result = get_top_n_similarity_matches(response.hits[0], N=10)\ndf_result\n</pre> df_result = get_top_n_similarity_matches(response.hits[0], N=10) df_result Out[38]: index similarity_score start_offset_sec end_offset_sec 0 212 0.435371 1272.0 1278.0 1 230 0.418007 1380.0 1386.0 2 210 0.411242 1260.0 1266.0 3 211 0.409344 1266.0 1272.0 4 208 0.408644 1248.0 1254.0 5 231 0.406000 1386.0 1392.0 6 209 0.404767 1254.0 1260.0 7 229 0.403729 1374.0 1380.0 8 203 0.403292 1218.0 1224.0 9 207 0.391671 1242.0 1248.0 In\u00a0[39]: Copied! <pre>def concatenate_contiguous_segments(df):\n    \"\"\"\n    Function to concatenate contiguous segments based on their start and end offsets.\n    Converts the concatenated segments to MM:SS format.\n\n    Args:\n    - df (pd.DataFrame): DataFrame with columns 'start_offset_sec' and 'end_offset_sec'.\n\n    Returns:\n    - List of tuples with concatenated segments in MM:SS format as (start_time, end_time).\n    \"\"\"\n    if df.empty:\n        return []\n\n    # Sort by start_offset_sec for ordered processing\n    df = df.sort_values(by=\"start_offset_sec\").reset_index(drop=True)\n\n    # Initialize the list to hold concatenated segments\n    concatenated_segments = []\n\n    # Initialize the first segment\n    start = df.iloc[0][\"start_offset_sec\"]\n    end = df.iloc[0][\"end_offset_sec\"]\n\n    for i in range(1, len(df)):\n        current_start = df.iloc[i][\"start_offset_sec\"]\n        current_end = df.iloc[i][\"end_offset_sec\"]\n\n        # Check if the current segment is contiguous with the previous one\n        if current_start &lt;= end:\n            # Extend the segment if it is contiguous\n            end = max(end, current_end)\n        else:\n            # Add the previous segment to the result list in MM:SS format\n            concatenated_segments.append(\n                (convert_seconds_to_mmss(start - 3), convert_seconds_to_mmss(end + 3))\n            )\n            # Start a new segment\n            start = current_start\n            end = current_end\n\n    # Add the final segment\n    concatenated_segments.append(\n        (convert_seconds_to_mmss(start - 3), convert_seconds_to_mmss(end + 3))\n    )\n\n    return concatenated_segments\n\n\ndef convert_seconds_to_mmss(seconds):\n    \"\"\"\n    Converts seconds to MM:SS format.\n\n    Args:\n    - seconds (float): Time in seconds.\n\n    Returns:\n    - str: Time in MM:SS format.\n    \"\"\"\n    minutes = int(seconds // 60)\n    seconds = int(seconds % 60)\n    return f\"{minutes:02}:{seconds:02}\"\n</pre> def concatenate_contiguous_segments(df):     \"\"\"     Function to concatenate contiguous segments based on their start and end offsets.     Converts the concatenated segments to MM:SS format.      Args:     - df (pd.DataFrame): DataFrame with columns 'start_offset_sec' and 'end_offset_sec'.      Returns:     - List of tuples with concatenated segments in MM:SS format as (start_time, end_time).     \"\"\"     if df.empty:         return []      # Sort by start_offset_sec for ordered processing     df = df.sort_values(by=\"start_offset_sec\").reset_index(drop=True)      # Initialize the list to hold concatenated segments     concatenated_segments = []      # Initialize the first segment     start = df.iloc[0][\"start_offset_sec\"]     end = df.iloc[0][\"end_offset_sec\"]      for i in range(1, len(df)):         current_start = df.iloc[i][\"start_offset_sec\"]         current_end = df.iloc[i][\"end_offset_sec\"]          # Check if the current segment is contiguous with the previous one         if current_start &lt;= end:             # Extend the segment if it is contiguous             end = max(end, current_end)         else:             # Add the previous segment to the result list in MM:SS format             concatenated_segments.append(                 (convert_seconds_to_mmss(start - 3), convert_seconds_to_mmss(end + 3))             )             # Start a new segment             start = current_start             end = current_end      # Add the final segment     concatenated_segments.append(         (convert_seconds_to_mmss(start - 3), convert_seconds_to_mmss(end + 3))     )      return concatenated_segments   def convert_seconds_to_mmss(seconds):     \"\"\"     Converts seconds to MM:SS format.      Args:     - seconds (float): Time in seconds.      Returns:     - str: Time in MM:SS format.     \"\"\"     minutes = int(seconds // 60)     seconds = int(seconds % 60)     return f\"{minutes:02}:{seconds:02}\" In\u00a0[40]: Copied! <pre>segments = concatenate_contiguous_segments(df_result)\nsegments\n</pre> segments = concatenate_contiguous_segments(df_result) segments Out[40]: <pre>[('20:15', '20:27'), ('20:39', '21:21'), ('22:51', '23:15')]</pre> <p>We can now spin-up the player and review the segments of interest. Video player is set to start in the middle of the first segment.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import HTML\n\nvideo_url = \"https://ia601401.us.archive.org/1/items/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net.mp4\"\n\nvideo_player = f\"\"\"\n&lt;video id=\"myVideo\" width=\"640\" height=\"480\" controls&gt;\n  &lt;source src=\"{video_url}\" type=\"video/mp4\"&gt;\n  Your browser does not support the video tag.\n&lt;/video&gt;\n\n\"\"\"\n\nHTML(video_player)\n</pre> from IPython.display import HTML  video_url = \"https://ia601401.us.archive.org/1/items/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net.mp4\"  video_player = f\"\"\"     Your browser does not support the video tag.   \"\"\"  HTML(video_player) Out[\u00a0]:    Your browser does not support the video tag.  In\u00a0[42]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete() <pre>Deactivated vespa-presales.videosearch in dev.aws-us-east-1c\nDeleted instance vespa-presales.videosearch.default\n</pre> <p>The following will delete the index created earlier where videos where uploaded:</p> In\u00a0[43]: Copied! <pre># Creating a client\nclient = TwelveLabs(api_key=TL_API_KEY)\n\nclient.index.delete(index_id)\n</pre> # Creating a client client = TwelveLabs(api_key=TL_API_KEY)  client.index.delete(index_id)"},{"location":"examples/video_search_twelvelabs_cloud.html#video-search-and-retrieval-with-vespa-and-twelvelabs","title":"Video Search and Retrieval with Vespa and TwelveLabs\u00b6","text":"<p>In the following notebook, we will demonstrate how to leverage TwelveLabs <code>Marengo-retrieval-2.7</code> a SOTA multimodal embedding model to demonstrate a use case of video embeddings storage and semantic search retrieval using Vespa.ai.</p> <p>The steps we will take in this notebook are:</p> <ol> <li>Setup and configuration</li> <li>Generate Attributes and Embeddings for 3 sample videos using the TwelveLabs python SDK.</li> <li>Deploy the Vespa application to Vespa Cloud and Feed the Data</li> <li>Perform a semantic search with hybrid multi-phase ranking on the videos</li> <li>Review the results</li> <li>Cleanup</li> </ol> <p>All the steps that are needed to provision the Vespa application, including feeding the data, can be done by running this notebook. We have tried to make it easy for others to run this notebook, to create your own Video semantic search application using TwelveLabs models with Vespa.</p> <p></p>"},{"location":"examples/video_search_twelvelabs_cloud.html#1-setup-and-configuration","title":"1. Setup and Configuration\u00b6","text":"<p>For reference, this is the Python version used for this notebook.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#11-install-libraries","title":"1.1 Install libraries\u00b6","text":"<p>Install the required Python dependencies from TwelveLabs python SDK and pyvespa python API.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#12-get-a-twelvelabs-api-key","title":"1.2 Get a TwelveLabs API key\u00b6","text":"<p>Sign-up for TwelveLabs.</p> <p>After logging in, navigate to your profile and get your API key. Copy it and paste it below.</p> <p>The Free plan includes indexing of 600 mins of videos, which should be sufficient to explore the capabilities of the API.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#13-sign-up-for-a-vespa-trial-account","title":"1.3 Sign-up for a Vespa Trial Account\u00b6","text":"<p>Pre-requisite:</p> <ul> <li>Spin-up a Vespa Cloud Trial account.</li> <li>Login to the account you just created and create a tenant at console.vespa-cloud.com.</li> <li>Save the tenant name.</li> </ul>"},{"location":"examples/video_search_twelvelabs_cloud.html#14-setup-the-tenant-name-and-the-application-name","title":"1.4 Setup the tenant name and the application name\u00b6","text":"<ul> <li>Paste below the name of the tenant name.</li> <li>Give your application a name. Note that the name cannot have <code>-</code> or <code>_</code>.</li> </ul>"},{"location":"examples/video_search_twelvelabs_cloud.html#2-generate-attributes-and-embeddings-for-sample-videos-using-twelvelabs-embedding-api","title":"2. Generate Attributes and Embeddings for sample videos using TwelveLabs Embedding API\u00b6","text":""},{"location":"examples/video_search_twelvelabs_cloud.html#21-generate-attributes-on-the-videos","title":"2.1 Generate attributes on the videos\u00b6","text":"<p>In this section, we will leverage the Pegasus 1.1 generative model to generate some attributes about our videos to store as part of the searchable information in Vespa. Attributes we want to store as part of the videos include:</p> <ul> <li>Keywords</li> <li>Summaries</li> </ul> <p>For video samples, we are selecting the 3 videos in the array below from the Internet Archive.</p> <p>You can customize this code with the urls of your choice. Note that there are certain restrictions such as the resolution of the videos.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#22-generate-embeddings","title":"2.2 Generate Embeddings\u00b6","text":"<p>The following code leverages the Embed API to create an asynchronous embedding task to embed the sample videos.</p> <p>Twelve Labs video embeddings capture all the subtle cues and interactions between different modalities, including the visual expressions, body language, spoken words, and the overall context of the video, encapsulating the essence of all these modalities and their interrelations over time.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#23-retrieve-embeddings","title":"2.3 Retrieve Embeddings\u00b6","text":"<p>Once the embedding task is completed, we can retrieve the results of the embedding task based on the task_ids.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#3-deploy-a-vespa-application","title":"3. Deploy a Vespa Application\u00b6","text":"<p>At this point, we are ready to deploy a Vespa Application. We have generated the attributes we needed on each video, as well as the embeddings.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#31-create-an-application-package","title":"3.1 Create an Application Package\u00b6","text":"<p>The application package has all the Vespa configuration files - create one from scratch:</p> <p>The Vespa schema deployed as part of the package is called <code>videos</code>. All the fields are matching the output of the Twelvelabs Embed API above. Refer to the Vespa documentation for more information on the schema specification.</p> <p>We can first define the schema using pyvespa</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#32-deploy-the-application-package","title":"3.2 Deploy the Application Package\u00b6","text":"<p>The app is now defined and ready to deploy to Vespa Cloud.</p> <p>Deploy <code>package</code> to Vespa Cloud, by creating an instance of VespaCloud:</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#33-feed-the-vespa-application","title":"3.3 Feed the Vespa Application\u00b6","text":"<p>The <code>vespa_feed</code> feed format for <code>pyvespa</code> expects a dict with the keys <code>id</code> and <code>fields</code>:</p> <p><code>{\u00a0\"id\": \"vespa-document-id\", \"fields\": {\"vespa_field\": \"vespa-field-value\"}}</code></p> <p>For the id, we will use a md5 hash of the video url.</p> <p>The video embedding output segments are added to the <code>fields</code> in <code>vespa_feed</code>.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#4-performing-search-on-the-videos","title":"4. Performing search on the videos\u00b6","text":""},{"location":"examples/video_search_twelvelabs_cloud.html#41-performing-a-hybrid-search-on-the-video","title":"4.1 Performing a hybrid search on the video\u00b6","text":"<p>As an example query, we will retrieve all the chunks which shows Santa Claus on his sleigh. The first step is to generate a text embedding for <code>Santa Claus on his sleigh</code> using the <code>Marengo-retrieval-2.7</code> model.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#5-review-results-optional","title":"5. Review results (Optional)\u00b6","text":"<p>We can review the results by spinning up a video player in the notebook and check the segments identified and judge by ourselves.</p> <p>But, first we need to obtain the contiguous segments, add 3 seconds overlap in the consolidated segments and convert to MM:SS so we can quickly find the segments to watch in the player. Let's write a function that takes the response as an input and provides the consolidated segments to view in the player.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#6-clean-up","title":"6. Clean-up\u00b6","text":"<p>The following will delete the application and data from the dev environment.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html","title":"Visual pdf rag with vespa colpali cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!python --version\n</pre> !python --version <p>Install dependencies:</p> <p>Note that the python pdf2image package requires poppler-utils, see other installation options here.</p> In\u00a0[\u00a0]: Copied! <pre>!sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y\n</pre> !sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y <p>Now install the required python packages:</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install colpali-engine==0.3.10 pdf2image pypdf==5.0.1 pyvespa&gt;=0.50.0 vespacli numpy==1.26.4 pillow==10.4.0 google-generativeai==0.8.3 transformers python-dotenv\n</pre> !pip3 install colpali-engine==0.3.10 pdf2image pypdf==5.0.1 pyvespa&gt;=0.50.0 vespacli numpy==1.26.4 pillow==10.4.0 google-generativeai==0.8.3 transformers python-dotenv In\u00a0[\u00a0]: Copied! <pre>import os\nimport json\nfrom typing import Tuple\nimport hashlib\nimport numpy as np\n\n# Vespa\nfrom vespa.package import (\n    ApplicationPackage,\n    Field,\n    Schema,\n    Document,\n    HNSW,\n    RankProfile,\n    Function,\n    FieldSet,\n    SecondPhaseRanking,\n    Summary,\n    DocumentSummary,\n)\nfrom vespa.deployment import VespaCloud\nfrom vespa.application import Vespa\nfrom vespa.io import VespaResponse\n\n# Google Generative AI for Google Gemini interaction\nimport google.generativeai as genai\n\n# Torch and other ML libraries\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom pdf2image import convert_from_path\nfrom pypdf import PdfReader\n\n# ColPali model and processor\nfrom colpali_engine.models import ColPali, ColPaliProcessor\nfrom colpali_engine.utils.torch_utils import get_torch_device\n\n# Load environment variables\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Avoid warning from huggingface tokenizers\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n</pre> import os import json from typing import Tuple import hashlib import numpy as np  # Vespa from vespa.package import (     ApplicationPackage,     Field,     Schema,     Document,     HNSW,     RankProfile,     Function,     FieldSet,     SecondPhaseRanking,     Summary,     DocumentSummary, ) from vespa.deployment import VespaCloud from vespa.application import Vespa from vespa.io import VespaResponse  # Google Generative AI for Google Gemini interaction import google.generativeai as genai  # Torch and other ML libraries import torch from torch.utils.data import DataLoader from tqdm import tqdm from pdf2image import convert_from_path from pypdf import PdfReader  # ColPali model and processor from colpali_engine.models import ColPali, ColPaliProcessor from colpali_engine.utils.torch_utils import get_torch_device  # Load environment variables from dotenv import load_dotenv  load_dotenv()  # Avoid warning from huggingface tokenizers os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" In\u00a0[\u00a0]: Copied! <pre>VESPA_TENANT_NAME = \"vespa-team\"  # Replace with your tenant name\n</pre> VESPA_TENANT_NAME = \"vespa-team\"  # Replace with your tenant name <p>Here, set your desired application name. (Will be created in later steps) Note that you can not have hyphen <code>-</code> or underscore <code>_</code> in the application name.</p> In\u00a0[\u00a0]: Copied! <pre>VESPA_APPLICATION_NAME = \"colpalidemodev\"\nVESPA_SCHEMA_NAME = \"pdf_page\"\n</pre> VESPA_APPLICATION_NAME = \"colpalidemodev\" VESPA_SCHEMA_NAME = \"pdf_page\" <p>Next, you can to create a token. This is an optional authentication method (the default is mTLS), and will be used for feeding data, and querying the application. For details, see Authenticating to Vespa Cloud. For now, we will use a single token with both read and write permissions. For production, we recommend separate tokens for feeding and querying, (the former with write permission, and the latter with read permission). The tokens can be created from the Vespa Cloud console in the 'Account' -&gt; 'Tokens' section. Please make sure to save the both the token id and it's value somwhere safe - you'll need it when you're going to connect to your app.</p> In\u00a0[\u00a0]: Copied! <pre># Replace this with the id of your token\nVESPA_TOKEN_ID = \"pyvespa_integration\"  # This needs to match the token_id that you created in the Vespa Cloud Console\n</pre> # Replace this with the id of your token VESPA_TOKEN_ID = \"pyvespa_integration\"  # This needs to match the token_id that you created in the Vespa Cloud Console <p>We also need to set the value of the write token to be able to feed data to the Vespa application (value of VESPA_TOKEN_ID_WRITE). Please run the cell below to set the variable.</p> In\u00a0[\u00a0]: Copied! <pre>VESPA_CLOUD_SECRET_TOKEN = os.getenv(\"VESPA_CLOUD_SECRET_TOKEN\") or input(\n    \"Enter Vespa cloud secret token: \"\n)\n</pre> VESPA_CLOUD_SECRET_TOKEN = os.getenv(\"VESPA_CLOUD_SECRET_TOKEN\") or input(     \"Enter Vespa cloud secret token: \" ) <p>We will use Google's Gemini API to create sample queries for our images. Create a Gemini API key from here. Once you have the key, please run the cell below. You can also use other VLM's to create these queries.</p> In\u00a0[\u00a0]: Copied! <pre>GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\") or input(\n    \"Enter Google Generative AI API key: \"\n)\n# Configure Google Generative AI\ngenai.configure(api_key=GOOGLE_API_KEY)\n</pre> GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\") or input(     \"Enter Google Generative AI API key: \" ) # Configure Google Generative AI genai.configure(api_key=GOOGLE_API_KEY) In\u00a0[\u00a0]: Copied! <pre>MODEL_NAME = \"vidore/colpali-v1.2\"\n\n# Set device for Torch\ndevice = get_torch_device(\"auto\")\nprint(f\"Using device: {device}\")\n\n# Load the ColPali model and processor\nmodel = ColPali.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float32,\n    device_map=device,\n).eval()\n\nprocessor = ColPaliProcessor.from_pretrained(MODEL_NAME)\n</pre> MODEL_NAME = \"vidore/colpali-v1.2\"  # Set device for Torch device = get_torch_device(\"auto\") print(f\"Using device: {device}\")  # Load the ColPali model and processor model = ColPali.from_pretrained(     MODEL_NAME,     torch_dtype=torch.float32,     device_map=device, ).eval()  processor = ColPaliProcessor.from_pretrained(MODEL_NAME) <p>As we can see, a lot of the information is in the form of tables, charts and numbers. These are not easily extractable using pdf-readers or OCR tools.</p> In\u00a0[\u00a0]: Copied! <pre>import requests\n\npdfs = [\n    {\n        \"url\": \"https://www.nbim.no/contentassets/c328a077177e4b03af6bee280e20d40e/gpfg-half-year-report-2024.pdf\",\n        \"path\": \"pdfs/gpfg-half-year-report-2024.pdf\",\n        \"year\": \"2024\",\n    },\n    {\n        \"url\": \"https://www.nbim.no/contentassets/75e18afc40974cb189e3747164def669/gpfg-annual-report_2023.pdf\",\n        \"path\": \"pdfs/gpfg-annual-report_2023.pdf\",\n        \"year\": \"2023\",\n    },\n]\n</pre> import requests  pdfs = [     {         \"url\": \"https://www.nbim.no/contentassets/c328a077177e4b03af6bee280e20d40e/gpfg-half-year-report-2024.pdf\",         \"path\": \"pdfs/gpfg-half-year-report-2024.pdf\",         \"year\": \"2024\",     },     {         \"url\": \"https://www.nbim.no/contentassets/75e18afc40974cb189e3747164def669/gpfg-annual-report_2023.pdf\",         \"path\": \"pdfs/gpfg-annual-report_2023.pdf\",         \"year\": \"2023\",     }, ] In\u00a0[\u00a0]: Copied! <pre>PDFS_DIR = \"pdfs\"\nos.makedirs(PDFS_DIR, exist_ok=True)\n\n\ndef download_pdf(url: str, path: str):\n    r = requests.get(url, stream=True)\n    with open(path, \"wb\") as f:\n        for chunk in r.iter_content(chunk_size=1024):\n            if chunk:\n                f.write(chunk)\n    return path\n\n\n# Download the pdfs\nfor pdf in pdfs:\n    download_pdf(pdf[\"url\"], pdf[\"path\"])\n</pre> PDFS_DIR = \"pdfs\" os.makedirs(PDFS_DIR, exist_ok=True)   def download_pdf(url: str, path: str):     r = requests.get(url, stream=True)     with open(path, \"wb\") as f:         for chunk in r.iter_content(chunk_size=1024):             if chunk:                 f.write(chunk)     return path   # Download the pdfs for pdf in pdfs:     download_pdf(pdf[\"url\"], pdf[\"path\"]) In\u00a0[\u00a0]: Copied! <pre>def get_pdf_images(pdf_path):\n    reader = PdfReader(pdf_path)\n    page_texts = []\n    for page_number in range(len(reader.pages)):\n        page = reader.pages[page_number]\n        text = page.extract_text()\n        page_texts.append(text)\n    # Convert to PIL images\n    images = convert_from_path(pdf_path)\n    assert len(images) == len(page_texts)\n    return images, page_texts\n\n\npdf_folder = \"pdfs\"\npdf_pages = []\nfor pdf in tqdm(pdfs):\n    pdf_file = pdf[\"path\"]\n    title = os.path.splitext(os.path.basename(pdf_file))[0]\n    images, texts = get_pdf_images(pdf_file)\n    for page_no, (image, text) in enumerate(zip(images, texts)):\n        pdf_pages.append(\n            {\n                \"title\": title,\n                \"year\": pdf[\"year\"],\n                \"url\": pdf[\"url\"],\n                \"path\": pdf_file,\n                \"image\": image,\n                \"text\": text,\n                \"page_no\": page_no,\n            }\n        )\n</pre> def get_pdf_images(pdf_path):     reader = PdfReader(pdf_path)     page_texts = []     for page_number in range(len(reader.pages)):         page = reader.pages[page_number]         text = page.extract_text()         page_texts.append(text)     # Convert to PIL images     images = convert_from_path(pdf_path)     assert len(images) == len(page_texts)     return images, page_texts   pdf_folder = \"pdfs\" pdf_pages = [] for pdf in tqdm(pdfs):     pdf_file = pdf[\"path\"]     title = os.path.splitext(os.path.basename(pdf_file))[0]     images, texts = get_pdf_images(pdf_file)     for page_no, (image, text) in enumerate(zip(images, texts)):         pdf_pages.append(             {                 \"title\": title,                 \"year\": pdf[\"year\"],                 \"url\": pdf[\"url\"],                 \"path\": pdf_file,                 \"image\": image,                 \"text\": text,                 \"page_no\": page_no,             }         ) In\u00a0[\u00a0]: Copied! <pre>len(pdf_pages)\n</pre> len(pdf_pages) In\u00a0[\u00a0]: Copied! <pre>MAX_PAGES = 10  # Set to None to use all pages\npdf_pages = pdf_pages[:MAX_PAGES] if MAX_PAGES else pdf_pages\n</pre> MAX_PAGES = 10  # Set to None to use all pages pdf_pages = pdf_pages[:MAX_PAGES] if MAX_PAGES else pdf_pages <p>We now have 176 pages, which will be the entity we define as one document in Vespa.</p> <p>Let us look at the extracted text from the pages displayed above.</p> In\u00a0[\u00a0]: Copied! <pre>pdf_pages[8][\"image\"]\n</pre> pdf_pages[8][\"image\"] In\u00a0[\u00a0]: Copied! <pre>print(pdf_pages[8][\"text\"])\n</pre> print(pdf_pages[8][\"text\"]) In\u00a0[\u00a0]: Copied! <pre># print(pdf_pages[95][\"text\"])\n</pre> # print(pdf_pages[95][\"text\"]) <p>As we can see, the extracted text fails to capture the visual information we see in the image, and it would be difficult for an LLM to correctly answer questions such as 'Price development in Technology sector from April 2023?' based on the text alone.</p> In\u00a0[\u00a0]: Copied! <pre>from pydantic import BaseModel\n\n\nclass GeneratedQueries(BaseModel):\n    broad_topical_question: str\n    broad_topical_query: str\n    specific_detail_question: str\n    specific_detail_query: str\n    visual_element_question: str\n    visual_element_query: str\n\n\ndef get_retrieval_prompt() -&gt; Tuple[str, GeneratedQueries]:\n    prompt = (\n        prompt\n    ) = \"\"\"You are an investor, stock analyst and financial expert. You will be presented an image of a document page from a report published by the Norwegian Government Pension Fund Global (GPFG). The report may be annual or quarterly reports, or policy reports, on topics such as responsible investment, risk etc.\nYour task is to generate retrieval queries and questions that you would use to retrieve this document (or ask based on this document) in a large corpus.\nPlease generate 3 different types of retrieval queries and questions.\nA retrieval query is a keyword based query, made up of 2-5 words, that you would type into a search engine to find this document.\nA question is a natural language question that you would ask, for which the document contains the answer.\nThe queries should be of the following types:\n1. A broad topical query: This should cover the main subject of the document.\n2. A specific detail query: This should cover a specific detail or aspect of the document.\n3. A visual element query: This should cover a visual element of the document, such as a chart, graph, or image.\n\nImportant guidelines:\n- Ensure the queries are relevant for retrieval tasks, not just describing the page content.\n- Use a fact-based natural language style for the questions.\n- Frame the queries as if someone is searching for this document in a large corpus.\n- Make the queries diverse and representative of different search strategies.\n\nFormat your response as a JSON object with the structure of the following example:\n{\n    \"broad_topical_question\": \"What was the Responsible Investment Policy in 2019?\",\n    \"broad_topical_query\": \"responsible investment policy 2019\",\n    \"specific_detail_question\": \"What is the percentage of investments in renewable energy?\",\n    \"specific_detail_query\": \"renewable energy investments percentage\",\n    \"visual_element_question\": \"What is the trend of total holding value over time?\",\n    \"visual_element_query\": \"total holding value trend\"\n}\n\nIf there are no relevant visual elements, provide an empty string for the visual element question and query.\nHere is the document image to analyze:\nGenerate the queries based on this image and provide the response in the specified JSON format.\nOnly return JSON. Don't return any extra explanation text. \"\"\"\n\n    return prompt, GeneratedQueries\n\n\nprompt_text, pydantic_model = get_retrieval_prompt()\n</pre> from pydantic import BaseModel   class GeneratedQueries(BaseModel):     broad_topical_question: str     broad_topical_query: str     specific_detail_question: str     specific_detail_query: str     visual_element_question: str     visual_element_query: str   def get_retrieval_prompt() -&gt; Tuple[str, GeneratedQueries]:     prompt = (         prompt     ) = \"\"\"You are an investor, stock analyst and financial expert. You will be presented an image of a document page from a report published by the Norwegian Government Pension Fund Global (GPFG). The report may be annual or quarterly reports, or policy reports, on topics such as responsible investment, risk etc. Your task is to generate retrieval queries and questions that you would use to retrieve this document (or ask based on this document) in a large corpus. Please generate 3 different types of retrieval queries and questions. A retrieval query is a keyword based query, made up of 2-5 words, that you would type into a search engine to find this document. A question is a natural language question that you would ask, for which the document contains the answer. The queries should be of the following types: 1. A broad topical query: This should cover the main subject of the document. 2. A specific detail query: This should cover a specific detail or aspect of the document. 3. A visual element query: This should cover a visual element of the document, such as a chart, graph, or image.  Important guidelines: - Ensure the queries are relevant for retrieval tasks, not just describing the page content. - Use a fact-based natural language style for the questions. - Frame the queries as if someone is searching for this document in a large corpus. - Make the queries diverse and representative of different search strategies.  Format your response as a JSON object with the structure of the following example: {     \"broad_topical_question\": \"What was the Responsible Investment Policy in 2019?\",     \"broad_topical_query\": \"responsible investment policy 2019\",     \"specific_detail_question\": \"What is the percentage of investments in renewable energy?\",     \"specific_detail_query\": \"renewable energy investments percentage\",     \"visual_element_question\": \"What is the trend of total holding value over time?\",     \"visual_element_query\": \"total holding value trend\" }  If there are no relevant visual elements, provide an empty string for the visual element question and query. Here is the document image to analyze: Generate the queries based on this image and provide the response in the specified JSON format. Only return JSON. Don't return any extra explanation text. \"\"\"      return prompt, GeneratedQueries   prompt_text, pydantic_model = get_retrieval_prompt() In\u00a0[\u00a0]: Copied! <pre>gemini_model = genai.GenerativeModel(\"gemini-1.5-flash-8b\")\n\n\ndef generate_queries(image, prompt_text, pydantic_model):\n    try:\n        response = gemini_model.generate_content(\n            [image, \"\\n\\n\", prompt_text],\n            generation_config=genai.GenerationConfig(\n                response_mime_type=\"application/json\",\n                response_schema=pydantic_model,\n            ),\n        )\n        queries = json.loads(response.text)\n    except Exception as _e:\n        print(_e)\n        queries = {\n            \"broad_topical_question\": \"\",\n            \"broad_topical_query\": \"\",\n            \"specific_detail_question\": \"\",\n            \"specific_detail_query\": \"\",\n            \"visual_element_question\": \"\",\n            \"visual_element_query\": \"\",\n        }\n    return queries\n</pre> gemini_model = genai.GenerativeModel(\"gemini-1.5-flash-8b\")   def generate_queries(image, prompt_text, pydantic_model):     try:         response = gemini_model.generate_content(             [image, \"\\n\\n\", prompt_text],             generation_config=genai.GenerationConfig(                 response_mime_type=\"application/json\",                 response_schema=pydantic_model,             ),         )         queries = json.loads(response.text)     except Exception as _e:         print(_e)         queries = {             \"broad_topical_question\": \"\",             \"broad_topical_query\": \"\",             \"specific_detail_question\": \"\",             \"specific_detail_query\": \"\",             \"visual_element_question\": \"\",             \"visual_element_query\": \"\",         }     return queries In\u00a0[\u00a0]: Copied! <pre>for pdf in tqdm(pdf_pages):\n    image = pdf.get(\"image\")\n    pdf[\"queries\"] = generate_queries(image, prompt_text, pydantic_model)\n</pre> for pdf in tqdm(pdf_pages):     image = pdf.get(\"image\")     pdf[\"queries\"] = generate_queries(image, prompt_text, pydantic_model) <p>Let's take a look at the queries and questions generated for the page displayed above.</p> In\u00a0[\u00a0]: Copied! <pre>pdf_pages[8][\"queries\"]\n</pre> pdf_pages[8][\"queries\"] In\u00a0[\u00a0]: Copied! <pre>def generate_embeddings(images, model, processor, batch_size=1) -&gt; np.ndarray:\n    \"\"\"\n    Generate embeddings for a list of images.\n    Move to CPU only once per batch.\n\n    Args:\n        images (List[PIL.Image]): List of PIL images.\n        model (nn.Module): The model to generate embeddings.\n        processor: The processor to preprocess images.\n        batch_size (int, optional): Batch size for processing. Defaults to 64.\n\n    Returns:\n        np.ndarray: Embeddings for the images, shape\n                    (len(images), processor.max_patch_length (1030 for ColPali), model.config.hidden_size (Patch embedding dimension - 128 for ColPali)).\n    \"\"\"\n\n    def collate_fn(batch):\n        # Batch is a list of images\n        return processor.process_images(batch)  # Should return a dict of tensors\n\n    dataloader = DataLoader(\n        images,\n        shuffle=False,\n        collate_fn=collate_fn,\n    )\n\n    embeddings_list = []\n    for batch in tqdm(dataloader):\n        with torch.no_grad():\n            batch = {k: v.to(model.device) for k, v in batch.items()}\n            embeddings_batch = model(**batch)\n            # Convert tensor to numpy array and append to list\n            embeddings_list.extend(\n                [t.cpu().numpy() for t in torch.unbind(embeddings_batch)]\n            )\n\n    # Stack all embeddings into a single numpy array\n    all_embeddings = np.stack(embeddings_list, axis=0)\n    return all_embeddings\n</pre> def generate_embeddings(images, model, processor, batch_size=1) -&gt; np.ndarray:     \"\"\"     Generate embeddings for a list of images.     Move to CPU only once per batch.      Args:         images (List[PIL.Image]): List of PIL images.         model (nn.Module): The model to generate embeddings.         processor: The processor to preprocess images.         batch_size (int, optional): Batch size for processing. Defaults to 64.      Returns:         np.ndarray: Embeddings for the images, shape                     (len(images), processor.max_patch_length (1030 for ColPali), model.config.hidden_size (Patch embedding dimension - 128 for ColPali)).     \"\"\"      def collate_fn(batch):         # Batch is a list of images         return processor.process_images(batch)  # Should return a dict of tensors      dataloader = DataLoader(         images,         shuffle=False,         collate_fn=collate_fn,     )      embeddings_list = []     for batch in tqdm(dataloader):         with torch.no_grad():             batch = {k: v.to(model.device) for k, v in batch.items()}             embeddings_batch = model(**batch)             # Convert tensor to numpy array and append to list             embeddings_list.extend(                 [t.cpu().numpy() for t in torch.unbind(embeddings_batch)]             )      # Stack all embeddings into a single numpy array     all_embeddings = np.stack(embeddings_list, axis=0)     return all_embeddings In\u00a0[\u00a0]: Copied! <pre># Generate embeddings for all images\nimages = [pdf[\"image\"] for pdf in pdf_pages]\nembeddings = generate_embeddings(images, model, processor)\n</pre> # Generate embeddings for all images images = [pdf[\"image\"] for pdf in pdf_pages] embeddings = generate_embeddings(images, model, processor) <p>Now, we have one embedding vector of dimension 128 for each patch of each image (1024 patches + some special tokens).</p> In\u00a0[\u00a0]: Copied! <pre>embeddings.shape\n</pre> embeddings.shape In\u00a0[\u00a0]: Copied! <pre>assert len(pdf_pages) == embeddings.shape[0]\nassert embeddings.shape[1] &gt; 1028  # Number of patches (including special tokens)\nassert embeddings.shape[2] == 128  # Embedding dimension per patch\n</pre> assert len(pdf_pages) == embeddings.shape[0] assert embeddings.shape[1] &gt; 1028  # Number of patches (including special tokens) assert embeddings.shape[2] == 128  # Embedding dimension per patch In\u00a0[\u00a0]: Copied! <pre>def float_to_binary_embedding(float_query_embedding: dict) -&gt; dict:\n    \"\"\"Utility function to convert float query embeddings to binary query embeddings.\"\"\"\n    binary_query_embeddings = {}\n    for k, v in float_query_embedding.items():\n        binary_vector = (\n            np.packbits(np.where(np.array(v) &gt; 0, 1, 0)).astype(np.int8).tolist()\n        )\n        binary_query_embeddings[k] = binary_vector\n    return binary_query_embeddings\n</pre> def float_to_binary_embedding(float_query_embedding: dict) -&gt; dict:     \"\"\"Utility function to convert float query embeddings to binary query embeddings.\"\"\"     binary_query_embeddings = {}     for k, v in float_query_embedding.items():         binary_vector = (             np.packbits(np.where(np.array(v) &gt; 0, 1, 0)).astype(np.int8).tolist()         )         binary_query_embeddings[k] = binary_vector     return binary_query_embeddings <p>We also need a couple of image processing helper functions. These are borrowed from vidore-benchmark repo.</p> In\u00a0[\u00a0]: Copied! <pre>import base64\nimport io\nfrom pathlib import Path\nfrom typing import Union\nfrom PIL import Image\n\n\ndef scale_image(image: Image.Image, new_height: int = 1024) -&gt; Image.Image:\n    \"\"\"\n    Scale an image to a new height while maintaining the aspect ratio.\n    \"\"\"\n    # Calculate the scaling factor\n    width, height = image.size\n    aspect_ratio = width / height\n    new_width = int(new_height * aspect_ratio)\n\n    # Resize the image\n    scaled_image = image.resize((new_width, new_height))\n    return scaled_image\n\n\ndef get_base64_image(img: Union[str, Image.Image], add_url_prefix: bool = True) -&gt; str:\n    \"\"\"\n    Convert an image (from a filepath or a PIL.Image object) to a JPEG-base64 string.\n    \"\"\"\n    if isinstance(img, str):\n        img = Image.open(img)\n    elif isinstance(img, Image.Image):\n        pass\n    else:\n        raise ValueError(\"`img` must be a path to an image or a PIL Image object.\")\n\n    buffered = io.BytesIO()\n    img.save(buffered, format=\"jpeg\")\n    b64_data = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n    return f\"data:image/jpeg;base64,{b64_data}\" if add_url_prefix else b64_data\n</pre> import base64 import io from pathlib import Path from typing import Union from PIL import Image   def scale_image(image: Image.Image, new_height: int = 1024) -&gt; Image.Image:     \"\"\"     Scale an image to a new height while maintaining the aspect ratio.     \"\"\"     # Calculate the scaling factor     width, height = image.size     aspect_ratio = width / height     new_width = int(new_height * aspect_ratio)      # Resize the image     scaled_image = image.resize((new_width, new_height))     return scaled_image   def get_base64_image(img: Union[str, Image.Image], add_url_prefix: bool = True) -&gt; str:     \"\"\"     Convert an image (from a filepath or a PIL.Image object) to a JPEG-base64 string.     \"\"\"     if isinstance(img, str):         img = Image.open(img)     elif isinstance(img, Image.Image):         pass     else:         raise ValueError(\"`img` must be a path to an image or a PIL Image object.\")      buffered = io.BytesIO()     img.save(buffered, format=\"jpeg\")     b64_data = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")     return f\"data:image/jpeg;base64,{b64_data}\" if add_url_prefix else b64_data <p>Note that we also store a scaled down (blurred) version of the image in Vespa. The purpose of this is to return this fast on first results to the frontend, to provide a snappy user experience, and then load the full resolution image async in the background.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_feed = []\nfor pdf, embedding in zip(pdf_pages, embeddings):\n    url = pdf[\"url\"]\n    year = pdf[\"year\"]\n    title = pdf[\"title\"]\n    image = pdf[\"image\"]\n    text = pdf.get(\"text\", \"\")\n    page_no = pdf[\"page_no\"]\n    query_dict = pdf[\"queries\"]\n    questions = [v for k, v in query_dict.items() if \"question\" in k and v]\n    queries = [v for k, v in query_dict.items() if \"query\" in k and v]\n    base_64_image = get_base64_image(\n        scale_image(image, 32), add_url_prefix=False\n    )  # Scaled down image to return fast on search (~1kb)\n    base_64_full_image = get_base64_image(image, add_url_prefix=False)\n    embedding_dict = {k: v for k, v in enumerate(embedding)}\n    binary_embedding = float_to_binary_embedding(embedding_dict)\n    # id_hash should be md5 hash of url and page_number\n    id_hash = hashlib.md5(f\"{url}_{page_no}\".encode()).hexdigest()\n    page = {\n        \"id\": id_hash,\n        \"fields\": {\n            \"id\": id_hash,\n            \"url\": url,\n            \"title\": title,\n            \"year\": year,\n            \"page_number\": page_no,\n            \"blur_image\": base_64_image,\n            \"full_image\": base_64_full_image,\n            \"text\": text,\n            \"embedding\": binary_embedding,\n            \"queries\": queries,\n            \"questions\": questions,\n        },\n    }\n    vespa_feed.append(page)\n</pre> vespa_feed = [] for pdf, embedding in zip(pdf_pages, embeddings):     url = pdf[\"url\"]     year = pdf[\"year\"]     title = pdf[\"title\"]     image = pdf[\"image\"]     text = pdf.get(\"text\", \"\")     page_no = pdf[\"page_no\"]     query_dict = pdf[\"queries\"]     questions = [v for k, v in query_dict.items() if \"question\" in k and v]     queries = [v for k, v in query_dict.items() if \"query\" in k and v]     base_64_image = get_base64_image(         scale_image(image, 32), add_url_prefix=False     )  # Scaled down image to return fast on search (~1kb)     base_64_full_image = get_base64_image(image, add_url_prefix=False)     embedding_dict = {k: v for k, v in enumerate(embedding)}     binary_embedding = float_to_binary_embedding(embedding_dict)     # id_hash should be md5 hash of url and page_number     id_hash = hashlib.md5(f\"{url}_{page_no}\".encode()).hexdigest()     page = {         \"id\": id_hash,         \"fields\": {             \"id\": id_hash,             \"url\": url,             \"title\": title,             \"year\": year,             \"page_number\": page_no,             \"blur_image\": base_64_image,             \"full_image\": base_64_full_image,             \"text\": text,             \"embedding\": binary_embedding,             \"queries\": queries,             \"questions\": questions,         },     }     vespa_feed.append(page) In\u00a0[\u00a0]: Copied! <pre># os.makedirs(\"output\", exist_ok=True)\n# with open(\"output/vespa_feed.jsonl\", \"w\") as f:\n#     vespa_feed_to_save = []\n#     for page in vespa_feed:\n#         document_id = page[\"id\"]\n#         put_id = f\"id:{VESPA_APPLICATION_NAME}:{VESPA_SCHEMA_NAME}::{document_id}\"\n#         vespa_feed_to_save.append({\"put\": put_id, \"fields\": page[\"fields\"]})\n#     json.dump(vespa_feed_to_save, f)\n</pre> # os.makedirs(\"output\", exist_ok=True) # with open(\"output/vespa_feed.jsonl\", \"w\") as f: #     vespa_feed_to_save = [] #     for page in vespa_feed: #         document_id = page[\"id\"] #         put_id = f\"id:{VESPA_APPLICATION_NAME}:{VESPA_SCHEMA_NAME}::{document_id}\" #         vespa_feed_to_save.append({\"put\": put_id, \"fields\": page[\"fields\"]}) #     json.dump(vespa_feed_to_save, f) In\u00a0[\u00a0]: Copied! <pre>colpali_schema = Schema(\n    name=VESPA_SCHEMA_NAME,\n    document=Document(\n        fields=[\n            Field(\n                name=\"id\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                match=[\"word\"],\n            ),\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"year\", type=\"int\", indexing=[\"summary\", \"attribute\"]),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(name=\"page_number\", type=\"int\", indexing=[\"summary\", \"attribute\"]),\n            Field(name=\"blur_image\", type=\"raw\", indexing=[\"summary\"]),\n            Field(name=\"full_image\", type=\"raw\", indexing=[\"summary\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;int8&gt;(patch{}, v[16])\",\n                indexing=[\n                    \"attribute\",\n                    \"index\",\n                ],\n                ann=HNSW(\n                    distance_metric=\"hamming\",\n                    max_links_per_node=32,\n                    neighbors_to_explore_at_insert=400,\n                ),\n            ),\n            Field(\n                name=\"questions\",\n                type=\"array&lt;string&gt;\",\n                indexing=[\"summary\", \"attribute\"],\n                summary=Summary(fields=[\"matched-elements-only\"]),\n            ),\n            Field(\n                name=\"queries\",\n                type=\"array&lt;string&gt;\",\n                indexing=[\"summary\", \"attribute\"],\n                summary=Summary(fields=[\"matched-elements-only\"]),\n            ),\n        ]\n    ),\n    fieldsets=[\n        FieldSet(\n            name=\"default\",\n            fields=[\"title\", \"text\"],\n        ),\n    ],\n    document_summaries=[\n        DocumentSummary(\n            name=\"default\",\n            summary_fields=[\n                Summary(\n                    name=\"text\",\n                    fields=[(\"bolding\", \"on\")],\n                ),\n                Summary(\n                    name=\"snippet\",\n                    fields=[(\"source\", \"text\"), \"dynamic\"],\n                ),\n            ],\n            from_disk=True,\n        ),\n        DocumentSummary(\n            name=\"suggestions\",\n            summary_fields=[\n                Summary(name=\"questions\"),\n            ],\n            from_disk=True,\n        ),\n    ],\n)\n\n# Define similarity functions used in all rank profiles\nmapfunctions = [\n    Function(\n        name=\"similarities\",  # computes similarity scores between each query token and image patch\n        expression=\"\"\"\n                sum(\n                    query(qt) * unpack_bits(attribute(embedding)), v\n                )\n            \"\"\",\n    ),\n    Function(\n        name=\"normalized\",  # normalizes the similarity scores to [-1, 1]\n        expression=\"\"\"\n                (similarities - reduce(similarities, min)) / (reduce((similarities - reduce(similarities, min)), max)) * 2 - 1\n            \"\"\",\n    ),\n    Function(\n        name=\"quantized\",  # quantizes the normalized similarity scores to signed 8-bit integers [-128, 127]\n        expression=\"\"\"\n                cell_cast(normalized * 127.999, int8)\n            \"\"\",\n    ),\n]\n\n# Define the 'bm25' rank profile\nbm25 = RankProfile(\n    name=\"bm25\",\n    inputs=[(\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\")],\n    first_phase=\"bm25(title) + bm25(text)\",\n    functions=mapfunctions,\n)\n\n\n# A function to create an inherited rank profile which also returns quantized similarity scores\ndef with_quantized_similarity(rank_profile: RankProfile) -&gt; RankProfile:\n    return RankProfile(\n        name=f\"{rank_profile.name}_sim\",\n        first_phase=rank_profile.first_phase,\n        inherits=rank_profile.name,\n        summary_features=[\"quantized\"],\n    )\n\n\ncolpali_schema.add_rank_profile(bm25)\ncolpali_schema.add_rank_profile(with_quantized_similarity(bm25))\n\n\n# Update the 'colpali' rank profile\ninput_query_tensors = []\nMAX_QUERY_TERMS = 64\nfor i in range(MAX_QUERY_TERMS):\n    input_query_tensors.append((f\"query(rq{i})\", \"tensor&lt;int8&gt;(v[16])\"))\n\ninput_query_tensors.extend(\n    [\n        (\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\"),\n        (\"query(qtb)\", \"tensor&lt;int8&gt;(querytoken{}, v[16])\"),\n    ]\n)\n\ncolpali = RankProfile(\n    name=\"colpali\",\n    inputs=input_query_tensors,\n    first_phase=\"max_sim_binary\",\n    second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10),\n    functions=mapfunctions\n    + [\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(embedding)), v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(\n            name=\"max_sim_binary\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        1 / (1 + sum(\n                            hamming(query(qtb), attribute(embedding)), v)\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n    ],\n)\ncolpali_schema.add_rank_profile(colpali)\ncolpali_schema.add_rank_profile(with_quantized_similarity(colpali))\n\n# Update the 'hybrid' rank profile\nhybrid = RankProfile(\n    name=\"hybrid\",\n    inputs=input_query_tensors,\n    first_phase=\"max_sim_binary\",\n    second_phase=SecondPhaseRanking(\n        expression=\"max_sim + 2 * (bm25(text) + bm25(title))\", rerank_count=10\n    ),\n    functions=mapfunctions\n    + [\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(embedding)), v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(\n            name=\"max_sim_binary\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        1 / (1 + sum(\n                            hamming(query(qtb), attribute(embedding)), v)\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n    ],\n)\ncolpali_schema.add_rank_profile(hybrid)\ncolpali_schema.add_rank_profile(with_quantized_similarity(hybrid))\n</pre> colpali_schema = Schema(     name=VESPA_SCHEMA_NAME,     document=Document(         fields=[             Field(                 name=\"id\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 match=[\"word\"],             ),             Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"year\", type=\"int\", indexing=[\"summary\", \"attribute\"]),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(name=\"page_number\", type=\"int\", indexing=[\"summary\", \"attribute\"]),             Field(name=\"blur_image\", type=\"raw\", indexing=[\"summary\"]),             Field(name=\"full_image\", type=\"raw\", indexing=[\"summary\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"embedding\",                 type=\"tensor(patch{}, v[16])\",                 indexing=[                     \"attribute\",                     \"index\",                 ],                 ann=HNSW(                     distance_metric=\"hamming\",                     max_links_per_node=32,                     neighbors_to_explore_at_insert=400,                 ),             ),             Field(                 name=\"questions\",                 type=\"array\",                 indexing=[\"summary\", \"attribute\"],                 summary=Summary(fields=[\"matched-elements-only\"]),             ),             Field(                 name=\"queries\",                 type=\"array\",                 indexing=[\"summary\", \"attribute\"],                 summary=Summary(fields=[\"matched-elements-only\"]),             ),         ]     ),     fieldsets=[         FieldSet(             name=\"default\",             fields=[\"title\", \"text\"],         ),     ],     document_summaries=[         DocumentSummary(             name=\"default\",             summary_fields=[                 Summary(                     name=\"text\",                     fields=[(\"bolding\", \"on\")],                 ),                 Summary(                     name=\"snippet\",                     fields=[(\"source\", \"text\"), \"dynamic\"],                 ),             ],             from_disk=True,         ),         DocumentSummary(             name=\"suggestions\",             summary_fields=[                 Summary(name=\"questions\"),             ],             from_disk=True,         ),     ], )  # Define similarity functions used in all rank profiles mapfunctions = [     Function(         name=\"similarities\",  # computes similarity scores between each query token and image patch         expression=\"\"\"                 sum(                     query(qt) * unpack_bits(attribute(embedding)), v                 )             \"\"\",     ),     Function(         name=\"normalized\",  # normalizes the similarity scores to [-1, 1]         expression=\"\"\"                 (similarities - reduce(similarities, min)) / (reduce((similarities - reduce(similarities, min)), max)) * 2 - 1             \"\"\",     ),     Function(         name=\"quantized\",  # quantizes the normalized similarity scores to signed 8-bit integers [-128, 127]         expression=\"\"\"                 cell_cast(normalized * 127.999, int8)             \"\"\",     ), ]  # Define the 'bm25' rank profile bm25 = RankProfile(     name=\"bm25\",     inputs=[(\"query(qt)\", \"tensor(querytoken{}, v[128])\")],     first_phase=\"bm25(title) + bm25(text)\",     functions=mapfunctions, )   # A function to create an inherited rank profile which also returns quantized similarity scores def with_quantized_similarity(rank_profile: RankProfile) -&gt; RankProfile:     return RankProfile(         name=f\"{rank_profile.name}_sim\",         first_phase=rank_profile.first_phase,         inherits=rank_profile.name,         summary_features=[\"quantized\"],     )   colpali_schema.add_rank_profile(bm25) colpali_schema.add_rank_profile(with_quantized_similarity(bm25))   # Update the 'colpali' rank profile input_query_tensors = [] MAX_QUERY_TERMS = 64 for i in range(MAX_QUERY_TERMS):     input_query_tensors.append((f\"query(rq{i})\", \"tensor(v[16])\"))  input_query_tensors.extend(     [         (\"query(qt)\", \"tensor(querytoken{}, v[128])\"),         (\"query(qtb)\", \"tensor(querytoken{}, v[16])\"),     ] )  colpali = RankProfile(     name=\"colpali\",     inputs=input_query_tensors,     first_phase=\"max_sim_binary\",     second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10),     functions=mapfunctions     + [         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(embedding)), v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(             name=\"max_sim_binary\",             expression=\"\"\"                 sum(                     reduce(                         1 / (1 + sum(                             hamming(query(qtb), attribute(embedding)), v)                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),     ], ) colpali_schema.add_rank_profile(colpali) colpali_schema.add_rank_profile(with_quantized_similarity(colpali))  # Update the 'hybrid' rank profile hybrid = RankProfile(     name=\"hybrid\",     inputs=input_query_tensors,     first_phase=\"max_sim_binary\",     second_phase=SecondPhaseRanking(         expression=\"max_sim + 2 * (bm25(text) + bm25(title))\", rerank_count=10     ),     functions=mapfunctions     + [         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(embedding)), v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(             name=\"max_sim_binary\",             expression=\"\"\"                 sum(                     reduce(                         1 / (1 + sum(                             hamming(query(qtb), attribute(embedding)), v)                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),     ], ) colpali_schema.add_rank_profile(hybrid) colpali_schema.add_rank_profile(with_quantized_similarity(hybrid)) In\u00a0[\u00a0]: Copied! <pre>from vespa.configuration.services import (\n    services,\n    container,\n    search,\n    document_api,\n    document_processing,\n    clients,\n    client,\n    config,\n    content,\n    redundancy,\n    documents,\n    node,\n    certificate,\n    token,\n    document,\n    nodes,\n)\nfrom vespa.configuration.vt import vt\nfrom vespa.package import ServicesConfiguration\n\nservice_config = ServicesConfiguration(\n    application_name=VESPA_APPLICATION_NAME,\n    services_config=services(\n        container(\n            search(),\n            document_api(),\n            document_processing(),\n            clients(\n                client(\n                    certificate(file=\"security/clients.pem\"),\n                    id=\"mtls\",\n                    permissions=\"read,write\",\n                ),\n                client(\n                    token(id=f\"{VESPA_TOKEN_ID}\"),\n                    id=\"token_write\",\n                    permissions=\"read,write\",\n                ),\n            ),\n            config(\n                vt(\"tag\")(\n                    vt(\"bold\")(\n                        vt(\"open\", \"&lt;strong&gt;\"),\n                        vt(\"close\", \"&lt;/strong&gt;\"),\n                    ),\n                    vt(\"separator\", \"...\"),\n                ),\n                name=\"container.qr-searchers\",\n            ),\n            id=f\"{VESPA_APPLICATION_NAME}_container\",\n            version=\"1.0\",\n        ),\n        content(\n            redundancy(\"1\"),\n            documents(document(type=\"pdf_page\", mode=\"index\")),\n            nodes(node(distribution_key=\"0\", hostalias=\"node1\")),\n            config(\n                vt(\"max_matches\", \"2\", replace_underscores=False),\n                vt(\"length\", \"1000\"),\n                vt(\"surround_max\", \"500\", replace_underscores=False),\n                vt(\"min_length\", \"300\", replace_underscores=False),\n                name=\"vespa.config.search.summary.juniperrc\",\n            ),\n            id=f\"{VESPA_APPLICATION_NAME}_content\",\n            version=\"1.0\",\n        ),\n        version=\"1.0\",\n    ),\n)\n</pre> from vespa.configuration.services import (     services,     container,     search,     document_api,     document_processing,     clients,     client,     config,     content,     redundancy,     documents,     node,     certificate,     token,     document,     nodes, ) from vespa.configuration.vt import vt from vespa.package import ServicesConfiguration  service_config = ServicesConfiguration(     application_name=VESPA_APPLICATION_NAME,     services_config=services(         container(             search(),             document_api(),             document_processing(),             clients(                 client(                     certificate(file=\"security/clients.pem\"),                     id=\"mtls\",                     permissions=\"read,write\",                 ),                 client(                     token(id=f\"{VESPA_TOKEN_ID}\"),                     id=\"token_write\",                     permissions=\"read,write\",                 ),             ),             config(                 vt(\"tag\")(                     vt(\"bold\")(                         vt(\"open\", \"\"),                         vt(\"close\", \"\"),                     ),                     vt(\"separator\", \"...\"),                 ),                 name=\"container.qr-searchers\",             ),             id=f\"{VESPA_APPLICATION_NAME}_container\",             version=\"1.0\",         ),         content(             redundancy(\"1\"),             documents(document(type=\"pdf_page\", mode=\"index\")),             nodes(node(distribution_key=\"0\", hostalias=\"node1\")),             config(                 vt(\"max_matches\", \"2\", replace_underscores=False),                 vt(\"length\", \"1000\"),                 vt(\"surround_max\", \"500\", replace_underscores=False),                 vt(\"min_length\", \"300\", replace_underscores=False),                 name=\"vespa.config.search.summary.juniperrc\",             ),             id=f\"{VESPA_APPLICATION_NAME}_content\",             version=\"1.0\",         ),         version=\"1.0\",     ), ) In\u00a0[\u00a0]: Copied! <pre># Create the Vespa application package\nvespa_application_package = ApplicationPackage(\n    name=VESPA_APPLICATION_NAME,\n    schema=[colpali_schema],\n    services_config=service_config,\n)\n</pre> # Create the Vespa application package vespa_application_package = ApplicationPackage(     name=VESPA_APPLICATION_NAME,     schema=[colpali_schema],     services_config=service_config, ) In\u00a0[\u00a0]: Copied! <pre># This is only needed for CI.\nVESPA_TEAM_API_KEY = os.getenv(\"VESPA_TEAM_API_KEY\", None)\n</pre> # This is only needed for CI. VESPA_TEAM_API_KEY = os.getenv(\"VESPA_TEAM_API_KEY\", None) In\u00a0[\u00a0]: Copied! <pre>vespa_cloud = VespaCloud(\n    tenant=VESPA_TENANT_NAME,\n    application=VESPA_APPLICATION_NAME,\n    key_content=VESPA_TEAM_API_KEY,\n    application_package=vespa_application_package,\n)\n\n# Deploy the application\nvespa_cloud.deploy()\n\n# Output the endpoint URL\nendpoint_url = vespa_cloud.get_token_endpoint()\nprint(f\"Application deployed. Token endpoint URL: {endpoint_url}\")\n</pre> vespa_cloud = VespaCloud(     tenant=VESPA_TENANT_NAME,     application=VESPA_APPLICATION_NAME,     key_content=VESPA_TEAM_API_KEY,     application_package=vespa_application_package, )  # Deploy the application vespa_cloud.deploy()  # Output the endpoint URL endpoint_url = vespa_cloud.get_token_endpoint() print(f\"Application deployed. Token endpoint URL: {endpoint_url}\") <p>Make sure to take note of the token endpoint_url. You need to put this in your <code>.env</code> file for your web application - <code>VESPA_APP_TOKEN_URL=https://abcd.vespa-app.cloud</code> - to access the Vespa application from your web application.</p> In\u00a0[\u00a0]: Copied! <pre># Instantiate Vespa connection using token\napp = Vespa(url=endpoint_url, vespa_cloud_secret_token=VESPA_CLOUD_SECRET_TOKEN)\napp.get_application_status()\n</pre> # Instantiate Vespa connection using token app = Vespa(url=endpoint_url, vespa_cloud_secret_token=VESPA_CLOUD_SECRET_TOKEN) app.get_application_status() <p>Now, let us feed the data to Vespa. If you have a large dataset, you could also do this async, with <code>feed_async_iterable()</code>, see Feeding Vespa cloud for a detailed comparison.</p> In\u00a0[\u00a0]: Copied! <pre>def callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n\n\n# Feed data into Vespa synchronously\napp.feed_iterable(vespa_feed, schema=VESPA_SCHEMA_NAME, callback=callback)\n</pre> def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         )   # Feed data into Vespa synchronously app.feed_iterable(vespa_feed, schema=VESPA_SCHEMA_NAME, callback=callback) <p>For now, we will just run a query with the default rank profile. We will need a utility function to generate embeddings for the query, and pass this to Vespa to use for calculating MaxSim. In the web application, we also provide function to generate binary embeddings, allowing the user to choose different rank profiles at query time.</p> In\u00a0[\u00a0]: Copied! <pre>query = \"Price development in Technology sector from April 2023?\"\n</pre> query = \"Price development in Technology sector from April 2023?\" In\u00a0[\u00a0]: Copied! <pre>def get_q_embs_vespa_format(query: str):\n    inputs = processor.process_queries([query]).to(model.device)\n    with torch.no_grad():\n        embeddings_query = model(**inputs)\n        q_embs = embeddings_query.to(\"cpu\")[0]  # Extract the single embedding\n    return {idx: emb.tolist() for idx, emb in enumerate(q_embs)}\n</pre> def get_q_embs_vespa_format(query: str):     inputs = processor.process_queries([query]).to(model.device)     with torch.no_grad():         embeddings_query = model(**inputs)         q_embs = embeddings_query.to(\"cpu\")[0]  # Extract the single embedding     return {idx: emb.tolist() for idx, emb in enumerate(q_embs)} In\u00a0[\u00a0]: Copied! <pre>q_emb = get_q_embs_vespa_format(query)\n</pre> q_emb = get_q_embs_vespa_format(query) In\u00a0[\u00a0]: Copied! <pre>with app.syncio() as sess:\n    response = sess.query(\n        body={\n            \"yql\": (\n                f\"select id, url, title, year, full_image, quantized  from {VESPA_SCHEMA_NAME} where userQuery();\"\n            ),\n            \"ranking\": \"default\",\n            \"query\": query,\n            \"timeout\": \"10s\",\n            \"hits\": 3,\n            \"input.query(qt)\": q_emb,\n            \"presentation.timing\": True,\n        }\n    )\n</pre> with app.syncio() as sess:     response = sess.query(         body={             \"yql\": (                 f\"select id, url, title, year, full_image, quantized  from {VESPA_SCHEMA_NAME} where userQuery();\"             ),             \"ranking\": \"default\",             \"query\": query,             \"timeout\": \"10s\",             \"hits\": 3,             \"input.query(qt)\": q_emb,             \"presentation.timing\": True,         }     ) In\u00a0[\u00a0]: Copied! <pre>assert len(response.json[\"root\"][\"children\"]) == 3\n</pre> assert len(response.json[\"root\"][\"children\"]) == 3 <p>Great. You have now deployed the Vespa application and fed the data to it, and made sure you are able to query it using the vespa endpoint and a token.</p> In\u00a0[\u00a0]: Copied! <pre>key_path = Path(\n    f\"~/.vespa/{VESPA_TENANT_NAME}.{VESPA_APPLICATION_NAME}.default/data-plane-private-key.pem\"\n).expanduser()\ncert_path = Path(\n    f\"~/.vespa/{VESPA_TENANT_NAME}.{VESPA_APPLICATION_NAME}.default/data-plane-public-cert.pem\"\n).expanduser()\n\nassert key_path.exists(), cert_path.exists()\n</pre> key_path = Path(     f\"~/.vespa/{VESPA_TENANT_NAME}.{VESPA_APPLICATION_NAME}.default/data-plane-private-key.pem\" ).expanduser() cert_path = Path(     f\"~/.vespa/{VESPA_TENANT_NAME}.{VESPA_APPLICATION_NAME}.default/data-plane-public-cert.pem\" ).expanduser()  assert key_path.exists(), cert_path.exists() In\u00a0[\u00a0]: Copied! <pre>!git clone --depth 1 --filter=blob:none --sparse https://github.com/vespa-engine/sample-apps.git src &amp;&amp; cd src &amp;&amp; git sparse-checkout set visual-retrieval-colpali\n</pre> !git clone --depth 1 --filter=blob:none --sparse https://github.com/vespa-engine/sample-apps.git src &amp;&amp; cd src &amp;&amp; git sparse-checkout set visual-retrieval-colpali <p>Now, you have the code for the webapp in your <code>src/visual-retrieval-colpali</code>-directory</p> In\u00a0[\u00a0]: Copied! <pre>os.listdir(\"src/visual-retrieval-colpali\")\n</pre> os.listdir(\"src/visual-retrieval-colpali\") In\u00a0[\u00a0]: Copied! <pre># rename src/visual-retrieval-colpali/.env.example\nos.rename(\n    \"src/visual-retrieval-colpali/.env.example\", dst=\"src/visual-retrieval-colpali/.env\"\n)\n</pre> # rename src/visual-retrieval-colpali/.env.example os.rename(     \"src/visual-retrieval-colpali/.env.example\", dst=\"src/visual-retrieval-colpali/.env\" ) <p>And you're ready to spin up your web app locally, and deploy to huggingface spaces if you want. Navigate to <code>src/visual-retrieval-colpali/</code> directory and follow the instructions in the <code>README.md</code> to continue. \ud83d\ude80</p> In\u00a0[\u00a0]: Copied! <pre>if os.getenv(\"CI\", \"false\") == \"true\":\n    vespa_cloud.delete()\n</pre> if os.getenv(\"CI\", \"false\") == \"true\":     vespa_cloud.delete()"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#visual-pdf-rag-with-vespa-colpali-demo-application","title":"Visual PDF RAG with Vespa - ColPali demo application\u00b6","text":"<p>We created an end-to-end demo application for visual retrieval of PDF pages using Vespa, including a frontend web application. To see the live demo, visit https://vespa-engine-colpali-vespa-visual-retrieval.hf.space/.</p> <p>The main goal of the demo is to make it easy for you to create your own PDF Enterprise Search application using Vespa. To deploy a full demo, you need two main components:</p> <ol> <li>A Vespa application that lets you index and search PDF pages using ColPali embeddings.</li> <li>A live web application that lets you interact with the Vespa application.</li> </ol> <p></p> <p>After running this notebook, you will have set up a Vespa application, and indexed some PDF pages. You can then test that you are able to query the Vespa application, and you will be ready to deploy the web application including the frontend.</p> <p>Some of the features we want to highlight in this demo are:</p> <ul> <li>Visual retrieval of PDF pages using ColPali embeddings</li> <li>Explainability by displaying similarity maps over the patches in the PDF pages for each query token.</li> <li>Extracting queries and questions from the PDF pages using <code>gemini-1.5-8b</code> model.</li> <li>Type-ahead search suggestions based on the extracted queries and questions.</li> <li>Comparison of different retrieval and ranking strategies (BM25, ColPali MaxSim, and a combination of both).</li> <li>AI-generated responses to the query based on the top ranked PDF pages. Also using the <code>gemini-1.5-8b</code> model.</li> </ul> <p>We also wanted to give a notion of which latency one can expect using Vespa for this use case. Event though your users might not state this explicitly, we consider it important to provide a snappy user experience.</p> <p>In this notebook, we will prepare the Vespa backend application for our visual retrieval demo. We will use ColPali as the model to extract patch vectors from images of pdf pages. At query time, we use MaxSim to retrieve and/or (based on the configuration) rank the page results.</p> <p></p> <p></p> <p>The steps we will take in this notebook are:</p> <ol> <li>Setup and configuration</li> <li>Download PDFs</li> <li>Convert PDFs to images</li> <li>Generate queries and questions</li> <li>Generate ColPali embeddings</li> <li>Prepare the Vespa application package</li> <li>Deploy the Vespa application to Vespa Cloud</li> <li>Feed the data to the Vespa application</li> <li>Test a query to the Vespa application</li> </ol> <p>All the steps that are needed to provision the Vespa application, including feeding the data, can be done by running this notebook. We have tried to make it easy for others to run this notebook, to create your own PDF Enterprise Search application using Vespa.</p> <p>If you want to run this notebook in Colab, you can do so by clicking the button below:</p> <p></p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#1-setup-and-configuration","title":"1. Setup and Configuration\u00b6","text":""},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#create-a-free-trial-in-vespa-cloud","title":"Create a free trial in Vespa Cloud\u00b6","text":"<p>Create a tenant from here. The trial includes $300 credit. Take note of your tenant name, and input it below.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#loading-the-colpali-model-from-huggingface","title":"Loading the ColPali model from huggingface \ud83e\udd17\u00b6","text":""},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#2-download-pdfs","title":"2. Download PDFs\u00b6","text":"<p>We are going to use public reports from the Norwegian Government Pension Fund Global (also known as the Oil Fund). The fund puts transparency at the forefront and publishes reports on its investments, holdings, and returns, as well as its strategy and governance.</p> <p>These reports are the ones we are going to use for this showcase. Here are some sample images:</p> <p> </p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#downloading-the-pdfs","title":"Downloading the PDFs\u00b6","text":"<p>We create a function to download the PDFs from the web to the provided directory.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#3-convert-pdfs-to-images","title":"3. Convert PDFs to Images\u00b6","text":""},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#4-generate-queries","title":"4. Generate Queries\u00b6","text":"<p>In this step, we want to generate queries for each page image. These will be useful for 2 reasons:</p> <ol> <li>We can use these queries as typeahead suggestions in the search bar.</li> <li>We could potentially use the queries to generate an evaluation dataset. See Improving Retrieval with LLM-as-a-judge for a deeper dive into this topic. This will not be within the scope of this notebook though.</li> </ol> <p>The prompt for generating queries is adapted from this wonderful blog post by Daniel van Strien.</p> <p>We have modified the prompt to also generate keword based queries, in addition to the question based queries.</p> <p>We will use the Gemini API to generate these queries, with <code>gemini-1.5-flash-8b</code> as the model.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#5-generate-embeddings","title":"5. Generate embeddings\u00b6","text":"<p>Now that we have the queries, we can use the ColPali model to generate embeddings for each page image.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#6-prepare-data-on-vespa-format","title":"6. Prepare Data on Vespa Format\u00b6","text":"<p>Now, that we have all the data we need, all that remains is to make sure it is in the right format for Vespa.</p> <p>We now convert the embeddings to Vespa JSON format so we can store (and index) them in Vespa. Details in Vespa JSON feed format doc.</p> <p>We use binary quantization (BQ) of the page level ColPali vector embeddings to reduce their size by 32x.</p> <p>Read more about binarization of multi-vector representations in the colbert blog post.</p> <p>The binarization step maps 128 dimensional floats to 128 bits, or 16 bytes per vector. Reducing the size by 32x. On the DocVQA benchmark, binarization results in only a small drop in ranking accuracy.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#optional-saving-the-feed-file","title":"[Optional] Saving the feed file\u00b6","text":"<p>If you have a large dataset, you can optionally save the file, and feed it using the Vespa CLI, which is more performant than the pyvespa client. See Feeding to Vespa Cloud for more details. Uncomment the cell below if you want to save the feed file.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#7-prepare-vespa-application","title":"7. Prepare Vespa Application\u00b6","text":""},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#configuring-the-application-package","title":"Configuring the application package\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>Here are some of the key components of this application package:</p> <ol> <li>We store images (and a scaled down version of the image) as a <code>raw</code> field.</li> <li>We store the binarized ColPali embeddings as a <code>tensor&lt;int8&gt;</code> field.</li> <li>We store the queries and questions as a <code>array&lt;str&gt;</code> field.</li> <li>We define 3 different ranking profiles:<ul> <li><code>default</code> Uses BM25 for first phase ranking and MaxSim for second phase ranking.</li> <li><code>bm25</code> Uses <code>bm25(title) + bm25(text)</code> (first phase only) for ranking.</li> <li><code>retrieval-and-rerank</code> Uses <code>nearestneighbor</code> of the query embedding over the document embeddings for retrieval, <code>binary_max_sim</code> for first phase ranking, and <code>max_sim</code> of the query-embeddings as float for second phase ranking. Vespa's phased ranking allows us to use different ranking strategies for retrieval and reranking, to choose attractive trade-offs between latency, cost, and accuracy.</li> </ul> </li> <li>We also calculate dot product between the query and each document, so that it can be returned with the results, to generate the similarity maps, which show which patches of the image is most similar to the query token embeddings.</li> </ol> <p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#configuring-the-servicesxml","title":"Configuring the <code>services.xml</code>\u00b6","text":"<p>services.xml is the primary configuration file for a Vespa application, with a plethora of options to configure the application.</p> <p>Since <code>pyvespa</code> version <code>0.50.0</code>, these configuration options are also available in <code>pyvespa</code>. See Pyvespa - Advanced configuration for more details. (Note that configurating this is optional, and pyvespa will use basic defaults for you if you opt out).</p> <p>We will use the advanced configuration to configure up dynamic snippets. This allows us to highlight matched terms in the search results and generate a <code>snippet</code> to display, rather than the full text of the document.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#8-deploy-vespa-application","title":"8. Deploy Vespa Application\u00b6","text":""},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#9-feed-data-to-vespa","title":"9. Feed Data to Vespa\u00b6","text":"<p>We will need the <code>enpdoint_url</code> and <code>colpalidemo_write</code> token to feed the data to the Vespa application.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#10-test-a-query-to-the-vespa-application","title":"10. Test a query to the Vespa application\u00b6","text":""},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#saving-the-generated-keycert-files","title":"Saving the generated key/cert files\u00b6","text":"<p>A key and cert file is generated for you as an alternative to using tokens for authentication. We advise you to save these files in a secure location, in case you want to use them for authentication in the future.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#11-deploying-your-web-app","title":"11. Deploying your web app\u00b6","text":"<p>To deploy a frontend to let users interact with the Vespa application. you can clone the sample app from sample-apps repo. It includes instructions for running and connecting your web application to your vespa app.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#setting-environment-variables-for-your-web-app","title":"Setting environment variables for your web app\u00b6","text":"<p>Now, you need to set the following variables in the <code>src/.env.example</code>-file:</p> <pre>VESPA_APP_TOKEN_URL=https://abcde.z.vespa-app.cloud # Your token endpoint url you got after deploying your Vespa app.\nVESPA_CLOUD_SECRET_TOKEN=vespa_cloud_xxxxxxxx # The value of the token that your created in this notebook. \nGEMINI_API_KEY=your_api_key # The same as GOOGLE_API_KEY in this notebook\nHF_TOKEN=hf_xxxx # If you want to deploy your web app to huggingface spaces - https://huggingface.co/settings/tokens\n</pre> <p>After, that, rename your file to .env.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#cleanup","title":"Cleanup\u00b6","text":"<p>As this notebook runs in CI, we will delete the Vespa application after running the notebook. DO NOT run the cell below unless you are sure you want to delete the Vespa application.</p>"}]}